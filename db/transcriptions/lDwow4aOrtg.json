[{"start":"4848.875","dur":"4.135","text":"uh, see, see you guys next Monday.","_id":"54f571a99e364761a532a0ea80303cc8"},{"start":"4847.24","dur":"1.635","text":"All right, let's break for today,","_id":"875dec3b26b842318281a34e31fdd16f"},{"start":"4843.24","dur":"4","text":"then you have the full complexity of the SVM norm, okay?","_id":"1624b14044ad4d5d8d4767494b9b9129"},{"start":"4840.6","dur":"2.64","text":"which is really a baby SVM and we add kernels to it,","_id":"92ebce782b4848aaba4349e9a0d19090"},{"start":"4838.335","dur":"2.265","text":"then you have the optimal management classifier","_id":"64f78d8ba0dc4c06804228c899853b8e"},{"start":"4836.52","dur":"1.815","text":"uh, when we convene next week,","_id":"27c96f707d8341129e6b3872040cc470"},{"start":"4833.985","dur":"2.535","text":"assuming your data's separable [NOISE] and we'll fix that assumption,","_id":"9e542e21c8174cc688e0df2c826fd97b"},{"start":"4831.645","dur":"2.34","text":"And if you give this a dataset then, you know,","_id":"63c7a8e7d2ab4e23a3aff912c69cfdde"},{"start":"4827.29","dur":"4.355","text":"very good numerical optimization packages to solve this optimization problem.","_id":"8f218584dff247fd9bf7e2a2e772b78d"},{"start":"4823.99","dur":"3.3","text":"then you will have the optimal margin classifier and they're","_id":"61d2f0e3a25e49b2a2893561f4059fd8"},{"start":"4819.325","dur":"4.665","text":"Um, but this turns out to be a convex optimization problem and if you optimize this,","_id":"f34af6da6c284a05a09ea9c82f800b2a"},{"start":"4816.31","dur":"3.015","text":"Uh, but the details I gave you in the lecture notes, okay?","_id":"d7b828b7b15c450a9892b909e755b31d"},{"start":"4811.84","dur":"4.47","text":"Th- th- the, the less of a normalization division effect you have, right?","_id":"5670606ef927499d92027bc87d306c51"},{"start":"4807.37","dur":"4.47","text":"um, uh, the smaller w is the bigger, right?","_id":"01b8cac761f040f5b57493208298e71a"},{"start":"4804.7","dur":"2.67","text":"And maybe one piece of intuition to take away is,","_id":"7e9d2a1e968044de8d4282b770a3b915"},{"start":"4800.335","dur":"4.365","text":"which is to try to minimize the norm of w, uh, subject to this.","_id":"f6f3dcde596b40c7bce3b6a56d8a426d"},{"start":"4796.165","dur":"4.17","text":"you can rewrite this optimization problem into the following equivalent form","_id":"29558701204249bcb9ef8c7fe014a101"},{"start":"4794.245","dur":"1.92","text":"uh, through a few steps, uh,","_id":"f114cd85e7f64318aecdd85c2ee29b74"},{"start":"4790.84","dur":"3.405","text":"And what we show in the lecture notes is that,","_id":"6555814500b94817adceaf9517879551"},{"start":"4788.695","dur":"2.145","text":"''Maximize the geometric margin.''","_id":"91b955b4adf54b81935b29e86fda8708"},{"start":"4785.395","dur":"3.3","text":"So this is the way to formulate optimization problem that says,","_id":"770b95a1dac84c9b8548a6147e06c2f8"},{"start":"4782.74","dur":"2.655","text":"equal to gamma and you want gamma to be as big as possible.","_id":"dddd3ba91cf44679881797448fba27c5"},{"start":"4780.22","dur":"2.52","text":"every example has a geometric margin greater or","_id":"57c3f28694c6499daf132a87188f69cc"},{"start":"4776.17","dur":"4.05","text":"So this problem is just you know solve for w and b to make sure that","_id":"d86277e7cf2940ea8caf8cf9fe44645f"},{"start":"4772.735","dur":"3.435","text":"Um, so it turns out- so I hope this problem makes sense, right?","_id":"2b5267079daa49ea844ff53f7c3fc4d9"},{"start":"4764.8","dur":"7.935","text":"to the geometric margin, right.","_id":"7bd330b31c1f4f4d9ea657353d7a295f"},{"start":"4760.15","dur":"4.65","text":"into the equivalent problem which is a minimizing norm of w subject","_id":"5e8202dd919d45a59e85a1fdfed6a4d0"},{"start":"4756.745","dur":"3.405","text":"you can reformulate this problem as, um,","_id":"95f3802eb5af4a3ba30a6193cd66bfe3"},{"start":"4753.4","dur":"3.345","text":"But it turns out that via a few steps of V writing,","_id":"74016619df554875a06c2488a139bba4"},{"start":"4750.7","dur":"2.7","text":"a gradient descent and initially known local optima and so on.","_id":"c82c44b62d0e48a48e0995cc36f0e24b"},{"start":"4749.14","dur":"1.56","text":"So it's difficult to solve this without","_id":"1776e6e27ba0426aa845160c0b424d21"},{"start":"4747.04","dur":"2.1","text":"this is in a convex optimization problems.","_id":"b5098f9f64dd4360bddb7c99ccda5df3"},{"start":"4745.57","dur":"1.47","text":"um- not in this form,","_id":"aebc6764c7b44d8ebbb2acf861389c39"},{"start":"4743.8","dur":"1.77","text":"And it turns out this is,","_id":"2a524e26986d4cd9b3bd8ac2f4c075cb"},{"start":"4739.21","dur":"4.59","text":"[NOISE] This causes you to maximize the worst-case geometric margin.","_id":"9f8f384d007a4bf7945771aca82d8388"},{"start":"4736.18","dur":"3.03","text":"that every single training example must have at least that geometric margin.","_id":"be1d143d2f60423cbe69370833c5134e"},{"start":"4733.69","dur":"2.49","text":"So you want gamma to be as big as possible subject to","_id":"ec0179f67c0a42bbb4f068e28bf06ebd"},{"start":"4729.49","dur":"4.2","text":"uh, uh, greater than or equal to gamma, right?","_id":"1988816c27a845d3aa36b205195e6ead"},{"start":"4726.925","dur":"2.565","text":"must have geometric margin,","_id":"27dc03dcace74b309b3dfa50100936b8"},{"start":"4722.43","dur":"4.495","text":"Subject to the every training example, um, uh,","_id":"83273e05f66f4cedadcb03cc49e339fe"},{"start":"4703.825","dur":"5.185","text":"So you want to maximize the geometric margin subject to that.","_id":"b5ce25faf09349408fd2890359d70914"},{"start":"4696.085","dur":"7.74","text":"one way to pose this problem is to maximize gamma w and b of gamma.","_id":"07fde26a875e4e588470eb99388a5138"},{"start":"4694.39","dur":"1.695","text":"But it turns out that, um,","_id":"be25f72360fd46eb872f1b6eda8dbf80"},{"start":"4690.4","dur":"3.99","text":"the last step and leave the in bet- in between steps to the lecture notes.","_id":"921b15fc61f8441b85465439e39dfa57"},{"start":"4687.745","dur":"2.655","text":"I'll just describe the beginning step and","_id":"8ad6838fdc6c408394f1dd6aec2bd62c"},{"start":"4684.64","dur":"3.105","text":"there are a few steps of this derivations I don't want to do but I'll,","_id":"4907839db2ed4afdadc6ab37f450267e"},{"start":"4679.65","dur":"4.99","text":"Now, uh, how you pose this mathematically,","_id":"f9ec16f1edc34231a595f4ce9058ecfd"},{"start":"4675.58","dur":"4","text":"to all of these examples, okay?","_id":"0163625f04184f9e9b6506e1cb72dbea"},{"start":"4669.985","dur":"5.595","text":"because that straight line maximizes the distance or maximizes the geometric margin","_id":"5d79a4b39e494b5cb61392838201442e"},{"start":"4666.16","dur":"3.825","text":"[NOISE] And so the optimal margin classifier will choose that straight line,","_id":"106d0df3534c466193dcdcc3008b1ab3"},{"start":"4659.83","dur":"6.33","text":"a SVM for linearly separable data, uh, at least for today.","_id":"97656b9710614307ba1dc156c129d434"},{"start":"4655.075","dur":"4.755","text":"thi- this- the optimal margin classifiers is the baby SVM, you know, it's like,","_id":"7b50c44924164023976709e40719bc38"},{"start":"4653.485","dur":"1.59","text":"Um, so in other words,","_id":"7a631eeabcf54efda08a09329fc2b3c3"},{"start":"4643.75","dur":"9.735","text":"b to maximize the geometric margin, okay?","_id":"48dc8eb792014709b3d527f9d746fc7f"},{"start":"4637.48","dur":"6.27","text":"um, choose the parameters w and","_id":"5733af02275c445ebb61b74e3de4d5ef"},{"start":"4628.99","dur":"8.49","text":"does is [NOISE] ,","_id":"3a19bcca15f14e629264c09fe99a0a8b"},{"start":"4626.22","dur":"2.77","text":"what the optimal margin classifier","_id":"cf00dab2df204f738399234dada8c9cd"},{"start":"4604.365","dur":"4.675","text":"okay? And so, um,","_id":"95f3e6818abf494495a4a98392138891"},{"start":"4595.93","dur":"8.435","text":"gamma is the geometric margin,","_id":"41f6dc11377c421185b9208af23e5710"},{"start":"4588.22","dur":"7.71","text":"So gamma hat was the functional margin and","_id":"68916ff4040a42bb9d8cc0d6b04a54ff"},{"start":"4586.9","dur":"1.32","text":"I hope the notation is clear, right.","_id":"3cdf887d2b334c08b01c4ee2158c810d"},{"start":"4585.235","dur":"1.665","text":"Uh, an- and so I hope the- sorry,","_id":"125eee2a80a24382bc07627e51f26ebd"},{"start":"4579.865","dur":"5.37","text":"um, and that is your geometric margin on the training set.","_id":"425f1c8263b1444c86b8a779c9a552d0"},{"start":"4575.545","dur":"4.32","text":"uh- look through all your training examples and pick the worst possible training example,","_id":"1caad996c7c84734ae55e63d10073058"},{"start":"4570.925","dur":"4.62","text":"where again uses worst-case notion of,","_id":"493421f5c3704dde99595fbdf12fe963"},{"start":"4552.61","dur":"18.315","text":"Finally, um, the geometric margin with respect to the training set is, um,","_id":"58fe6d83973f42b7a1cfb86c869743b2"},{"start":"4534.61","dur":"18","text":"the norm of w. [NOISE]","_id":"5ba5c8eb012f43dba99919de2170fc3b"},{"start":"4528.64","dur":"5.97","text":"that the geometric margin is equal to the functional margin divided by","_id":"12281cd4298c4c0686beaaaae0d52c94"},{"start":"4523.44","dur":"5.2","text":"And so the relationship between the geometric margin and the functional margin is","_id":"9f6e6c0cca154149a3fbf412a93aaec4"},{"start":"4516.82","dur":"6.58","text":"uh, and this definition applies to positive examples and the negative examples, okay?","_id":"bda1755ef6524b7983e793191e3f48a9"},{"start":"4507.01","dur":"9.81","text":"I'm going to define the geometric margin to be equal to this,","_id":"884eb6c4972b466ab5ae4f684a453647"},{"start":"4504.28","dur":"2.73","text":"Uh, more generally, um,","_id":"9d98b472195b4340a34c211f4a301d08"},{"start":"4497.305","dur":"6.975","text":"Um, and uh, a- and this is [NOISE] for the positive example I guess.","_id":"a91f69d660874a93a62ce3ca0dac3e43"},{"start":"4494.5","dur":"2.805","text":"and the decision boundary, okay?","_id":"83d5229aaca84520b9e12cb41458d46b"},{"start":"4490.18","dur":"4.32","text":"to be the way you compute the Euclidean distance between that example and uh,","_id":"2325990c34bf4a08b5cd8172b3323a61"},{"start":"4487.93","dur":"2.25","text":"here but the proof is given in the lecture notes but this turns out","_id":"d2032bc1e39c4bc69a41b0678aaeb9d9"},{"start":"4485.845","dur":"2.085","text":"Uh, but, and then, I'm not proving this","_id":"da83fed1e7df4be5bfe25058d9be7e90"},{"start":"4482.08","dur":"3.765","text":"measuring the Euclidean distance that I just drew [NOISE] in the picture up there, okay.","_id":"f5ce1b6e6b3642fa9d8dd3e66933f9d4"},{"start":"4478.405","dur":"3.675","text":"the lecture notes shows why this is the right formula for","_id":"063dddb077c145bd91866ade669132ac"},{"start":"4476.32","dur":"2.085","text":"the proof is given in the lecture notes but, uh,","_id":"1822b9713868423d92bf79ed024e17b5"},{"start":"4472.63","dur":"3.69","text":"Um, and let's see I'm not proving why this is the case,","_id":"772fe5c8378948dfb1fbded9fa10ae86"},{"start":"4465.07","dur":"7.56","text":"w transpose x plus b over [NOISE] the normal w. [NOISE]","_id":"7ce96ab60a9144f39160d11108207f25"},{"start":"4460.9","dur":"4.17","text":"This is going to be gamma i equals","_id":"a637ca2336a44ec7a92bb854b13a8094"},{"start":"4454.72","dur":"6.18","text":"b with respect to one example x_i, y_i.","_id":"5356ffe983974fdeb13c00a59d591797"},{"start":"4450.28","dur":"4.44","text":"you know, the classifier of the hyperplane defined by w,","_id":"08b6332395d741e499f131d2eeb6505c"},{"start":"4443.86","dur":"6.42","text":"So the geometric margin of,","_id":"c9a0e36410944ac09ac20fec4f587e3e"},{"start":"4429.82","dur":"14.04","text":"[NOISE]","_id":"869f26bfb69b4e31bfbcd77113e9c728"},{"start":"4427.34","dur":"2.48","text":"So let me just write down what that is.","_id":"b54d63e5944b4fa3b2aa046bd0a605f6"},{"start":"4422.335","dur":"5.005","text":"is the Euc- the Euclidean distance is what will define to be the geometric margin.","_id":"f83a3d205acb4b8390fc137c33f28f73"},{"start":"4415.39","dur":"6.945","text":"to be that geometric margin of this training example,","_id":"40d5d21e868643929ca936b64c3a0f78"},{"start":"4406.83","dur":"8.56","text":"So what we're going to do is define this distance, um,","_id":"a4a99163f24d461597146d34da5fa12f"},{"start":"4403.06","dur":"3.61","text":"from predicting negative to positive is the decision boundary.","_id":"be795c080e8645b69895a786007bd232"},{"start":"4399.69","dur":"3.37","text":"Right, and that's why this straight line where it switches","_id":"2a6535cd905b40108a5058f4a693e232"},{"start":"4393.145","dur":"5.155","text":"Whereas in this lower half region would be predicting h of x equals negative 1.","_id":"cea70f03462d443181364c4d76190827"},{"start":"4388.435","dur":"4.71","text":"your classifier is predicting plus 1, right?","_id":"265fc712e7d7444bb44c2a2dbd8df970"},{"start":"4385.66","dur":"2.775","text":"And so in the- in this upper-right region, uh,","_id":"3c508145a01c448a81cbd6421a4b6198"},{"start":"4381.64","dur":"4.02","text":"here in this half plane w transpose x plus b is greater than 0.","_id":"628ad5e599d14c74ab58ce20a5e745e8"},{"start":"4378.22","dur":"3.42","text":"Because in the upper right half- half plane-","_id":"de709d31424c4dac93f14488418f5640"},{"start":"4371.275","dur":"6.945","text":"And so, um, your classifiers classify this example correctly, right?","_id":"5610a01629fe46dba9aef3fdb1e00976"},{"start":"4363.805","dur":"7.47","text":"And, uh, let's say it's a positive example, okay?","_id":"812b07ed1b744723b86d4dd5bfd7d0e8"},{"start":"4358.57","dur":"5.235","text":"So that's a training example, x_i, y_i.","_id":"8f4631a5e7cb490a82a5b942b7d4c334"},{"start":"4353.935","dur":"4.635","text":"Now, let's say you have one training example here, right?","_id":"a5eee47513094a70babe5c87a616bd6e"},{"start":"4349.21","dur":"4.725","text":"is where it'll predict y is equal to negative 1, okay.","_id":"ea5e48b4ed9a48b2a4dcedaaa30c15e0"},{"start":"4345.025","dur":"4.185","text":"that's where your cost high will predict y equals 1 and the lower left","_id":"bba2a929006c47808242eff76fea729f"},{"start":"4341.965","dur":"3.06","text":"And so given parameters w and b, the upper right,","_id":"af8a2b64702f4eea8a2700b847bf8a27"},{"start":"4338.38","dur":"3.585","text":"this equation w transpose x plus b equals 0, right.","_id":"b1e34712246f41839caef345ed6233da"},{"start":"4335.08","dur":"3.3","text":"And in between this- the straight line given by","_id":"ded033ef1c10415b84ec73bdb88657d9"},{"start":"4330.445","dur":"4.635","text":"And in this half, you'll have w transpose x plus b is less than 0.","_id":"b9611e779ac54945adbd444291f537ce"},{"start":"4326.545","dur":"3.9","text":"you'll have w transpose x plus b is greater than 0.","_id":"ee62d79317ce46d5a9aace12cdf9a985"},{"start":"4325.42","dur":"1.125","text":"in this half of the plane,","_id":"151bbcb1fdaa4ffa8a4ab3498a711cfe"},{"start":"4323.185","dur":"2.235","text":"and then half of this plane you know,","_id":"74bfb94be391431dbe884b3c389bfeb0"},{"start":"4320.305","dur":"2.88","text":"Uh, so the axes here are x_1 and x_2,","_id":"53183761b42f4f7cb601396bb110fbdd"},{"start":"4316.105","dur":"4.2","text":"wx plus b equals 0 defines the equation of a straight line.","_id":"85d894e6759a4839b478f217607c6916"},{"start":"4312.61","dur":"3.495","text":"defines a linear classifier and the equation,","_id":"0cc828f6b22c46ef965dd422460aa6f1"},{"start":"4309","dur":"3.61","text":"All right, so given parameters w and b that","_id":"734f3d01e03b47acab52efd45529e856"},{"start":"4301.9","dur":"5.62","text":"Which is, um- so let's see- let's say you have a classifier.","_id":"03a1bff6637647c6b0d8e52317134e46"},{"start":"4298.45","dur":"3.45","text":"geometric margin with respect to a single example.","_id":"251984162dfc4181b0844b9d26d3fc15"},{"start":"4296.935","dur":"1.515","text":"let's define the, uh,","_id":"f72630b7b91740518a94a9d7b16d8f5e"},{"start":"4293.89","dur":"3.045","text":"Um, so les- let's,","_id":"3661229990054f6792e213b6947858fd"},{"start":"4290.29","dur":"3.6","text":"the geometric and functional margin relate to each other.","_id":"d3e9326c094244638a997d8ff9ab274c"},{"start":"4289.03","dur":"1.26","text":"An- and you'll see in a second how","_id":"52a376c2ba804ee88abc9403c9e88fec"},{"start":"4287.275","dur":"1.755","text":"let's define the geometric margin.","_id":"9af4ea41ed1c4e899cef349658029b46"},{"start":"4284.995","dur":"2.28","text":"All right. So to find the functional margin,","_id":"394c40db7d70475b935d5184ddd8be65"},{"start":"4271.345","dur":"13.65","text":"in a little bit. Okay. [NOISE]","_id":"0a8f8787748c4d9e9fdb998365e4646b"},{"start":"4268.405","dur":"2.94","text":"Okay. So we'll come back and use this property,","_id":"06e869a11b7e42d0ad27c71dd951b873"},{"start":"4266.395","dur":"2.01","text":"right, and the classification stays the same.","_id":"c765b90f57004d23bd19cfe241a793cd"},{"start":"4259.74","dur":"6.655","text":"You can choose to replace this by w over 17 and b over 17 or any other number or any,","_id":"650b5c76074f42b187a81e2737edaf6d"},{"start":"4256.06","dur":"3.68","text":"any other values you want and- and it doesn't- doesn't matter, right?","_id":"f9e4733fdb314bacb9a8952e60459740"},{"start":"4253.39","dur":"2.67","text":"more generally you could actually scale w and b by","_id":"ffe40305cef6418e9d25e91712bd9d1f"},{"start":"4250.825","dur":"2.565","text":"Okay. Um, and in fact,","_id":"b3ccb49408904126ad3911028328357c"},{"start":"4246.64","dur":"4.185","text":"display of cheating on the functional margin.","_id":"6208c25f716f466f9759f04ebd455dc6"},{"start":"4245.005","dur":"1.635","text":"but that it prevents, you know,","_id":"c70e63b884d64200bbf0e901fd65e8d2"},{"start":"4243.46","dur":"1.545","text":"Ah, but, ah, but,","_id":"46c33a03c20b496e93334a788f310661"},{"start":"4241.96","dur":"1.5","text":"It's just rescaling the parameters.","_id":"2b51311b28d04ed28e3ad0cf00765a3b"},{"start":"4240.25","dur":"1.71","text":"and this doesn't change any classification,","_id":"390e77e69e2d41b0a9cd23b9977f4626"},{"start":"4235.959","dur":"4.291","text":"by the- by the Euclidean length of the parameter vector w,","_id":"6fb1724ad37f406d8752f08e6e9d0255"},{"start":"4233.2","dur":"2.759","text":"just the value of parameters through by the magnitude,","_id":"bca44cac436942fb8899a5303ee054df"},{"start":"4228.07","dur":"5.13","text":"over normal b and replace b with, [NOISE] right,","_id":"c9fd2f3c82da4220a90ae3ced5fea711"},{"start":"4223.33","dur":"4.74","text":"or another way to do that would be to take w and b and replace it with w","_id":"0ded64b5151a4e0ca5616c3d0b762d92"},{"start":"4220.63","dur":"2.7","text":"the normal w is equal to 1,","_id":"1e57c0c5d82744d2aad261e220ba68a0"},{"start":"4216.7","dur":"3.93","text":"So for example, hypothetically you could impose a constraint,","_id":"dbb158ac727e4368918e3c7587cecf18"},{"start":"4211.98","dur":"4.72","text":"um, would be to normalize the length for your parameters.","_id":"8d34977d8bef4f53a5c6ea5accd47411"},{"start":"4206.935","dur":"4.765","text":"replace, one thing you could do,","_id":"3f3018889d314d75bc9befdfdac9946d"},{"start":"4200.92","dur":"6.015","text":"Um, so one thing you could do is, ah,","_id":"29b98808cdba4ed18e027b8ea5321905"},{"start":"4197.29","dur":"3.63","text":"just to multiply all of your parameters by a factor of 10.","_id":"244df05e541d45fab6e0472beb2f36b4"},{"start":"4195.7","dur":"1.59","text":"It doesn't actually change any classification,","_id":"92df9a32b8be403585641a227aa5c069"},{"start":"4192.73","dur":"2.97","text":"but, ah, this doesn't actually change the decision boundary, right?","_id":"d5e12e83716f4b11a978114a02ae07f2"},{"start":"4189.58","dur":"3.15","text":"increased the functional margin of your training examples as 10x,","_id":"c10cfa3e32af4e7d876e4fe8ac9037da"},{"start":"4186.58","dur":"3","text":"can [NOISE] multiply all your parameters by 10 and then you've actually","_id":"dcac67609efc49758efe8f2bd9b88dac"},{"start":"4183.37","dur":"3.21","text":"the parameters by 2 or instead of 2 maybe you","_id":"82112a17d2f84ab196425b5bb6d67324"},{"start":"4180.58","dur":"2.79","text":"one way to cheat on the functional margin is just by scaling","_id":"813ab3d986b846dcb2ffdd023d4d03e7"},{"start":"4179.155","dur":"1.425","text":"Okay, so- so one,","_id":"385e0276ab774fdd866d76f53107171d"},{"start":"4176.26","dur":"2.895","text":"right, but you haven't actually changed anything meaningful.","_id":"a4f2e3964d1f42e2a9148ae584130598"},{"start":"4172.465","dur":"3.795","text":"here just multiplies by two and you've doubled the functional margin,","_id":"ff2ea80e51044b14b776009d0c9965ce"},{"start":"4169.09","dur":"3.375","text":"[NOISE] then, um, everything","_id":"b2bc2475181a4745b36f8234c3194144"},{"start":"4163.93","dur":"5.16","text":"you know, and multiply it by 2 and take b and multiply it by 2.","_id":"bcdc703a553f42349a6c0eeb387ae671"},{"start":"4160.07","dur":"3.86","text":"in regards to this formula is if you take w,","_id":"ff3183df2d7d40bf81bbbd9d7fc34425"},{"start":"4158.485","dur":"1.585","text":"And one thing you can do, um,","_id":"838fb1e66c454713868dcc3eb555a18a"},{"start":"4154.57","dur":"3.915","text":"really easy to cheat and increase the functional margin, right?","_id":"e09c4518545c43a6b08a9ba29135d0a5"},{"start":"4151.24","dur":"3.33","text":"about the definition of the functional margin is, it's actually","_id":"7c6675bd818c40ab83596241fb904a8d"},{"start":"4141.73","dur":"9.51","text":"Okay? [NOISE]. Now, one thing","_id":"8ddbbf89e05a4608b56d3924a617d572"},{"start":"4137.35","dur":"4.38","text":"the functional margin to be the functional margin of the worst training example.","_id":"7f24f4b1cc8e4f3f84d685b8fb1ffd06"},{"start":"4134.65","dur":"2.7","text":"we'll use this kind of worst-case notion and define","_id":"7ac4d495f1114cbc900425d5d991b4f7"},{"start":"4131.56","dur":"3.09","text":"that the training set is, um, linearly separable,","_id":"e6830ecacd58460e9597b8abbef65028"},{"start":"4129.505","dur":"2.055","text":"but because we're assuming, just for today,","_id":"560c47cb66824b1d863e042078a352ae"},{"start":"4126.82","dur":"2.685","text":"it on a straight line [NOISE] that will relax this later,","_id":"27e7df02ddb24ca187f1cc63ba4372c3"},{"start":"4124.21","dur":"2.61","text":"[NOISE] And that you can separate","_id":"be271c1361544cc1aa7ae9d5e533fd21"},{"start":"4122.89","dur":"1.32","text":"you know, it looks like this.","_id":"22065874eb3d43f38bfa6e391a0c8306"},{"start":"4121.24","dur":"1.65","text":"So we're gonna assume that the training set,","_id":"50afc5993e4b407283b9ff87f2ca5b2f"},{"start":"4117.67","dur":"3.57","text":"for today, we're assuming that the training set is linearly separable.","_id":"b61ad6f4b074428db9c6917142bce591"},{"start":"4113.395","dur":"4.275","text":"Okay, ah, this is a little bit of a plateau notion and we're for now,","_id":"a062c892d994475fbaa0c1e5078a0da6"},{"start":"4109.765","dur":"3.63","text":"how well are you doing on the worst example in your training set?","_id":"63febd86416d4f409a7ccda68f0f14e2"},{"start":"4105.79","dur":"3.975","text":"And we'll define the function margin with respect to the entire training set as,","_id":"685bbb57901644c180ed38b5030c4bae"},{"start":"4103.195","dur":"2.595","text":"which is how well are you doing on that one training example?","_id":"696b2baafd8c492c88ce136372c8f215"},{"start":"4099.175","dur":"4.02","text":"on the left we defined functional margins with respect to a single training example,","_id":"7ac0b42cf36f4bb28c86d520b52f0a25"},{"start":"4096.22","dur":"2.955","text":"but so this definition of a function margin,","_id":"7d545382a244408f96d76512acb97eb7"},{"start":"4093.265","dur":"2.955","text":"Okay. So, um, this is a worst-case notion,","_id":"42e4ebd1a6b44df49f0ffdc886083af3"},{"start":"4089.5","dur":"3.765","text":"where here i [NOISE] equals ranges over your training examples.","_id":"d057e022d0f34257a198f0fbfeec42c3"},{"start":"4084.67","dur":"4.83","text":"equals min over i of Gamma hat i,","_id":"826c82104ede4e999160e24ce3d5a783"},{"start":"4081.28","dur":"3.39","text":"the functional margin with respect to the training set to be Gamma hat,","_id":"4c90a649e6f847c7acfd80ea27da5ded"},{"start":"4080.47","dur":"0.81","text":"I'm gonna define","_id":"b1f970c8fbe44b3a9d5c9bf3f38d0c1e"},{"start":"4070.27","dur":"10.2","text":"[NOISE]","_id":"cb178f7f64504435841d827b7337bd63"},{"start":"4066.87","dur":"3.4","text":"So one other definition,","_id":"59431915aad945ee8476e36504e86942"},{"start":"4061.36","dur":"4.06","text":"the logistic regression case is either very close to 1 or very close to 0.","_id":"04aefa8abf9d418eaab147ae935ba9a0"},{"start":"4059.665","dur":"1.695","text":"the probability of output in","_id":"034a03dd081744808e4cb84b3dd5d1c3"},{"start":"4058.09","dur":"1.575","text":"then that means it, you know,","_id":"73bac16fdb3f46119f3242667486881b"},{"start":"4055.465","dur":"2.625","text":"And if it is much greater than 0 much less than 0,","_id":"c76fdf3239014ba598e3a12ca62a8cf2"},{"start":"4052.57","dur":"2.895","text":"a little bit below 0.5, probably 0 so that at least gets it, right?","_id":"1d21a22aa32942398aab3efc524f4e48"},{"start":"4049.99","dur":"2.58","text":"the prediction is at least a little bit above 0.5,","_id":"24ae9522c6b1428b806fb1cef89c7525"},{"start":"4046.78","dur":"3.21","text":"so if it is greater than 0 it means in- in the logistic regression case it means that,","_id":"304864e7836a43a3ac235ce9d9b152e2"},{"start":"4043.705","dur":"3.075","text":"And if- if much greater than 0 then it means, you know,","_id":"8c5f37af7248450b87f87fa18c11f6c7"},{"start":"4040.315","dur":"3.39","text":"this one example correct at least, right?","_id":"56d4c1fd33684c2481c5857987b39e55"},{"start":"4038.425","dur":"1.89","text":"And it means that the algorithm gets","_id":"fc7064bfcad84def9ac1ae0d2bd49773"},{"start":"4034.27","dur":"4.155","text":"this is less than 0 depending on the sign of the label.","_id":"6cbfa6995c5842848c216792a755fc12"},{"start":"4032.26","dur":"2.01","text":"ah, either this is bigger than 0,","_id":"478771f7bedf49218e431c91680625e6"},{"start":"4027.55","dur":"4.71","text":"so long as this Gamma hat i is greater than 0, it means that,","_id":"77172d5083034db1818d47759f7ce98a"},{"start":"4022.24","dur":"5.31","text":"[NOISE] Ah, so- so- so long as the, um, functional margin,","_id":"93e2a43195e24fc285dd1ccd180023c4"},{"start":"4015.985","dur":"6.255","text":"um, right, is equal to y_i.","_id":"73ae380e04904d6e89a825a4c27fc342"},{"start":"4013.54","dur":"2.445","text":"that means the algorithm,","_id":"db7612c3b15742fc9be138af715d971e"},{"start":"4008.85","dur":"4.69","text":"so long as Gamma hat i is greater than 0,","_id":"97a4c52a5d8c4895b02ef1488e8226b1"},{"start":"4000.58","dur":"5.98","text":"ah, one property of this as well is that, um,","_id":"194fb79f64544a6d84a9a51670bcd6de"},{"start":"3990.41","dur":"10.17","text":"[NOISE] And- and as an aside,","_id":"19b6e644cdd24c17a2183609c2401fb4"},{"start":"3989.075","dur":"1.335","text":"So we just hope that.","_id":"bdbfef38e39a4070bcfcac847937e882"},{"start":"3984.485","dur":"4.59","text":"Um, and so either way it's just saying that you hope this would be very large, okay?","_id":"07c334902d664ffdadc8d8e3aab26995"},{"start":"3982.82","dur":"1.665","text":"very large negative number.","_id":"0cf57e78d981470a836414373eabf7ca"},{"start":"3981.62","dur":"1.2","text":"you want this to be a very,","_id":"f7be2181afbe4d039325c655fb00a199"},{"start":"3979.955","dur":"1.665","text":"If y_i is negative 1,","_id":"bb1b6751b7f645eea945e06cbd633c23"},{"start":"3975.95","dur":"4.005","text":"uh, uh, and so y is equal to 1 you want this to be very, very large.","_id":"0e9ffef4fb9a4481ab8bcf023c037234"},{"start":"3971.69","dur":"4.26","text":"because y_i now is plus 1 or minus 1 and,","_id":"2733ce54849d499c9cb5325b332a4aa6"},{"start":"3968","dur":"3.69","text":"Gamma hat i is much greater than 0, right,","_id":"5582b2d0471e48eaa09327256cc3b340"},{"start":"3964.865","dur":"3.135","text":"these two statements together is basically saying that you hope that","_id":"cdd1fe1750264a2ab709d907fcd8d533"},{"start":"3961.24","dur":"3.625","text":"er, that, [NOISE] then, you know,","_id":"5872b98e7cd64e53861b561233a5dee5"},{"start":"3957.07","dur":"3.8","text":"right, and multiply it with,","_id":"f3b1db23b16a49259029cbf6ca0e685b"},{"start":"3953.615","dur":"2.335","text":"if you take y_i,","_id":"8a3540c1f1374985870ab0cdca919e63"},{"start":"3952.01","dur":"1.605","text":"combine these two statements,","_id":"9e7299de6abe4a10a5395ed1242cfd6f"},{"start":"3949.91","dur":"2.1","text":"Um, and if you, kind of,","_id":"2d6e53a64a9a4653bebb2e77e72ffcc1"},{"start":"3941.3","dur":"8.61","text":"[NOISE] Then we want or we hope that [NOISE] this is much smaller than 0.","_id":"dbf3e5c6133944c3addbac073e30395a"},{"start":"3938.21","dur":"3.09","text":"and that the label is equal to minus 1.","_id":"e10b127b2a4c42e490d29fd3fb7ec523"},{"start":"3936.335","dur":"1.875","text":"much greater than 0,","_id":"1a7fb5fb0640434ab31c033ab1ddda02"},{"start":"3929.3","dur":"7.035","text":"um, is that w transpose x_i plus b is greater than,","_id":"48c392a3892e482e8899c01de18acdf7"},{"start":"3919.805","dur":"9.495","text":"And so, um, so if y_i equals 1 then what we want or what we hope for,","_id":"f8e54a17920e41f9a35d67a18d863b52"},{"start":"3916.19","dur":"3.615","text":"our classifier to achieve a large functional margin, right?","_id":"035450bf5e1d4d719d98d0c01cdf2846"},{"start":"3913.685","dur":"2.505","text":"So really what we hope for is for","_id":"67cfbfe28735429cb2b5e88375d7cc09"},{"start":"3909.77","dur":"3.915","text":"um, you know, if y equals 1 we hope for that, if y equals 0, we hope for that.","_id":"1d8271bf4cbd4365869ae2e540a5b3cb"},{"start":"3905.645","dur":"4.125","text":"And so if you compare this with the equations we had up there,","_id":"50c9ac829c2749e6974ab89445c41f0d"},{"start":"3902.96","dur":"2.685","text":"we're going to define as this.","_id":"26af392649a34229901e421b6d3b69d7"},{"start":"3898.67","dur":"4.29","text":"functional margin of this classifier with respect to one training example,","_id":"06f168bf2c504041a3f9b336049bf30d"},{"start":"3896.75","dur":"1.92","text":"So this is linear classifiers, so its just, you know,","_id":"e894ac8e8282421db8076e4ca2a69f88"},{"start":"3895.535","dur":"1.215","text":"right, but in high dimension.","_id":"5823219e935f4de1ad08da3ecfe48e82"},{"start":"3893.165","dur":"2.37","text":"um, and hyperplane just means straight line,","_id":"f51557e183464c37b89a241260b1f3f8"},{"start":"3890.135","dur":"3.03","text":"We're going to write as this,","_id":"fb1168a4f21442ae96351539c6d602fb"},{"start":"3884.69","dur":"5.445","text":"a hyperplane defined by this with respect to one training example.","_id":"351d51f53b5846769f680ee427e1eaf5"},{"start":"3882.29","dur":"2.4","text":"Okay, so the functional margin of","_id":"567b85b7f1464fee95801bb02698fe5e"},{"start":"3855.605","dur":"26.685","text":"actually my hyperplane [NOISE]","_id":"4a2578472ef74b79bc5c02f60ab434a0"},{"start":"3851.465","dur":"4.14","text":"And so we're gonna say the functional margin of the,","_id":"80728b0e4cda4febb3760eb2e0e4f552"},{"start":"3848.285","dur":"3.18","text":"ah, ah, separating out the positive and negative examples.","_id":"ce7dd9228c3f4167a057f490e9b6917e"},{"start":"3844.25","dur":"4.035","text":"or in high dimensions it'd be a plane or a hyperplane that defines a straight line,","_id":"2ffd18372777434cb2a5092ad8ced4c5"},{"start":"3842.45","dur":"1.8","text":"Ah, but defines a line,","_id":"fd2758a506d944d1856b49b4ee8efb8d"},{"start":"3840.44","dur":"2.01","text":"re- really defines a hyperplane.","_id":"e0c2a3280b484aafb3ffee22edfd7eaf"},{"start":"3838.73","dur":"1.71","text":"defines the a- a, uh, uh,","_id":"3e23bbd83c9b4bc9b9016ee06b596f57"},{"start":"3835.415","dur":"3.315","text":"wh- what- the formulas we just wrote down the parameters w and b,","_id":"7c5074086745454bb86e248d3ea4c156"},{"start":"3834.185","dur":"1.23","text":"right, so you know,","_id":"ff81a9260e0f4c00934edd8ced205235"},{"start":"3829.72","dur":"4.465","text":"so the parameters w and b are defined as linear classifier,","_id":"b93ee325f85c4a2f97957683960a1078"},{"start":"3821.93","dur":"7.54","text":"[NOISE] So um, ah,","_id":"7e86c784c142403d92d2f790996a7bb8"},{"start":"3816.005","dur":"5.925","text":"All right. So let me formalize the definition of a functional margin.","_id":"bc4856b6c97f4159bcb6b1cd1f7fde64"},{"start":"3800.12","dur":"15.885","text":"[NOISE].","_id":"01df41d8bacf4cdb8459f38a28bd82ef"},{"start":"3792.89","dur":"7.23","text":"Since we've gotten rid of [NOISE] x_0.","_id":"57790fd34a1f4033af578c6995a1a43d"},{"start":"3788.87","dur":"4.02","text":"uh, w_i x_i plus b, right?","_id":"90b3017e374a46d297c43ab879d6cd2b"},{"start":"3782.195","dur":"6.675","text":"And so this term here becomes sum from i equals 1 through N,","_id":"5e6e9219405f44549e3ab29dfdbfaf77"},{"start":"3779.09","dur":"3.105","text":"And so um, uh, yeah, right.","_id":"1a56099de9b6441d8274ce7ddbb47e72"},{"start":"3772.145","dur":"6.945","text":"theta 0 which was previously multiplying to x_o, right?","_id":"336953903aa34c8b81dcdb179d99265b"},{"start":"3768.74","dur":"3.405","text":"So you just separate out the, the, the, uh,","_id":"4958a6312c534ea3bbad091572b23d95"},{"start":"3766.55","dur":"2.19","text":"and this is a new w. Okay?","_id":"c31d1a67d90849638e8b7097f40d65d4"},{"start":"3764.345","dur":"2.205","text":"then this is a new b,","_id":"93d2887f18fe40aebf28ebd25d1ec182"},{"start":"3762.65","dur":"1.695","text":"theta 2, theta 3,","_id":"c64a7a6f0b1247fb980ee2eaaf7197f3"},{"start":"3760.685","dur":"1.965","text":"you know, theta 0, theta 1,","_id":"49166bf83ed94615ae42f4869fe80e82"},{"start":"3759.35","dur":"1.335","text":"is if the parameters are,","_id":"e288da8565e1499c94227599a107b795"},{"start":"3757.565","dur":"1.785","text":"Um, and one way to think about this,","_id":"b19cd7f9dde547cfb5e3fe4d60b0fed6"},{"start":"3753.71","dur":"3.855","text":"So this is a standard notation used to develop support vector machines.","_id":"96885d6eaa7d4a3b82f949d221895c8e"},{"start":"3750.695","dur":"3.015","text":"So separate out w and b as follows.","_id":"ddb74039d79a43e39dee52bba6c3061e"},{"start":"3747.155","dur":"3.54","text":"and where dropping the x_0 equals 1 [NOISE] constraint.","_id":"7cd873e3885c4d199e980e8a2df5f001"},{"start":"3742.55","dur":"4.605","text":"And hypothesis applied to x will be g of this,","_id":"4ed1b3a299bb4079899cb0d974a1476c"},{"start":"3739.37","dur":"3.18","text":"the parameters of the SVM will be the parameters w and b.","_id":"c2ce2a1cd2434056ac90e443822631c2"},{"start":"3734.83","dur":"4.54","text":"Okay. Um, so for the SVM,","_id":"fd228f9478534a87a9cad641180c14bb"},{"start":"3714.245","dur":"6.745","text":"For the SVM, we will have h of, I'll just write this out.","_id":"b718f625194946a9a4cf342e3c8c3bfa"},{"start":"3708.2","dur":"6.045","text":"Where, uh, this was R N plus 1 with x_0 equals 1.","_id":"57c1287a470a4ad6a31591ff8986aa56"},{"start":"3697.1","dur":"11.1","text":"And finally, where previously we had for logistic regression, right?","_id":"7ade73bcc1bb42439f533d6d99b60098"},{"start":"3686.18","dur":"10.92","text":"[NOISE]","_id":"840bc803a4e24213980b56033dae349c"},{"start":"3680.12","dur":"6.06","text":"an abrupt transition from negative 1 to, um, plus 1.","_id":"78e4ddbcc51145b68679160f6ada4ec8"},{"start":"3678.89","dur":"1.23","text":"we have a hard transition,","_id":"1d45e3afc79b46ad98b1dd4be406a989"},{"start":"3676.85","dur":"2.04","text":"So instead of a smooth transition from 0 to 1,","_id":"872e3ae327844099b1feb1e6078b0e7b"},{"start":"3672.11","dur":"4.74","text":"and minus 1 otherwise, okay.","_id":"e5ace9b91fbf4011a9509b1078a4b97b"},{"start":"3667.235","dur":"4.875","text":"So output 1 if z is greater than equal to 0,","_id":"35ce39f5c4e8430ab3df295bd66633d6"},{"start":"3658.385","dur":"8.85","text":"And so, uh, g of z becomes minus 1 or 1, um, actually.","_id":"f8e5b4de6cdf4b9c9b60a6a6172018ab"},{"start":"3654.335","dur":"4.05","text":"the support vector machine will output either minus 1 or plus 1.","_id":"87e57cd4889a4271b78257a7a82a21d4"},{"start":"3650.555","dur":"3.78","text":"a probability like you saw in logistic regression,","_id":"21eb0266d9644581ac9341b41e4f4bbe"},{"start":"3647.47","dur":"3.085","text":"So rather than having a hypothesis output","_id":"6b4641ba06bb4e1d99da4fc3d447d146"},{"start":"3633.335","dur":"9.805","text":"And, um, we're going to have a H output.","_id":"55982966105542c98005bd1638c659ee"},{"start":"3627.575","dur":"5.76","text":"minus 1 and plus 1 to denote the class labels.","_id":"94563fc7f2db46f8a8fd2447009efd35"},{"start":"3625.475","dur":"2.1","text":"we're going to use, um,","_id":"db0922a703fd4ddc96224a0d292994da"},{"start":"3622.895","dur":"2.58","text":"So when developing SVMs,","_id":"584fac9ae57f42fda11a4160eebb7994"},{"start":"3620.825","dur":"2.07","text":"makes then the math a little bit easier.","_id":"92e683d22bb64eff85ccb12547639fbb"},{"start":"3618.275","dur":"2.55","text":"using slightly different notation to describe them,","_id":"7c4b1a829e43402193406cacd474e6f7"},{"start":"3614.585","dur":"3.69","text":"You know, because these algorithms have different properties, um,","_id":"3cbe0228f4e64f01840606670760a0db"},{"start":"3612.365","dur":"2.22","text":"I'm going to change the notation a little bit again.","_id":"3fcc2cf74b2445e7bbb966136c41454b"},{"start":"3609.98","dur":"2.385","text":"um, in order to develop SVMs,","_id":"33c2e36e5669471e827d51eb21bba587"},{"start":"3602.915","dur":"7.065","text":"So, um, [NOISE] now,","_id":"3676f59f7e7a4392bf9cdab08a5f9b72"},{"start":"3598.69","dur":"4.225","text":"the green line to classify these examples, okay?","_id":"d83bf661fe994eada021d5322ed7a7ae"},{"start":"3595.315","dur":"3.375","text":"is pose an optimization problem to try to find","_id":"ae908098c489436cac2013a1ec51fccf"},{"start":"3593.26","dur":"2.055","text":"also called the optimal margin classifier,","_id":"ae13dc0ccc634986b2cf59c4a25b3acf"},{"start":"3590.98","dur":"2.28","text":"what the SVM and low-dimensional spaces will do,","_id":"36d785e46c3549019bcaaecc5502ca26"},{"start":"3588.49","dur":"2.49","text":"So what the rudimentary SVM does,","_id":"22f1206d957b4ef6a8d94b4818ef1f70"},{"start":"3585.49","dur":"3","text":"the algorithm that tries to maximize the geometric margin.","_id":"d3d58f3cb977459394f109ba56e38e96"},{"start":"3583.04","dur":"2.45","text":"I guess the optimal margin classifier which based in","_id":"1a2be92538c548d7a7792896aa59abaf"},{"start":"3581.345","dur":"1.695","text":"and it will pose the, the,","_id":"173c4b0cea3d4c04aff43091dfdbe9f4"},{"start":"3578.93","dur":"2.415","text":"formalize definition geometric margin,","_id":"8f80211d1bec46bb9fe66c3e28e8a29a"},{"start":"3576.08","dur":"2.85","text":"next 20 minutes is formalize definite functional margin,","_id":"a5e02ecf97e04ce4acd75c6cc131fa17"},{"start":"3572.75","dur":"3.33","text":"uh, next several, I guess in the next, next,","_id":"c3c348880fd9487980460070feac36e7"},{"start":"3567.155","dur":"5.595","text":"Okay. Um, and so what I'd like to do in the,","_id":"b5478184858946c789d0190a61945f93"},{"start":"3563.24","dur":"3.915","text":"from the trained examples even as it separates them.","_id":"d7813e3690f44698a3fbe3281f70c0b3"},{"start":"3560.09","dur":"3.15","text":"But there's a much bigger geometric margin meaning a physical separation","_id":"5a6da92641e84708a9802bca535c27ee"},{"start":"3558.41","dur":"1.68","text":"uh, which is called the geometric margin.","_id":"0ab6a466eec94fe5b655a60fd738e05a"},{"start":"3553.15","dur":"5.26","text":"the green line has a much bigger separation,","_id":"c81a7a50a873417f8f573e66f5b7dd0c"},{"start":"3550.72","dur":"2.43","text":"perfectly separate the positive and negative examples,","_id":"ec4e8d91b4cb481c8aa9891ba078173a"},{"start":"3545.56","dur":"5.16","text":"So even though the red line and the [NOISE] green line both, you know,","_id":"53a2120e0e304a8d868aa94efef17d4f"},{"start":"3542.71","dur":"2.85","text":"Just has a much bigger distance from the positive and negative examples.","_id":"91fc64931df241bc82ad287c76a46a31"},{"start":"3538.07","dur":"4.64","text":"you know, has a much bigger separation, right?","_id":"7807da8c399e4536bf5626b5909687ff"},{"start":"3534.635","dur":"3.435","text":"whereas the green line,","_id":"c91355df456544489745c4cab016e9f0"},{"start":"3528.41","dur":"6.225","text":"Well, the red line comes really close to a few of the training examples,","_id":"e9236a6694bd477bae6dff44b5813802"},{"start":"3526.19","dur":"2.22","text":"So, uh, why is that?","_id":"e0e1b8b7d961418a8c6ba61048bffe73"},{"start":"3521.9","dur":"4.29","text":"But somehow the green line looks much better than the red line, okay?","_id":"af3e57f5a95c47bdbd681b9ee539221f"},{"start":"3519.89","dur":"2.01","text":"that also separates the positive negative examples.","_id":"c45373622eb74c42962950ecbac911f7"},{"start":"3513.77","dur":"6.12","text":"[NOISE] Um, that's another decision boundary in red,","_id":"6337f6ce34b14811a92bdc4c48344b93"},{"start":"3511.265","dur":"2.505","text":"for separating the positive [NOISE] and negative examples.","_id":"eb4f683bebb24f498e7f78211a720fb0"},{"start":"3509.13","dur":"2.135","text":"that seems like a pretty good decision boundary","_id":"c16e5fe0c6b548738f38c3d9f95e27a4"},{"start":"3500.84","dur":"8.29","text":"[NOISE]","_id":"d1639cb35f74433eb257ef29f40f6383"},{"start":"3499.61","dur":"1.23","text":"Now,","_id":"37ca23ce38644484a9e3bf4f338a61df"},{"start":"3498.32","dur":"1.29","text":"[NOISE]","_id":"5c1099f1113445879747ff9707cb1111"},{"start":"3495.82","dur":"2.5","text":"So let's say that's the data set.","_id":"c8a4e244af0f4056ab9811c2c267be35"},{"start":"3489.13","dur":"3.32","text":"Okay. Um, right.","_id":"458a66bbb30e45f2b06d1371dcb0964f"},{"start":"3485.57","dur":"3.01","text":"is linearly separable.","_id":"93ae5d38a77547bab9081f2239d14379"},{"start":"3483.695","dur":"1.875","text":"And for now, let's assume the data is,","_id":"9f4c440ab44f498db26a85816d415902"},{"start":"3478.775","dur":"4.92","text":"which is called the geometric margin and that's the following.","_id":"270fc611eb4a458399b15ab0aa91d02a"},{"start":"3475.625","dur":"3.15","text":"there's a different thing we'll define in a second","_id":"7cdd19e25f1547ad8b280237902c38ac"},{"start":"3472.04","dur":"3.585","text":"Um, so looking ahead a little bit,","_id":"c65258b63f154d34a68ecd103afeabe5"},{"start":"3467.54","dur":"4.5","text":"it means that these two statements are true, right?","_id":"5b565ad7193049ecbc2206f5bad8def2"},{"start":"3461.675","dur":"5.865","text":"uh, captures [NOISE] this idea that if a classifier has a large functional margin,","_id":"7d91ba8b5cdf4f7397bddb3f5dfac4f0"},{"start":"3458.27","dur":"3.405","text":"So the functional margin which we'll define in a second,","_id":"8092feacb01a41bba98d3f1052acf31c"},{"start":"3449.44","dur":"8.83","text":"So, um.","_id":"c2509a421b81457fa7630895e5b9bf23"},{"start":"3445.82","dur":"1.72","text":"Okay.","_id":"bec0d1f2f70f47c18e142e008a46c68b"},{"start":"3442.91","dur":"2.91","text":"then the algorithm is doing very well on this example.","_id":"651781c7f78c41cd9c7c31cb6de2a8f6"},{"start":"3441.515","dur":"1.395","text":"Because, uh, if this is true,","_id":"0029e39773124977b04a0e026d51b91b"},{"start":"3436.37","dur":"5.145","text":"is that theta transpose xi is much less than 0, right?","_id":"1be44af8fcd94c7db3b0fa359928e47d"},{"start":"3433.82","dur":"2.55","text":"then what we want or what we hope,","_id":"7eff06945a3a494ca50bf510e15d41a6"},{"start":"3429.35","dur":"4.47","text":"Um, and if y_i is equal to 0,","_id":"574155c8baa842ff80c1cfd05f19e3dd"},{"start":"3427.595","dur":"1.755","text":"That, that equals 1.","_id":"ce5137b54ef84ce0856a669019fc0f94"},{"start":"3424.505","dur":"3.09","text":"Very correct and confident prediction, right?","_id":"78e1b323c58e458192bc78620c3b7e91"},{"start":"3421.85","dur":"2.655","text":"it's giving a very good, very accurate prediction.","_id":"f11445a892844220883ec514dbef2864"},{"start":"3416.675","dur":"5.175","text":"then g of theta transpose x will be very close to 1 which means that is,","_id":"9ac95939a6754fdda691bf34fc6655e8"},{"start":"3412.985","dur":"3.69","text":"And if indeed theta transpose x is much greater than 0,","_id":"adf8c41e8bbb4881a076227c911e5b2c"},{"start":"3411.65","dur":"1.335","text":"very close to 1.","_id":"b407340dbadb47eea6eef218f4f0981f"},{"start":"3410.03","dur":"1.62","text":"So the output probability is very,","_id":"b65c6f7b771243a2b340ad0f4cb3d70c"},{"start":"3408.575","dur":"1.455","text":"Will be faster there, right?","_id":"e8303474609740ec918dab6027360f39"},{"start":"3404.585","dur":"3.99","text":"hopefully theta transpose x, right?","_id":"4a6b418b80ad497a8277e4d87e9eb121"},{"start":"3402.155","dur":"2.43","text":"then if the algorithm is doing well,","_id":"23be8e3b2dab4cccbc26d107dfe94284"},{"start":"3397.95","dur":"4.205","text":"Um, uh, because if the true label is 1,","_id":"0d4a4cc7e2c6481ca754f3991141dee7"},{"start":"3396.2","dur":"1.75","text":"it means much greater, right?","_id":"f77ac2c0830b412fa2d5e78ffeb6d8bd"},{"start":"3393.98","dur":"2.22","text":"Uh, this double greater than sign,","_id":"a83936072a36472a9456424e0ecf74e5"},{"start":"3387.17","dur":"6.81","text":"theta transpose x_i is much greater than 0.","_id":"5e40e41de6af4a779e4bdfb512b1bc1e"},{"start":"3381.62","dur":"5.55","text":"Then hope or we want that","_id":"17b307a2c08c4ffab41facd96be3a30f"},{"start":"3375.155","dur":"6.465","text":"this means that if y_i is equal to 1, right?","_id":"644d96012d46445180b325103ce37bb9"},{"start":"3371.87","dur":"3.285","text":"So in other words,","_id":"0a8fb48e1a8149ac8add75dee4329a3f"},{"start":"3366.77","dur":"5.1","text":"logistic regression output 1 or 0 rather than output a probability, right.","_id":"20ca12bbb6264d6b851af7525db82c1a"},{"start":"3363.725","dur":"3.045","text":"Okay. So this is what will happen if you have, um,","_id":"29b02cf65276425a84b29347b880dbd6"},{"start":"3361.79","dur":"1.935","text":"then you predict that this class is 0.","_id":"dc36c216677a409eb4f6582d6a58e8f2"},{"start":"3358.925","dur":"2.865","text":"And if theta transpose x is less than 0,","_id":"a96b0c7b9abc4ed4a980d91b781aa058"},{"start":"3355.22","dur":"3.705","text":"a class  being 1 is greater than 50/50, and so you predict 1.","_id":"4025b9131c6d4519aa5cc1413831a63f"},{"start":"3352.385","dur":"2.835","text":"meaning that the upper probability- the estimated probability of","_id":"621d1a8a886a4a9eac63452a300d627f"},{"start":"3346.925","dur":"5.46","text":"Um, and so you predict 1 if theta transpose x is greater than equal to 0,","_id":"ad0b1d7a1fc84d75b20fa21a61b5d63a"},{"start":"3345.185","dur":"1.74","text":"it doesn't really matter what you do.","_id":"90291ab2dd6b40f1a4c9769edb677765"},{"start":"3343.46","dur":"1.725","text":"It is, it's exactly 0.5,","_id":"c6d5abf9b32c46e0b1b362e6065b0af2"},{"start":"3340.7","dur":"2.76","text":"and you can have greater than or greater than equal to, it doesn't matter.","_id":"f881ce171b13414b9a772f83adcc6ce9"},{"start":"3334.535","dur":"6.165","text":"um, g of theta transpose x is greater than 0.5 [NOISE],","_id":"1c77f33628eb4f7da0ab576a66b8d1a2"},{"start":"3329.9","dur":"4.635","text":"Okay. Because theta transpose x greater than 0, this means that,","_id":"d7d9c059469242d2846bbeb51548631d"},{"start":"3323.522","dur":"6.378","text":"Um, and predict 0 otherwise.","_id":"990cf3d8a25045d3b0c71ba457b730e9"},{"start":"3317.47","dur":"6.052","text":"If theta transpose x is greater than 0, right?","_id":"2c1309c69d1147988b0d1d30dfec6b32"},{"start":"3312.08","dur":"4.09","text":"then what this classifier will do is, uh, predict 1.","_id":"9369d0d8ff904b128d739f43e5e2d8c2"},{"start":"3307.475","dur":"4.605","text":"if you have this algorithm predict not a probability but predict 0 or 1,","_id":"3039a4ee7c6540fbad013fc927c676b9"},{"start":"3301.775","dur":"5.7","text":"And so, um, if you turn this into a binary classification, if, if,","_id":"5705802c49bc4626acef7a4b9b27105d"},{"start":"3298.835","dur":"2.94","text":"equals the logistic function of pi to theta transpose x.","_id":"37310320e0c043079f76c7877352ab56"},{"start":"3296.57","dur":"2.265","text":"So this, this is a classifier H of theta","_id":"826256e48b7c4d79b36a9be82e7a3484"},{"start":"3290.42","dur":"6.15","text":"So, so let's, let's start by motivating this with logistic regression [NOISE].","_id":"a0f8508096734ba983828c9610f5d92f"},{"start":"3287.255","dur":"3.165","text":"and we're gonna use logistic regression, right?","_id":"1aedec75ddc54b8485e65ca28780e5cc"},{"start":"3283.865","dur":"3.39","text":"Uh, we're gonna go to binary classification,","_id":"952dbc7dd2fa400faca294b72966401d"},{"start":"3282.14","dur":"1.725","text":"Um, so here's what I mean.","_id":"61e7b28d16c1471692058fb1ef1d80bf"},{"start":"3278.105","dur":"4.035","text":"how confidently and accurately do you classify an example.","_id":"d33a87cb55274c288bc4d0d1c954af02"},{"start":"3274.265","dur":"3.84","text":"the functional margin of the classifier is how well- how,","_id":"7f4c1138d20e4de3b056831ea50b6043"},{"start":"3272.015","dur":"2.25","text":"which is, uh, informally,","_id":"576095513fab4d2b8e3b83b376cad2ff"},{"start":"3266.885","dur":"5.13","text":"So, um, first, let me define the functional margin,","_id":"bcd25bb534cf4b80a31a067621970129"},{"start":"3260.99","dur":"5.895","text":"[NOISE]","_id":"dadbc138a3124b41b88a778a34508e6e"},{"start":"3254.52","dur":"6.47","text":"All right so let's start developing the optimal margin classifier.","_id":"c899c307d46f4fc68da11e3f7a58d50b"},{"start":"3247.75","dur":"5.65","text":"support vector machines and Naive Bayes I think do get used and are important.","_id":"902f0037f0e440319b9d1c18f3adcc2a"},{"start":"3244.2","dur":"3.55","text":"Right. But still there, there are all these other techniques that including","_id":"5853bc66b9c54440866859ad2680df4f"},{"start":"3241.465","dur":"2.735","text":"Which- which is totally not a neural network technique.","_id":"d24a1156c49549d09675015eb9a446ae"},{"start":"3238.66","dur":"2.805","text":"Where we're gonna use factor analysis or something very similar to it.","_id":"c93b8216f9874e2aa3a02256a1ae0c08"},{"start":"3235.209","dur":"3.451","text":"uh, that one of my teams is working on in manufacturing.","_id":"12a0915648764ea5bcadf4350b9a1cca"},{"start":"3231.56","dur":"3.649","text":"right, there's an unsupervised learning algorithm and there's an application,","_id":"7840c01f1f3546c89c0e5116c696adb5"},{"start":"3228.8","dur":"2.76","text":"about factor analysis which we'll learn about later in CS229","_id":"b488d6eb083246a480f8360bb90ded54"},{"start":"3223.97","dur":"4.83","text":"uh, and so yeah and then late last night I was talking to an engineer, uh,","_id":"eb55a7fdf3b143f6af4ba16989373563"},{"start":"3220.58","dur":"3.39","text":"you know but- but they're not- they're not the only thing in the world,","_id":"1039537222bd42bcb3969d5162502e8f"},{"start":"3218.75","dur":"1.83","text":"I knew that's like- I loved that,","_id":"3d6cfa001baa499b900dc7459e818c16"},{"start":"3212.99","dur":"5.76","text":"some way there's quite disproportionate to what I find useful, you know,","_id":"5d7b1218e840426eafdb2a922a3ee528"},{"start":"3208.535","dur":"4.455","text":"It's just that deep learning attracts the attention of the media in","_id":"400686f539e74ecfb5a972fc9969ebca"},{"start":"3205.745","dur":"2.79","text":"We actually use many, many tools in machine learning.","_id":"a81cfcf7b58848d29598b675df9a084a"},{"start":"3202.37","dur":"3.375","text":"So- so we do not live in a neural networks only world.","_id":"49587a9e31dc48b98cdeaa9f28b9c23a"},{"start":"3199.67","dur":"2.7","text":"is actually much wider than neural networks and deep learning.","_id":"456e5bf8484341b09ba01947802e6bb8"},{"start":"3196.315","dur":"3.355","text":"Uh, the set of algorithms is actually used in practice,","_id":"2d222d87f3c34f2087c2d099de8650ec"},{"start":"3192.01","dur":"4.305","text":"But if you look at what actually happens in practice in machine learning.","_id":"dfe93739e0254eecb0fde28f6d9852d4"},{"start":"3188.905","dur":"3.105","text":"And you'll hear about neural networks and deep learning a little bit later in this class.","_id":"139291d941b54d3eaeee7431b227f2c9"},{"start":"3187.06","dur":"1.845","text":"you know, neural networks all the time, right?","_id":"931da4ba121c4c879c526d1b15e5ac01"},{"start":"3183.52","dur":"3.54","text":"the media talks a lot about machine learning, the media just talks about,","_id":"bffc8b45917642e39bb1e822ef6897fc"},{"start":"3180.41","dur":"3.11","text":"I think that if you read in the news","_id":"713a922ab482448f972ed804a6052bde"},{"start":"3177.215","dur":"3.195","text":"you know, th-the machine learning world has become a little bit funny.","_id":"4950d04228ee4cedbe04b81899b7a2ef"},{"start":"3173.62","dur":"3.595","text":"So [NOISE] and by the way I,","_id":"efab6b5983304b84a091f37cc79804a2"},{"start":"3155.54","dur":"6.52","text":"this next, uh, Monday okay.","_id":"e561f08e72d949c3aca9dc43758a3d21"},{"start":"3150.29","dur":"5.25","text":"[NOISE] So we're gonna do this today and then","_id":"9ddcbea70cad46aab0b482b0a5552212"},{"start":"3147.59","dur":"2.7","text":"uh, we'll talk about the inseparable case.","_id":"e2eb3c109f7849e1a6bd1d1f26237f9f"},{"start":"3144.515","dur":"3.075","text":"Okay, um, and then finally,","_id":"428e1bd1b7c9413695f64be0aecdd690"},{"start":"3139.265","dur":"5.25","text":"because the kernels will allow you to choose an infinitely large set of features.","_id":"5d1f7056c0a94223b6819178cfad4901"},{"start":"3136.43","dur":"2.835","text":"So you just don't have to fiddle with these features too much","_id":"6a179f7e2e5e488380d99658da1ef17a"},{"start":"3133.52","dur":"2.91","text":"or maybe X 1, X 2 to the power of two thirds.","_id":"4507a2e3b1a74a409794449bc2cc6b17"},{"start":"3130.82","dur":"2.7","text":"right, like do you want to have square root of X 1","_id":"f2907172cf254b439f72a3377c9ea6ef"},{"start":"3128.06","dur":"2.76","text":"from a lot of the burden of manually picking features,","_id":"05803da99d1b4c9a9b977fe0274e1fe5"},{"start":"3125.285","dur":"2.775","text":"And, um, what this does is it relieves us","_id":"cd3e3d19a7ac45ac8c33fea4d2dc7564"},{"start":"3122.12","dur":"3.165","text":"into maybe infinite dimensional set of features.","_id":"fcc081807ec14b46a08228b383249edc"},{"start":"3119.87","dur":"2.25","text":"this two-dimensional feature vector space","_id":"821103b2b39148019b866ed893d2fd4a"},{"start":"3117.695","dur":"2.175","text":"medical conditions you're trying to predict and map","_id":"aaf0c363a7954c35ac5825b2ca5fe2e6"},{"start":"3116.3","dur":"1.395","text":"For, uh, you know,","_id":"3588c74bfde7488c9bdaf738dcc72a16"},{"start":"3111.815","dur":"4.485","text":"our original set of features that you are given for the houses you're trying to sell.","_id":"ff0a328e44c84826973ec9dd72049e05"},{"start":"3107.975","dur":"3.84","text":"Um, and so with the kernel formulation we're gonna take","_id":"d99d6b1be0b04d91bef6e6168809f77c"},{"start":"3101.645","dur":"6.33","text":"It might be R100,000 or it might even be R infinite.","_id":"00f84ea9bd0344a293d3b75b93aa6561"},{"start":"3098.6","dur":"3.045","text":"this high dimensional set of features may not be R 5.","_id":"75944b13305944fdbd984050afb36ea2"},{"start":"3096.26","dur":"2.34","text":"And- and the cool thing about kernels is that","_id":"1ff7c77e21af4ce1b3964f14d4b95b6b"},{"start":"3090.95","dur":"5.31","text":"right, and then train an algorithm on this high dimensional set of features.","_id":"f0cc0d4c9e844ce0ac62a4ea8916a602"},{"start":"3087.56","dur":"3.39","text":"In our example there that was R 5,","_id":"74679555913b4006a34602f2e49d869f"},{"start":"3080.93","dur":"6.63","text":"right, and map it to a much higher dimensional set of features.","_id":"f51db168370d4d879d4def15437c9dac"},{"start":"3075.71","dur":"5.22","text":"Is, um, how do you take a feature vector x, maybe this is R 2,","_id":"bd19225ec2424628ac81fbcc1368c745"},{"start":"3071.66","dur":"4.05","text":"And the kernel idea is one of the most powerful ideas in machine learning.","_id":"d6658607c1d84a929c78f609132b53d4"},{"start":"3067.595","dur":"4.065","text":"which is next Monday is an idea called kernels.","_id":"ad5835ca86784a388245a02c455d7969"},{"start":"3065.945","dur":"1.65","text":"excuse me, next Monday,","_id":"5aaab9d9220b457da3d3e161dc60d7f0"},{"start":"3061.505","dur":"4.44","text":"And then what you'll see on Wednesday is, um,","_id":"4a7833c34f064942b0b04b8502c6912e"},{"start":"3059.495","dur":"2.01","text":"Um, so we'll do that today.","_id":"04f2fed1367246e982669f3f6b05aa22"},{"start":"3055.1","dur":"4.395","text":"for training sets like this that we assume for now can be linearly separated.","_id":"a9abb78187384d328405fbfb674a100d"},{"start":"3051.605","dur":"3.495","text":"in important ways that to find a linear classifier","_id":"0ef000f411c8496c917ee11dbdc8170c"},{"start":"3047.63","dur":"3.975","text":"logistic regression but that allows us to scale, uh,","_id":"81ba040a035442a7a3731154efdf8cc0"},{"start":"3045.485","dur":"2.145","text":"that' ll be- that will have some similarities to","_id":"6069d61d9aca4de88c003328b110739d"},{"start":"3041.315","dur":"4.17","text":"and, uh, we'll first derive an algorithm, uh,","_id":"ae58d3a36ff84fac8b1987775bf9fc90"},{"start":"3038.87","dur":"2.445","text":"the basic building block for the support vector machine,","_id":"61157862572f414a80d2cafe1bd4bbb9"},{"start":"3035.844","dur":"3.026","text":"Right, and so the optimal margin classifier is","_id":"3308233453044bb6a6fecdf1ea4f7a2d"},{"start":"3030.76","dur":"5.084","text":"um, that we assume look like this and that are linearly separable.","_id":"d5b1742257cb4b089fee6ff0cadaba75"},{"start":"3026.36","dur":"4.12","text":"and what that means is going to start off with datasets,","_id":"937a47b704db4cd9aa1a2574e63600c7"},{"start":"3020.945","dur":"5.415","text":"we'll start with the separable case","_id":"b36c8c4b0e5c4016865d6545e90d98e8"},{"start":"3012.335","dur":"8.61","text":"We talked about the optimal [NOISE] margin classifier today, and, uh,","_id":"80a4af0157694c3a95050ffa77359c78"},{"start":"3008.3","dur":"4.035","text":"uh, we're going to develop the following set of ideas.","_id":"0a9c6c13048945b9902fad4d3e9c0448"},{"start":"2997.595","dur":"10.705","text":"Okay, um so the road map is,","_id":"26ee97211f9843f7b6be208b13818757"},{"start":"2993.23","dur":"4.365","text":"many parameters like the learning rate and other things that you had to fiddle with.","_id":"02ed7d2f409146f3b134e435fe219375"},{"start":"2990.74","dur":"2.49","text":"You kind of just turn the key and it works and there isn't as","_id":"52eb0fcce4aa428a9d8b84f228eadfcc"},{"start":"2985.47","dur":"5.27","text":"But, um, uh, but one great property of support vector machines is- is- is turn key.","_id":"c92c6648d2ca4b0c950fb8e5d3218571"},{"start":"2980.94","dur":"4.53","text":"support vector machines are not as effective as neural networks for many problems.","_id":"388e095d0379477a90d36f6a4cbd552b"},{"start":"2977.43","dur":"3.51","text":"Um, so I think in the grand scheme of things today I would say","_id":"3c14f06e42194eae83387e84094d123d"},{"start":"2973.83","dur":"3.6","text":"kind of, converge without you having to worry too much about the details.","_id":"e04ed5ee5faf4726b1a3e84db423347c"},{"start":"2970.935","dur":"2.895","text":"you know, on a problem and you just run it and the algorithm will,","_id":"0ac7ce3ef689423fa23b92c80623b071"},{"start":"2967.53","dur":"3.405","text":"can just download to train the support vector machine on- on any on,","_id":"aaebce07884445eeaae2595528c7abd0"},{"start":"2964.185","dur":"3.345","text":"robust, very mature software packages that you","_id":"203fdfe07e434af6b16c42990be70ea8"},{"start":"2959.15","dur":"5.035","text":"Um, support vector machine today has a very, uh,","_id":"eddb48b492364da4b2447ca536fdf95c"},{"start":"2955.49","dur":"3.66","text":"We`ll try a few values and hope you didn't mess up how you set that value.","_id":"4834a69837d943068bca89b5b502852d"},{"start":"2953.825","dur":"1.665","text":"And that's just another thing to fit in with.","_id":"54a0a6f6ee484a27aeab555e962761d3"},{"start":"2951.26","dur":"2.565","text":"uh, tune the learning rate sorry, tune the learning rate alpha.","_id":"b0d2ea41701b41d3924359faaf2e84a2"},{"start":"2947.8","dur":"3.46","text":"You know you might have to tune the gradient descent parameter,","_id":"d16ca521bd664471a9cb6ce62bc8de57"},{"start":"2943.65","dur":"4.15","text":"Uh, even for logistic regression or for linear regression.","_id":"95a3d4d12b2f43bca050c3c7dfd1d176"},{"start":"2939.75","dur":"3.9","text":"And what I mean by that is it doesn't have too many parameters to fiddle with.","_id":"7d3b492e2567423995f844c65a4b5d0c"},{"start":"2935.385","dur":"4.365","text":"support vector machines are used today is- is a relatively turn-key algorithm.","_id":"2b4e3e677a8d43039c73b566177a3f57"},{"start":"2933.15","dur":"2.235","text":"one of the- actually one of the reasons, uh,","_id":"62f285545b75448db3760037c0d99e2f"},{"start":"2931.14","dur":"2.01","text":"uh, you know, a support vector machine,","_id":"87fd9e45626c42edb3acc09a46a18261"},{"start":"2928.98","dur":"2.16","text":"Okay. Um, and I think,","_id":"572ec8f0805e490da2465626bd191173"},{"start":"2923.52","dur":"5.46","text":"But different in details that allows you to learn very non-linear decision boundaries.","_id":"0f231d4ef96e4fb0b3ba15d824b0e30c"},{"start":"2921.105","dur":"2.415","text":"uh, in a way similar to logistic regression.","_id":"1448e2882dad418c97aa13fd6d2c92c5"},{"start":"2918.27","dur":"2.835","text":"Uh, and then apply a linear classifier,","_id":"1b14270f6a454280ab1008cf36220773"},{"start":"2913.605","dur":"4.665","text":"map them to a much higher dimensional set of features.","_id":"de4d7cf9fb0d4a84938e00c358154091"},{"start":"2908.37","dur":"5.235","text":"will be able to derive an algorithm that can take say input features X 1, X 2,","_id":"da0d85c65b3341fa8f07e3ea0300ffd5"},{"start":"2903.33","dur":"5.04","text":"Um, and what we will see with support vector machines is that we","_id":"7d097b36957e4513a5c856fbbd0186be"},{"start":"2900.405","dur":"2.925","text":"Rather than just an ellipse and more complex as your boundary.","_id":"88abb89cb0224443912f688de7672794"},{"start":"2895.38","dur":"5.025","text":"uh, set of features could get you a decision boundary like that right.","_id":"a1c960d24361495085202f132a88d3ad"},{"start":"2894.285","dur":"1.095","text":"you know, type of a,","_id":"553beffd417b4332b25efbca6b439d01"},{"start":"2891.645","dur":"2.64","text":"What I- I- I actually don't know what,","_id":"7ef908125e8d4ce6bec4617c94406dc1"},{"start":"2888.54","dur":"3.105","text":"these features is little bit of a pain right. I- I- I don't know.","_id":"5ff85df744bd460388c43fe7aeed3e46"},{"start":"2885.69","dur":"2.85","text":"Um, but randomly choosing","_id":"baa253352f9e4ae3ac8901256b0afa15"},{"start":"2882.81","dur":"2.88","text":"This is- there's a- there's a shape of an ellipse, right.","_id":"c43ee69a5ee8418e9c5a9c77816e32c7"},{"start":"2881.04","dur":"1.77","text":"and you actually learn the decision boundary.","_id":"12acdefd05ce40309e55e19613cae4b5"},{"start":"2878.711","dur":"2.329","text":"Uh, with these other features it's just regression","_id":"3fee8938f1e747e6af47eab4976a3ef3"},{"start":"2875.1","dur":"3.611","text":"then logistic regression can learn non-linear decision boundaries.","_id":"a3f393439c704d3bbfd450f04b4846ef"},{"start":"2871.08","dur":"4.02","text":"logistic regression to this augmented feature vector, uh,","_id":"e129dab1e65d481784761aa8341b907f"},{"start":"2868.365","dur":"2.715","text":"it turns out if you do this and then apply","_id":"cbf6447a02c9443a96f934c5a2fd0d10"},{"start":"2863.01","dur":"5.355","text":"x. That- that has these high-dimensional features right, now, um,","_id":"b8954c8d95614878a072588412edd5c3"},{"start":"2860.01","dur":"3","text":"And have a new feature vector which we would call phi of","_id":"9ae1e30bbc524f5fb153f80aceffcf7c"},{"start":"2858.555","dur":"1.455","text":"X 2 cubed and so on.","_id":"544ffac6d5e8469ba77d3329290516df"},{"start":"2854.58","dur":"3.975","text":"X 2 squared X 1, X 2 maybe X 1 cubed,","_id":"c49f3f5fb03b4f7c98448e49b1d31e25"},{"start":"2851.49","dur":"3.09","text":"you know, X 1, X 2, X 1 squared,","_id":"c06d4d61a35a43af880d6663453e329e"},{"start":"2845.52","dur":"5.97","text":"your feature vector X 1 X 2 and map it to a high dimensional feature vector with,","_id":"a0b2a4a5e8a9495781c9c2ecad04a817"},{"start":"2842.505","dur":"3.015","text":"So one way to apply logistic regression like this would be to take","_id":"99243c665e9046c197f7556a58269382"},{"start":"2839.37","dur":"3.135","text":"Gaussian discriminant analysis will end up with a straight line decision boundary.","_id":"3ff0aaaa3a2b4bb0910975f7b8c4ed6b"},{"start":"2835.44","dur":"3.93","text":"so logistic regression will fit the three lines of data,","_id":"554dc8533e4d433783391d6eace44249"},{"start":"2831.555","dur":"3.885","text":"But if this is X 1, this is X 2, right,","_id":"4503cc8c73f342f3b00f8d81fd997710"},{"start":"2826.425","dur":"5.13","text":"Now one way to build a classifier like this would be to use logistic regression.","_id":"0db6b83c769b4f80b03a78c05f654924"},{"start":"2822.54","dur":"3.885","text":"find potentially very very non-linear decision boundaries like this.","_id":"9e28269c2b2d449889f8575490aaaf41"},{"start":"2819.06","dur":"3.48","text":"So the support vector machine will be an algorithm to help us","_id":"56036f0aacac4ab9a434da95944ebcd9"},{"start":"2815.805","dur":"3.255","text":"like a nonlinear decision boundary, right?","_id":"1c380d3522c74a39894869834d8c31fb"},{"start":"2812.535","dur":"3.27","text":"and so you want an algorithm to find, you know,","_id":"7af3e52d1f384124ae6145f9b5a62e68"},{"start":"2809.94","dur":"2.595","text":"Right, where the data set looks like this, uh,","_id":"d84300c77f77474d9dd6831c8f17a1c9"},{"start":"2801","dur":"8.94","text":"[NOISE].","_id":"f2afe344899f4702af10cd241e32b23b"},{"start":"2794.67","dur":"6.33","text":"Um, let's say the classification problem,","_id":"5416a8d03f244a7ca86832487062c0d4"},{"start":"2785.46","dur":"9.21","text":"[NOISE] su-support vector machines, SVMs.","_id":"e69060414b0e4182b1b95cbd5cb50b8c"},{"start":"2782.45","dur":"3.01","text":"Okay so,","_id":"6fa9c76c4946462bbae3e786f339a968"},{"start":"2774.3","dur":"5.05","text":"Yeah, The NLP class, yeah, pretty sure, actually I am sure they do.","_id":"2b1e5c1b632848848add7bd30e855b49"},{"start":"2771.66","dur":"2.64","text":"Actually CS224N I think also covers this.","_id":"2631119b7e324670b8f5b2116f0ea4e0"},{"start":"2758.16","dur":"13.5","text":"[LAUGHTER].","_id":"8256afc2cf0d4f9bbc2370b57f2b7f45"},{"start":"2757.125","dur":"1.035","text":"CS 229 so","_id":"bf03956ecc3e4b93822ec832e7faa72d"},{"start":"2755.46","dur":"1.665","text":"no we don't do that we just covered that in","_id":"454329ecec3143138490c5a43ed73511"},{"start":"2754.17","dur":"1.29","text":"somebody's got a question they go,","_id":"cc57fcd0d3b1419ebc335c1638cbc836"},{"start":"2753.06","dur":"1.11","text":"In some of the other classes,","_id":"d926f2ff8b87417d8ae4680dcea8f6c1"},{"start":"2750.57","dur":"2.49","text":"By the way I do this in the other classes too.","_id":"3c630f62f9444322973c3129aca3110e"},{"start":"2745.38","dur":"5.19","text":"[NOISE] Cool.","_id":"662669c589ba412da48d106ea7ac6c48"},{"start":"2740.1","dur":"5.28","text":"more knowledge baked in, right. Cool. Anything else?","_id":"9861159c5a5b472097dcd28a29c392f1"},{"start":"2737.76","dur":"2.34","text":"a good text classifier because it comes in with","_id":"57c385eed4084ca9ae8ffe4545488dc9"},{"start":"2735.435","dur":"2.325","text":"Will reduce the number of training examples you need so they are","_id":"6aceac7733694be59211375a4c47d020"},{"start":"2733.74","dur":"1.695","text":"These are techniques from neural networks really.","_id":"4be5379486994ad58c6bfb92f6b4bfc5"},{"start":"2731.37","dur":"2.37","text":"Uh, so the word embeddings techniques.","_id":"8822f373827a4da3aee58b47ef72a084"},{"start":"2727.56","dur":"3.81","text":"the videos and resources from CS 230 if you want to learn about that.","_id":"c929e2c3c205467ea336d25309b9fa3f"},{"start":"2724.455","dur":"3.105","text":"but you can also read up on word embeddings or look at some of","_id":"adfba8feab9f45468ab990a46d804d5c"},{"start":"2720.345","dur":"4.11","text":"So in- in- in neural networks [NOISE] , right,","_id":"aee7b7dd8eaa4404bd5b2f8042a6bcfc"},{"start":"2714.105","dur":"6.24","text":"this is a technique that I was not planning to teach here but that is taught in CS 230.","_id":"ccf807004d71404aa591bdd452746a2e"},{"start":"2712.32","dur":"1.785","text":"Uh, and so, uh,","_id":"2a560f88e50b4c459177a6b184dd4418"},{"start":"2710.19","dur":"2.13","text":"quite similar to each other because they are both city names.","_id":"c4ac600178f64d61b475959f492c301f"},{"start":"2707.76","dur":"2.43","text":"whatever London and Tokyo are","_id":"0fc5cff3a250413aa1285dda44064129"},{"start":"2705.885","dur":"1.875","text":"Yeah the words, um,","_id":"5b9dfc71a183445081b1c4585d559c70"},{"start":"2703.05","dur":"2.835","text":"Uh, the words mother and father are quite similar to each other.","_id":"0cfd86f64bcd4d5193f734a9899ec5b5"},{"start":"2700.11","dur":"2.94","text":"the fact that the words one and two are quite similar to each other.","_id":"b702e27e240142e39ca8651873a152a9"},{"start":"2697.86","dur":"2.25","text":"choose the feature representation that encodes","_id":"4fd7008260cf412bbf25e79ff1a7d078"},{"start":"2691.8","dur":"6.06","text":"um-[NOISE] In which you","_id":"88a935a1ca964920aee0924e8dfffd51"},{"start":"2689.73","dur":"2.07","text":"uh, there's a technique called word embeddings,","_id":"a421d037d2634e7c81c5c562dd3ae12a"},{"start":"2685.98","dur":"3.75","text":"So, um, in machine learning there are other ways of representing words,","_id":"b8355bd4b84446ca8cb1f38464b50585"},{"start":"2682.635","dur":"3.345","text":"feature representation, it doesn't know the relationship between these words.","_id":"4b08b89636f94c5c92d3a44b9da501e5"},{"start":"2679.35","dur":"3.285","text":"Uh, and so wi- wi-with this, uh,","_id":"da60d632b028428c80a5829b46c02e19"},{"start":"2676.47","dur":"2.88","text":"you know, like mother and father are quite similar.","_id":"f97d0b00a3ba4b35a7d85a791c0fe3bb"},{"start":"2673.155","dur":"3.315","text":"And so the words one and two are quite similar and the words,","_id":"35df2d8588c347fa861f0fa62b192b75"},{"start":"2670.98","dur":"2.175","text":"you know, separate from each other.","_id":"41448d71a23a429981cbf558a9420982"},{"start":"2667.71","dur":"3.27","text":"the Naive Bayes Algorithm is that it treats all of the words as completely,","_id":"e872cc636a6045f7b27d05c813a08e8f"},{"start":"2666.06","dur":"1.65","text":"So one of the weaknesses of","_id":"4e3e36f4de0a41018a8365d1be210ed5"},{"start":"2661.68","dur":"4.38","text":"Oh I see yeah right yes so yes, uh, right.","_id":"3d2c2badc8a843c3b274614992f165e4"},{"start":"2649.22","dur":"12.46","text":"[OVERLAPPING] Discrete variables [inaudible]","_id":"fd722a568b664dc2964b07a6ddf667e4"},{"start":"2646.85","dur":"2.37","text":"Sorry you can use logistic regression with.","_id":"ee71f712960947db935824aacd23273f"},{"start":"2627.14","dur":"19.71","text":"[inaudible].","_id":"34c440c081184ff2b3a6e676bb38ac49"},{"start":"2622.114","dur":"5.026","text":"Um, let me just check any questions about this before I move on. Yeah.","_id":"3f70884634f04cf8afce32dac5947201"},{"start":"2620.48","dur":"1.634","text":"which is a support vector machine.","_id":"4c650236fb024d32b178949182424708"},{"start":"2615.77","dur":"4.71","text":"The next thing I wanna do is move on to a different cla- type of classifier, ah,","_id":"53436d040c6b403ca2bb5feed16ad83a"},{"start":"2612.575","dur":"3.195","text":"and generative learning algorithms.","_id":"b039615eda05420292431cd6c28d77a2"},{"start":"2610.25","dur":"2.325","text":"uh, Naive Bayes, um,","_id":"cf8c3a82231e4727a6bc815f3965d924"},{"start":"2609.005","dur":"1.245","text":"So that's it for,","_id":"4f8c890e968641d490f1dd555555831e"},{"start":"2605.345","dur":"3.66","text":"Okay? All right.","_id":"2a85030ecefa47a4aabed2b74555183b"},{"start":"2603.17","dur":"2.175","text":"and then- and then go from there.","_id":"26f90642384f441da74989d16471a2b2"},{"start":"2601.715","dur":"1.455","text":"and then see how that performs,","_id":"422b71624b1147fc95dd02b79281c050"},{"start":"2600.23","dur":"1.485","text":"Or start with Naive Bayes,","_id":"7a880ffcb63b434c9c50d4b38150e829"},{"start":"2597.62","dur":"2.61","text":"a neural network or not- not something more complicated.","_id":"77c1eeb3f13348889392b1772633cc81"},{"start":"2595.475","dur":"2.145","text":"Start with logistic regression not- not","_id":"aea03a92ab4e42389e3b4d2068e92352"},{"start":"2593.855","dur":"1.62","text":"uh, and apply something simple.","_id":"7677e88e311d4a518ad6e7d280d88f85"},{"start":"2591.77","dur":"2.085","text":"but instead get a data set,","_id":"401e71a646914a658da78807edafb868"},{"start":"2588.845","dur":"2.925","text":"Uh, if you have an applicant- if- if you- if you- if you have an applied project,","_id":"b8075e0c2df4466ba2202ce694093444"},{"start":"2585.38","dur":"3.465","text":"don't spend weeks designing exactly what you're going to do.","_id":"3c00849a6e0a4f2dacc8214ae9b3ece1"},{"start":"2582.935","dur":"2.445","text":"I would advise most of you to, um,","_id":"9dd234bed96345fb858f1266fc954330"},{"start":"2580.82","dur":"2.115","text":"you know, as you start working on your project,","_id":"a3a55f76c34d4e72aa6c11b011372b80"},{"start":"2577.355","dur":"3.465","text":"I would advise most of you to uh, uh,","_id":"97bf9bcbe6ae489fb66c6324f9db00ce"},{"start":"2574.86","dur":"2.495","text":"And so I think for your project as well,","_id":"889c16a77c70479cb08a486df44eb2c0"},{"start":"2571.855","dur":"3.005","text":"um, get going more quickly.","_id":"c78e5c36ea434198963034c424d4b0f6"},{"start":"2567.61","dur":"4.245","text":"So it can help you implement that quick and dirty thing that helps you,","_id":"4f322973159b4229938c92b9244281be"},{"start":"2564.97","dur":"2.64","text":"and also they are- they are simple to implement.","_id":"0504378a0a104679919ac21ed1fe1f81"},{"start":"2563.17","dur":"1.8","text":"So it's very competition efficient,","_id":"5567314a9dd4448199e071927bb9889c"},{"start":"2559.775","dur":"3.395","text":"and GDA is just computing means and co-variances, right.","_id":"d1fe27699af54f008db050bdd5c4850d"},{"start":"2558.32","dur":"1.455","text":"Uh, uh, this is just counting,","_id":"eee5824569724c5c91a3017285b18c3b"},{"start":"2554.27","dur":"4.05","text":"they are very quick to train or it's non-iterative.","_id":"f8a891f9b0a34a4b9430ef0e8e1926d2"},{"start":"2552.064","dur":"2.206","text":"and Naive Bayes is that, um,","_id":"dc26c0cebebb4dc6971384646c9d6769"},{"start":"2549.28","dur":"2.784","text":"But the advantage of Gaussian discriminant analysis,","_id":"8cf8f4cd07474fb8bc3f2556fccb39fd"},{"start":"2545.38","dur":"3.9","text":"which will almost always give you higher classification accuracy than these algorithms.","_id":"a1c30b99ef7b4dd581576cfde8295877"},{"start":"2542.29","dur":"3.09","text":"which we talked about, or neural networks we'll talk about later,","_id":"8369575724f948d395e702a7b2ed7fe9"},{"start":"2539.2","dur":"3.09","text":"their are other algorithms like logistic regression or SVM","_id":"3d7a3a7e63824c3f8fd5045213993839"},{"start":"2536.42","dur":"2.78","text":"If you want the highest classification accuracy,","_id":"e48249742bd24380b7b5985134e9fa14"},{"start":"2533.465","dur":"2.955","text":"uh, they're not going to be the most accurate algorithms.","_id":"66a09dc1f3f24ec0b75037879b00e81c"},{"start":"2529.52","dur":"3.945","text":"GDA Gaussian discriminant analysis as well as Naive Bayes is that- is,","_id":"683b24c6e4a648d6aed776bed60b0ee3"},{"start":"2526.01","dur":"3.51","text":"Okay. So one of the uses of, um,","_id":"c02418149a6244e78c9eff0b586f7963"},{"start":"2521.825","dur":"4.185","text":"Go work on something else instead or at least- at least treat that as a low priority.","_id":"5ce8288a93384bc8ad8e19a2b0af6fd5"},{"start":"2520.67","dur":"1.155","text":"then I would say don't bother.","_id":"ac39551538674745a89fd919c9d3d2a3"},{"start":"2516.725","dur":"3.945","text":"and you see that it's not misclassifying a lot of examples of these misspelled words,","_id":"095beb48f96d44e48bee72a498d69f55"},{"start":"2515.015","dur":"1.71","text":"Right. When you implement a spam filter,","_id":"d04b5432a9ff4b718c2e335db4f8ade2"},{"start":"2512.99","dur":"2.025","text":"the deliberately misspelled words problem.","_id":"b3d82c213d3543fd9d72ecb25aafd9d3"},{"start":"2509.945","dur":"3.045","text":"spending a bunch of time solving the misspelled words,","_id":"5ef4dd8e60704289aeeb14970962da74"},{"start":"2507.125","dur":"2.82","text":"It's only then that you have more evidence that it's worth","_id":"37f0e80f18944287a00847853e35054c"},{"start":"2503.735","dur":"3.39","text":"misclassifying a lot of examples with these deliberately misspelled words.","_id":"b66d1161cc284389b5ab7a5b7df02dc3"},{"start":"2500.99","dur":"2.745","text":"you find that your sp- anti-spam algorithm is","_id":"1213eeaf958a4756a4e297c597b8bd57"},{"start":"2497.45","dur":"3.54","text":"And you'll find that, if after you've implemented a quick and dirty algorithm,","_id":"a130d9dacd634013b5df21a6120fad20"},{"start":"2493.25","dur":"4.2","text":"And then look at the examples that your learning algorithm is still misclassifying.","_id":"e801c3705f68464c814f11052de1de17"},{"start":"2491.255","dur":"1.995","text":"Almost implement something quick and dirty.","_id":"54ac5757f6274068bea08b0aec203ee8"},{"start":"2488.795","dur":"2.46","text":"But you instead implement a more basic algorithm.","_id":"b4492d2c0ca9489f8b96fae33a6f71c5"},{"start":"2484.78","dur":"4.015","text":"you know, six months on trying to analyze email headers.","_id":"c2d3c1097141421e90729aa5d90c6edf"},{"start":"2481.105","dur":"3.675","text":"and spend six months on improving this or spend,","_id":"3b17138bed5943979e1e8234e8f85467"},{"start":"2477.355","dur":"3.75","text":"is to not so-somewhat arbitrarily dive in,","_id":"179e181c990c4f3b8af44e3824e2fb7e"},{"start":"2475.03","dur":"2.325","text":"if your primary goal is to just get this thing to work,","_id":"a3b121c1c626448393c5d761d99b3ac6"},{"start":"2473.68","dur":"1.35","text":"those who work on projects,","_id":"4ce60914b914457e8418533d892e1a5b"},{"start":"2471.425","dur":"2.255","text":"So my advice to, ah,","_id":"1c12b1a9083d423d8180101bef6a6b30"},{"start":"2467.87","dur":"3.555","text":"how do you actually know which of these is the best investments of your time.","_id":"de1d770cd8754fb683a0e3c363d8fa1c"},{"start":"2464.855","dur":"3.015","text":"But when you are building say a new spam filter for the first time,","_id":"62f707106d684c43946b40e1c450c824"},{"start":"2459.77","dur":"5.085","text":"And any one of these topics could easily be three months or six months of research.","_id":"0376ad01f24d4c9cb36fc265212ea0ae"},{"start":"2455.735","dur":"4.035","text":"Right, there are a lot of things that you could do to improve a spam filter.","_id":"54a47d2fcd3d450289ec358774811da7"},{"start":"2453.56","dur":"2.175","text":"and then analyze the web pages that you get to.","_id":"8e1fbcd871884f90b620720b7e16b7fe"},{"start":"2450.545","dur":"3.015","text":"try to fetch the URLs that are referred to in the email,","_id":"382c78f5826949afb5c0065d4eaf9a77"},{"start":"2446.66","dur":"3.885","text":"Um, ah, an- an- another thing you might do is, ah,","_id":"e42e2deb1a23496eb5fc391be263512a"},{"start":"2443.96","dur":"2.7","text":"you know, address and other information.","_id":"5ba378942c8b49529d34d88c0349b2ed"},{"start":"2441.74","dur":"2.22","text":"by spoofing the email header that,","_id":"20e711b82ddc423193f50785a9fb3981"},{"start":"2436.895","dur":"4.845","text":"spam has often tried to hide where the email truly came from, uh,","_id":"1563d43aefb446709013154107c8e8fc"},{"start":"2432.56","dur":"4.335","text":"[NOISE] You know, uh,","_id":"f04b1db089c64ab18aab5dacc101cee6"},{"start":"2427.775","dur":"4.785","text":"Um, another idea might be a lot of spam email spoofs email headers.","_id":"3d6095248c01408cabb86d83da91545b"},{"start":"2426.32","dur":"1.455","text":"So- so that's one idea.","_id":"5104e1b7cdf7417498a7f94aaa84a674"},{"start":"2422.99","dur":"3.33","text":"So the spam filter can see the words the way that humans see them, right.","_id":"01948bc92ae14b9284fb11df046cfafa"},{"start":"2419.48","dur":"3.51","text":"actually wrote a paper mapping this back to words like that.","_id":"4720d13b33bd4b32af3558d592f1657e"},{"start":"2416.375","dur":"3.105","text":"spam or- actually one of our PhD students [inaudible]","_id":"5b1b68049133474f920c385341a925a1"},{"start":"2413.495","dur":"2.88","text":"So that's one idea for improving, um,","_id":"4fbaa7ba687d4d0b9da2c4fd6609073c"},{"start":"2411.08","dur":"2.415","text":"and that's the lightest way to slip by this spam filter.","_id":"8374f5a2e262427e8a578fd0832d4b02"},{"start":"2408.695","dur":"2.385","text":"There it was off by just a letter and it hasn't seen this before,","_id":"1be8bb5b6bcf412a810b1987aa7f870c"},{"start":"2405.815","dur":"2.88","text":"This might map the word to- to an unknown word.","_id":"970137ed8f234e1b937d12d3684d388c"},{"start":"2404.135","dur":"1.68","text":"this will trip up a spam filter.","_id":"0a8b308ed52149aa843c5aa387cd3afa"},{"start":"2399.89","dur":"4.245","text":"But all of us as people have no trouble reading this as a word mortgage but uh,","_id":"7bbca3c0572c4ecc8883bce5c0da5756"},{"start":"2396.86","dur":"3.03","text":"uh, slash slash, right.","_id":"9f32471d325042fbaf3924e93aeb93d2"},{"start":"2393.01","dur":"3.85","text":"Right. Or instead of G-A-G-E, maybe,","_id":"2538bd147cbe49fc84aa097c7853d50e"},{"start":"2384.305","dur":"8.605","text":"mortgage spammers will write M-0-R-T-G-A-G-E.","_id":"c2eec2b6577a473280666c11c3258c00"},{"start":"2380.51","dur":"3.795","text":"But instead of writing th- the words uh,","_id":"bcd50805a24844f492dc000a67825144"},{"start":"2378.725","dur":"1.785","text":"refinance your mortgage or whatever.","_id":"4639cbf075c14d98933f0656938ab234"},{"start":"2376.475","dur":"2.25","text":"Uh, you know, a lot of mortgage spam, right,","_id":"4a93adc8f030429691f4649b44d6cecd"},{"start":"2372.74","dur":"3.735","text":"For example, spammers will deliberately misspell words.","_id":"2df4d3604ff84b428526a77acb8b7b62"},{"start":"2371.36","dur":"1.38","text":"there are lots of you could work on.","_id":"ed13e8b9b6fa4aeb8ca67cb5b437010c"},{"start":"2368.99","dur":"2.37","text":"So if you want to build an anti-spam classifier,","_id":"80e8c1dbd66f4c5c9eaf0502be6b668c"},{"start":"2364.595","dur":"4.395","text":"um, it's hard to know what's the hardest part of the problem, right.","_id":"a468826346914f1b8aa4fe7b497d9f39"},{"start":"2358.55","dur":"6.045","text":"And, um, it turns out that when your'e starting out on a new application problem,","_id":"31f6380290554f54b8b312b9143463ec"},{"start":"2355.58","dur":"2.97","text":"My student worked on spam classification many years ago.","_id":"17677628f6ac4830b9d834485ea0a37f"},{"start":"2351.605","dur":"3.975","text":"I- I actually started a conference on email and anti-spam.","_id":"6e07afd363e84763ae4372e2b6b87401"},{"start":"2349.805","dur":"1.8","text":"This is actually something I used to work on.","_id":"81148c43719843f0a6cf5eaead1501fc"},{"start":"2347.375","dur":"2.43","text":"Um, so here's- here's- here's one example.","_id":"498bf0ade11c477bba1c9f48a9d2a178"},{"start":"2343.46","dur":"3.915","text":"getting to a better performing algorithm faster.","_id":"ba57ad746ec04eb090fb59f2fc447c04"},{"start":"2341.18","dur":"2.28","text":"You often end up, um, uh,","_id":"5488c8a16d70451cadd7346ac4e4228c"},{"start":"2338.795","dur":"2.385","text":"see what it's doing wrong to improve from there.","_id":"88516968b8f848e2a0f5431c288d571c"},{"start":"2335.435","dur":"3.36","text":"and then- and then use the- see what it's doing wrong,","_id":"8b5af1cea3ba4ac7b2f575e77430037a"},{"start":"2333.545","dur":"1.89","text":"you build a simpler algorithm, test it,","_id":"fe2bd28aa71344688eb4e0c03a152bb7"},{"start":"2329.285","dur":"4.26","text":"Uh, instead of building a very complicated algorithm from the get-go, um,","_id":"dfc37c7785184a6590fe5035eaeb033b"},{"start":"2326.495","dur":"2.79","text":"Um, and I think it's similar for machine learning.","_id":"9af41f4ddfb546b380c63a6edf3ddc18"},{"start":"2323.21","dur":"3.285","text":"and then start to see what syntax errors you're getting for the first time.","_id":"74d7c2a42f2c4d6ba60a4dbd415aaac6"},{"start":"2321.56","dur":"1.65","text":"Rather than write 10,000 lines of code,","_id":"af6752c9a2a14bac839cfd28a40b8100"},{"start":"2319.94","dur":"1.62","text":"and then build up a program incrementally.","_id":"ca217c32fc2d41228ac4d474d385fe9a"},{"start":"2318.29","dur":"1.65","text":"it test it- unit testing,","_id":"554fe7a628884aaba6fd804182670562"},{"start":"2315.35","dur":"2.94","text":"And it's a, you know, you should write small modules, run it,","_id":"dd947b7300664d96a4b59941cfb1ed0c"},{"start":"2313.655","dur":"1.695","text":"And that's clearly a bad idea, right?","_id":"961b35b3924c401e98497489651a5063"},{"start":"2310.505","dur":"3.15","text":"and then to try compiling it for the first time, right.","_id":"a5507153ab06487f9ff6d21505a28042"},{"start":"2306.785","dur":"3.72","text":"One approach is to write all 10,000 lines of code first,","_id":"720382ce706746879de8a41ab080d530"},{"start":"2301.805","dur":"4.98","text":"So if you're writing a new computer program with 10,000 lines of code, right?","_id":"21f47a6f8bab4416bc281747f3c4b9ac"},{"start":"2299.18","dur":"2.625","text":"uh, uh, let's see.","_id":"21da0248ecbe4d0b97e06a85c63926b7"},{"start":"2296.57","dur":"2.61","text":"um, if you are,","_id":"d3e3fe2c7ac7412c8240ac321b73b212"},{"start":"2291.05","dur":"5.52","text":"Um, you know one- one- one analogy I sometimes make is that,","_id":"e18ea43c70cd49d99728e4ec522dba13"},{"start":"2288.8","dur":"2.25","text":"and use that to drive your development.","_id":"3cfcda8ee7b14700b3ebe9c5dc2ccbc2"},{"start":"2286.129","dur":"2.671","text":"and then do error analysis which we'll talk about later,","_id":"983661f5b6f947149dbc1a822c5331ec"},{"start":"2282.2","dur":"3.929","text":"uh, so that you can then better understand how it's performing,","_id":"20e0171d56f3441ba998a253c7c4e852"},{"start":"2278.955","dur":"3.245","text":"um, I would recommend implementing something quickly,","_id":"7e08b6cd972b431fbfde2f13b88d49af"},{"start":"2275.87","dur":"3.085","text":"building a very complicated algorithm at the onset,","_id":"811e7b3553074bfdb26329466c078cb8"},{"start":"2273.44","dur":"2.43","text":"Then rather than, uh,","_id":"6b772c21a44c44f9af0e7c850a8281aa"},{"start":"2269.9","dur":"3.54","text":"Uh, and your primary objective is just make an algorithm work.","_id":"a00550a6b36e439e9151f992ee58347e"},{"start":"2264.89","dur":"5.01","text":"understanding news better or improving the environment or estimating prices or whatever.","_id":"dae6ea3f5d104b4a82815f7373487c45"},{"start":"2261.365","dur":"3.525","text":"you're working on an application on- on","_id":"3fad26e86dd744d19aee0df07ed8a197"},{"start":"2259.94","dur":"1.425","text":"If you- if your main goal is, uh,","_id":"80e12ad18e0d4764b2f2dbf701b08824"},{"start":"2257","dur":"2.94","text":"on a new technical, you know, contribution.","_id":"28421c086b1d4693a0283b7f02917c05"},{"start":"2254.3","dur":"2.7","text":"rather than inventing a new learning algorithm and publishing a paper","_id":"57c8f1b342d14b0aab915cb093e66983"},{"start":"2252.74","dur":"1.56","text":"um, work for an application,","_id":"c6782a855a5549599ea482fc5de5f1a0"},{"start":"2250.115","dur":"2.625","text":"But if your goal is to make something,","_id":"67ebf24057894827b69d8931b3bf7626"},{"start":"2246.89","dur":"3.225","text":"So we're very good at coming up with very complicated algorithms.","_id":"0116f86cabe44d19ac0b69d50fb17b69"},{"start":"2245.675","dur":"1.215","text":"we're- we're- that's at Stanford.","_id":"2525d889e1bb46c5b7336b93f9e8f5a5"},{"start":"2244.07","dur":"1.605","text":"So I think, you know,","_id":"51aac6b3ae1a4a29bb1471e1f6d6856f"},{"start":"2241.91","dur":"2.16","text":"and keep iterating on- on that.","_id":"da6e768a31224ebd8b340dd51bf7e0f5"},{"start":"2239.87","dur":"2.04","text":"and then use that to deep out the algorithm,","_id":"c458514c5bd443a299e3d42f38ddd702"},{"start":"2238.61","dur":"1.26","text":"look at how it performs,","_id":"a612d12d22a5490a91af24dc323cb50f"},{"start":"2236.555","dur":"2.055","text":"and, uh, train the algorithm,","_id":"7001235b8bd34f32bc2f2b9fff19d848"},{"start":"2234.62","dur":"1.935","text":"Start by implementing something quickly,","_id":"70801b949adb4b1c9ef4c892f0332c31"},{"start":"2231.8","dur":"2.82","text":"That's been implemented in most complicated possible learning algorithms.","_id":"790065e1f5424fc6a395f9be5487abbe"},{"start":"2228.95","dur":"2.85","text":"start by implementing something quick and dirty.","_id":"f1eb693bb6c54fd48fb76781cdd51d35"},{"start":"2225.785","dur":"3.165","text":"when you get started on a machine learning project,","_id":"9f2b32cfe2564e58be92b85c3be4e33d"},{"start":"2221.84","dur":"3.945","text":"then rule of thumb that's suggested here is, um, ah,","_id":"3e85e65df6174bb2bbeb0aa3023e6808"},{"start":"2219.485","dur":"2.355","text":"but to take the existing algorithms and apply them,","_id":"136877d181a04b81abfd8f47e74fca04"},{"start":"2215.375","dur":"4.11","text":"And if your goal is not to invent a brand new learning algorithm,","_id":"ea2d4042e62849a1a30eefae2b30371f"},{"start":"2212.36","dur":"3.015","text":"taking this on SCPD, taking this remotely.","_id":"7114c094f34f4475b260e89fa731d9d9"},{"start":"2209.96","dur":"2.4","text":"build or apply to a business application for some of you","_id":"812774975924425ba33b715d166ee9af"},{"start":"2207.11","dur":"2.85","text":"Stanford or apply to a fun application you wanna","_id":"30cd61667a984e47a7a64a57600983f6"},{"start":"2204.92","dur":"2.19","text":"Apply to a research project you're working on somewhere in","_id":"4b4fbc28b1c7474992865a84fc6e8b01"},{"start":"2199.67","dur":"5.25","text":"to apply a learning algorithm to a project that you care about.","_id":"c83df77f7703470fb054fe934bc21589"},{"start":"2195.695","dur":"3.975","text":"Um, the majority of class projects in CS229 will try","_id":"cf39a61c3dbd47e1b4502d78ab271212"},{"start":"2192.02","dur":"3.675","text":"It helps a lot of people on a lot different applications so that's one.","_id":"044192f3efb54af7adc7da9f5e80fd12"},{"start":"2189.755","dur":"2.265","text":"inventing the machine learning algorithm is a great thing to do.","_id":"a2b6dfce912d4f16b145b7c4006c332b"},{"start":"2187.775","dur":"1.98","text":"Um, and I think, you know,","_id":"6e6fa0fe72f444bda6cdef95a67282a6"},{"start":"2185.989","dur":"1.786","text":"and write a research paper.","_id":"ead91a659ce64bc7abbf7c6ba41563af"},{"start":"2181.37","dur":"4.619","text":"probably a minority will try to invent a new machine learning algorithm,","_id":"173ed7127b2a4c8fb316fe5a108c1e11"},{"start":"2180.155","dur":"1.215","text":"I think some of you","_id":"2c46d103300947f6afd6b9f2aafe140c"},{"start":"2176.84","dur":"3.315","text":"um, you know as you work on your class projects,","_id":"19ae0d1e3bfb4607b8bd095dade9ade3"},{"start":"2174.995","dur":"1.845","text":"Um, and I think,","_id":"955fbee41be14cd9ba379ac977e8a799"},{"start":"2171.47","dur":"3.525","text":"then Naive Bayes is- is maybe a reasonable choice.","_id":"474c940e93a44662ac221aa2455f6dd1"},{"start":"2168.11","dur":"3.36","text":"way you go is to implement something quick and dirty,","_id":"db1a00c47b554461b1b737ec0c688be6"},{"start":"2164.12","dur":"3.99","text":"So if you are, uh, facing a problem,","_id":"e904f4d995c44959939d3fb2b5efcb97"},{"start":"2159.98","dur":"4.14","text":"and the number of lines of code needed to implement Naive Bayes is relatively small.","_id":"81dc25040768496aa6f8bf54d90769e1"},{"start":"2156.995","dur":"2.985","text":"And it also doesn't require an iterative gradient descent thing,","_id":"9dc5984f15554047becc66df111dadc4"},{"start":"2154.01","dur":"2.985","text":"and second it's relatively quick to implement, right?","_id":"68884ec93db942a38afc9b13a58662a6"},{"start":"2152","dur":"2.01","text":"first it's computationally very efficient,","_id":"942520139b6644d3bd215896db3c2184"},{"start":"2147.95","dur":"4.05","text":"But the- the- the advantages of Naive Bayes is, uh,","_id":"83088d28d16146a29b2f9a27f1b88f2d"},{"start":"2141.725","dur":"6.225","text":"will work better in terms of delivering a higher accuracy than Naive Bayes.","_id":"a6bddac0ce114a8cb78658896934bff9"},{"start":"2137.36","dur":"4.365","text":"Uh, so for most problems you find that logistic regression,um,","_id":"1b6a47fd48854e36b37f200e91711dd0"},{"start":"2135.11","dur":"2.25","text":"very competitive with other learning algorithms.","_id":"8ee57791d2404408a398699be5945886"},{"start":"2133.16","dur":"1.95","text":"It turns out Naive Bayes algorithm is actually not","_id":"b0a6d34f59f24caeb7b236664ebb6006"},{"start":"2130.49","dur":"2.67","text":"when would you use the Naive Bayes algorithm.","_id":"1980a1d1b9dd419cbc7ba6afcda7cd63"},{"start":"2126.575","dur":"3.915","text":"Um, so, you know,","_id":"94e2351489ed4f96a599a69d0e5c4aa7"},{"start":"2122.3","dur":"4.275","text":"um, more detail in the lecture notes.","_id":"53936099fc0845639f552eb293f2dfc7"},{"start":"2118.67","dur":"3.63","text":"including the details that maximum likelihood estimate are written out in,","_id":"0e0cd5024ef4452eb777632f4b323d43"},{"start":"2116.375","dur":"2.295","text":"So I think both of the models, ah, ah,","_id":"862f27aa27b64b938edde944522fe288"},{"start":"2113.74","dur":"2.635","text":"Um, all right.","_id":"63dfe1b6901b429fbc8b28f1bb773cb2"},{"start":"2108.44","dur":"4.57","text":"Cool. Okay great.","_id":"eeee9db3cc4d4ad4beaee12bcf6f683b"},{"start":"2106.46","dur":"1.98","text":"so 1 minus y-i will give us 0- yeah.","_id":"5745b9c67fbe4a57a9071a6139818bf6"},{"start":"2101.93","dur":"4.53","text":"Uh, I guess if y-i is 01 this- this is the same as not y-i I guess,","_id":"0a463b7e44b749dcb099c9f5f27b2392"},{"start":"2097.79","dur":"4.14","text":"that's either true or false depending on whether y-i is 0.","_id":"287aaa20385e48608e5a62b4f0b6f473"},{"start":"2093.44","dur":"4.35","text":"Yes, uh, but this is a- this is a little formula","_id":"9931ca335c364bfa8a966c8a35650616"},{"start":"2090.095","dur":"3.345","text":"So that's the- yeah, um, cool.","_id":"c2e4aca850044d40a08c108a0664b523"},{"start":"2085.835","dur":"4.26","text":"3 equals 5 is- is 0, is false.","_id":"9545fad9e10b4fc8b5bdce9942c281a8"},{"start":"2083.72","dur":"2.115","text":"An indicator of, you know,","_id":"c2c33b6df83948959d3456a2bb6d2e7e"},{"start":"2080.09","dur":"3.63","text":"2 equals 1 plus 1. This is true.","_id":"bf70036a87c94667953d423fcf43dd30"},{"start":"2076.85","dur":"3.24","text":"Means uh- well, so indicator of, you know,","_id":"8e198069ee9941f496311e124e1e7850"},{"start":"2073.01","dur":"3.84","text":"um, and so this is- this notation, right?","_id":"44e6471de9a149c3a829f1fb6692474c"},{"start":"2070.175","dur":"2.835","text":"boy- so if- if,","_id":"d2e49ef635434c74b76f818fac991549"},{"start":"2067.115","dur":"3.06","text":"Uh, uh, so indicator function uh,","_id":"96e83774824d411ea6fdc03e7305ff36"},{"start":"2063.935","dur":"3.18","text":"Oh, this is an indicator function notation.","_id":"c400f8baa38e477eb2a57ba5fbb8e9c7"},{"start":"2062.57","dur":"1.365","text":"Oh, why did I write the run before?","_id":"c40e1f0f34424ca4adc7f118ba923c29"},{"start":"2056.6","dur":"5.97","text":"[inaudible].","_id":"89a9416c0e4c4907b7d3ddd0559b35d2"},{"start":"2052.715","dur":"3.885","text":"your unknown word token or the unknown words special symbol. Yeah.","_id":"ea6d99b33774447c927f62bf6a0b949a"},{"start":"2049.76","dur":"2.955","text":"then everything that's not in the top 10,000 words can map to","_id":"d39bfcd918f041c090c2bd8b19b26664"},{"start":"2046.16","dur":"3.6","text":"you decide to take just the top 10,000 words in- into your dictionary,","_id":"ab6e84c7e10840f2b0bafd2dfcb13cd1"},{"start":"2042.935","dur":"3.225","text":"So, um, if in your training set, uh,","_id":"2cbe6830a62b4306bd282c03c5564c04"},{"start":"2038.18","dur":"4.755","text":"a special token which traditionally is denoted UNK for unknown words.","_id":"19e729ef5c3947b7af6a525e2a7a2282"},{"start":"2034.79","dur":"3.39","text":"Uh, second approach, is to take the rare words and map them to","_id":"9c6e68886ebe4a62ba2df25fc4902b6f"},{"start":"2032.525","dur":"2.265","text":"Just ignore it, disregard it, that's one.","_id":"355ff8f7fc48442e92781868b9f370d7"},{"start":"2030.185","dur":"2.34","text":"One is, um, just throw it away.","_id":"b425b75eb6a84b63934fb68f1e1efaa2"},{"start":"2027.095","dur":"3.09","text":"So, um, uh, there are two approaches to that.","_id":"f39275fe33d14b43bf02b18e2f25cb3a"},{"start":"2024.95","dur":"2.145","text":"Oh, what do you do if the word's not in your dictionary?","_id":"786dbb924b1149a6965901a8caf83b86"},{"start":"2022.22","dur":"2.73","text":"[inaudible].","_id":"fbc3ce9557a24237b957c36312d27069"},{"start":"2019.595","dur":"2.625","text":"Makes sense? Cool. Yeah. Question?","_id":"4754b5379463411eaf2edcf02d7263d9"},{"start":"2016.97","dur":"2.625","text":"so you add 10,000 to the denominator.","_id":"5f235c2e35874f5aa2dc88cef6761518"},{"start":"2012.845","dur":"4.125","text":"And so the number of possible values for X is 10,000,","_id":"ec597817a5614f8087e11594279f13db"},{"start":"2010.775","dur":"2.07","text":"If you have a list of 10,000 words you're modeling.","_id":"11695965ae694db2b79f98b8812507ea"},{"start":"2004.085","dur":"6.69","text":"where k ranges from 1-10,000 if you have a dictionary size.","_id":"fefe9ee71ddb49afa0c681e3d2a28a0e"},{"start":"2000.365","dur":"3.72","text":"X being equal to the value of k,","_id":"de6eeee2ef9f4fb1b3a64de8043fbd36"},{"start":"1995.115","dur":"5.25","text":"So, um, uh, so this is the probability of, um,","_id":"3bbcdbeed0944dc6893230d695cc0f4f"},{"start":"1991.62","dur":"3.495","text":"possible outcomes in the denominator which in this case was there 10,000.","_id":"e7e5b0a4e43247f1b4ae193b3e70102d"},{"start":"1988.74","dur":"2.88","text":"see I want a numerator and add to number of the","_id":"be04cbccd20e47c1837faf3281d1dcde"},{"start":"1986.385","dur":"2.355","text":"Yeah. Right? So, um, uh,","_id":"0259110fa62c47d88dea06c9263385be"},{"start":"1984.255","dur":"2.13","text":"Yeah, but here k is an index.","_id":"38e3bb9c861746d89dab2203e6bce63e"},{"start":"1981.885","dur":"2.37","text":"I think I used k as the number of possible outcomes.","_id":"b7abde87e09b4e1f8218d3c551f5424e"},{"start":"1980.58","dur":"1.305","text":"When defining the possibility,","_id":"ce5097ed3f68452a9484d97c42f5d6aa"},{"start":"1977.175","dur":"3.405","text":"Oh, I think I just realized why you say k I think, uh, overloading notation.","_id":"a02ec5cc39944922af8d9570888b8612"},{"start":"1973.92","dur":"3.255","text":"Yeah, Right.","_id":"8564f10f7ec04739b283a8234779bf2a"},{"start":"1971.73","dur":"2.19","text":"Cool. Yeah. Yeah. All right.","_id":"b393f9a625874f19b8985a163f9da218"},{"start":"1967.86","dur":"3.87","text":"[inaudible].","_id":"9201783609c34720be9e05e253d7e9b8"},{"start":"1965.265","dur":"2.595","text":"10,000. Cool. How come? Why 10,000?","_id":"2aefbb1b64254b4bbe6986a8b312d60d"},{"start":"1963.62","dur":"1.645","text":"About 10,000.","_id":"683dc4a760234f8eb5301e5ef420ac16"},{"start":"1957.21","dur":"4.57","text":"the words? What do you have?","_id":"a801728f0ade421798a33cad57cfbe52"},{"start":"1955.11","dur":"2.1","text":"So k indexes into, ah,","_id":"adbc088e5ea64f90b1d42582b5c22c32"},{"start":"1953.13","dur":"1.98","text":"Not k, right? k is a variable.","_id":"fe0f56f8e3f946f09b81a6453b550bcd"},{"start":"1950.925","dur":"2.205","text":"Uh, wait. But what is k?","_id":"cd7337075bcb458d9476625483eb6766"},{"start":"1938.895","dur":"12.03","text":"Actually, what- what- what- what would you add to the denominator? Uh-","_id":"70f1e27c72404954a8924680dfa5a970"},{"start":"1935.79","dur":"3.105","text":"and then, um, let's see.","_id":"7601b74eab5440c0b07058b5ddd71d0d"},{"start":"1932.01","dur":"3.78","text":"um, add 1 to the numerator as usual,","_id":"886c6383443f4ccdabc6cfd152b67474"},{"start":"1926.76","dur":"5.25","text":"[NOISE] to implement Laplace smoothing with this, you would,","_id":"763af6200beb4a19bd1af0da823a97a4"},{"start":"1919.935","dur":"6.825","text":"Okay? Oh, and then lastly, um,","_id":"b5d3c3eb8221478eb5b1ea1f6f836913"},{"start":"1916.545","dur":"3.39","text":"200 times, then this ratio will be 200 over 100,000.","_id":"5e13a5d98a484c5ea05839aada5630a8"},{"start":"1915.09","dur":"1.455","text":"that occurs, uh, you know,","_id":"bbe9cd60b190459fa9fe2cad5733f4e4"},{"start":"1908.985","dur":"6.105","text":"100,000 words in your non-spam emails and 200 of them are the word drugs,","_id":"71b6b001a51f4a568e46ea525cd9bd2e"},{"start":"1906.675","dur":"2.31","text":"um, uh, ah, you know,","_id":"c325d632898540aea5ff63653dd1b382"},{"start":"1903.39","dur":"3.285","text":"And so, uh, uh, if in your training set you have,","_id":"cb6b08d3393243c4aa40d99b9e267114"},{"start":"1897.87","dur":"5.52","text":"go over the words in that email and see how many words are that word k. Right.","_id":"b825b4970a554b9f8993cee5c922dc0f"},{"start":"1894.12","dur":"3.75","text":"and for the non-spam email j equals 1 through ni,","_id":"46673fe8ddb24ce3b69506365fb9e082"},{"start":"1889.95","dur":"4.17","text":"So, you know, count up only the things for non-spam email,","_id":"c198ddff85ae42c19e8035552ae4bdf2"},{"start":"1887.19","dur":"2.76","text":"indicates a y equals 0.","_id":"7a4b331fcbaa4ca69abf84d4abc61336"},{"start":"1884.85","dur":"2.34","text":"sum from i equals 1 through m,","_id":"3552de66b0004ca99354377b8bea1b81"},{"start":"1881.325","dur":"3.525","text":"um, and the numerator as some of your training set,","_id":"2d75cbb6d7e341428059ea13ecf993dc"},{"start":"1877.23","dur":"4.095","text":"the total number of words in all of your non-spam emails in your training set,","_id":"cc5bf7ae62424e299374b765ab75460d"},{"start":"1874.23","dur":"3","text":"So the denominator ends up being","_id":"88892c19610e4db4a1b0436e569f9c13"},{"start":"1871.545","dur":"2.685","text":"times the number of words in that email.","_id":"f5daed06cd0e4b6c8f5eace08c493bef"},{"start":"1866.25","dur":"5.295","text":"the denominator is sum of your training set, indicator is not spam,","_id":"a682cf0c7f064038b0afe1c8a06db418"},{"start":"1864.075","dur":"2.175","text":"And so, um, in math,","_id":"788f40adcf8c475f8e1f488dd33a8a95"},{"start":"1860.34","dur":"3.735","text":"appearing in the non-spam email in some position in that email, right?","_id":"d4a2aa11d4624083b4cd333dedf41c49"},{"start":"1856.86","dur":"3.48","text":"And that's, uh, your estimate of the chance of the word drugs","_id":"e25b7b96ca4748c48e9ac715c3e4c554"},{"start":"1854.295","dur":"2.565","text":"what fraction of those words is the word drugs?\"","_id":"570b4bcd8c694e7e967e073ef0a7aad0"},{"start":"1853.05","dur":"1.245","text":"and so all of those words,","_id":"3d7a0bb4d8504dffafbef219a183dcda"},{"start":"1850.695","dur":"2.355","text":"and look at all of the words in all of the emails,","_id":"95d0bcdd5e9f4b4e9971911cf48b677b"},{"start":"1848.58","dur":"2.115","text":"all the emails of y equals 0,","_id":"b8e9fd59b6d64d56849e7a2268692498"},{"start":"1844.95","dur":"3.63","text":"\"Look at all the words in all of your non-spam emails,","_id":"541fcca9b3694584834d1493e660d2ee"},{"start":"1841.02","dur":"3.93","text":"the English meaning of this complicated formula is, this basically says,","_id":"126ebedbaf544b0fa8b3c12b85fedfe2"},{"start":"1837.915","dur":"3.105","text":"so this space means- so- and so if you figure out what","_id":"6808f322544a446d832af59bb4b7c12b"},{"start":"1835.875","dur":"2.04","text":"So the denominator, um,","_id":"8bfb06f3329a4401b84d37904f3798be"},{"start":"1830.72","dur":"5.155","text":"uh, what this actually means.","_id":"4c0a2879a92d4780b107dd3f4a7ea760"},{"start":"1827.025","dur":"2.965","text":"I'll just say in a second,","_id":"b6173867f782404f9b999779d58b04c3"},{"start":"1823.05","dur":"3.975","text":"I find that- well, this indicates a function notation. It looks complex.","_id":"73e8a5c3584e4219b06bc90929bdfa80"},{"start":"1801.72","dur":"21.33","text":"Um, the chance of that is equal to [NOISE]","_id":"4ea2df68d2e54f4cb9778f348ca68faf"},{"start":"1798.15","dur":"3.57","text":"a non-spam email being the word drugs, let's say?","_id":"fc2d6fff80ce465fb22f03908a34c0de"},{"start":"1795.15","dur":"3","text":"being word k. What's the chance of some word in","_id":"6a3763ae35b54e0f900de01d92497099"},{"start":"1790.155","dur":"4.995","text":"the chance of a given word is really anywhere in any position,","_id":"7b2dfa5b4c4446c58b1d4aed71335151"},{"start":"1785.91","dur":"4.245","text":"[NOISE] Your estimate of, uh,","_id":"69f8bd3a890648fd8b3929f971d39955"},{"start":"1784.605","dur":"1.305","text":"I'll just write out one of them.","_id":"1e3b739297af45d08895f0436b9c3114"},{"start":"1781.83","dur":"2.775","text":"And then for the maximum likelihood estimate of the parameters,","_id":"885cfc18aa7f4f71b8bd723c62212fe7"},{"start":"1780.21","dur":"1.62","text":"is that y equals 0.","_id":"116b3a2f84974488b499b6845df5702c"},{"start":"1778.41","dur":"1.8","text":"Kind of just with y equals 1,","_id":"a113611b474b4530b81e736122e1f0f6"},{"start":"1775.98","dur":"2.43","text":"[NOISE] Right.","_id":"0489bff5f7bb42d69f77e71f4a078409"},{"start":"1773.775","dur":"2.205","text":"And then, and then the other set of the parameters is this.","_id":"4e5f5e8c40814110ba7e1d0ec3f593f8"},{"start":"1766.83","dur":"6.945","text":"Um, oh, and then, um, I wrote down, uh, [NOISE] right.","_id":"afe0b81368f34e718bf5581f41f720a4"},{"start":"1746.52","dur":"20.31","text":"[NOISE]","_id":"4d66192e50174be18dfab652724bdbe9"},{"start":"1745.815","dur":"0.705","text":"Okay?","_id":"91439a3ad7884c7ca84e06f58c149a8b"},{"start":"1741.525","dur":"4.29","text":"plugging these parameters that you estimate from the data into this formula.","_id":"09904620cb9a40fea274be3e5adbad7b"},{"start":"1738.12","dur":"3.405","text":"you would calculate this probability is by, you know,","_id":"d87c240cca3f4004a47f9c2e79018459"},{"start":"1735.885","dur":"2.235","text":"a test email, um, uh,","_id":"19390291e3f84e61943c7f1537a016ca"},{"start":"1734.58","dur":"1.305","text":"uh, given a new email,","_id":"848a1b725e244f6c84e8c62290a31b42"},{"start":"1729.885","dur":"4.695","text":"um, uh, and, and so the way that,","_id":"19b6360f15ff4c9c986542b0a67f375d"},{"start":"1726.315","dur":"3.57","text":"Um, and so the way you calculate the probability, the way you would,","_id":"0a2ead6747f6402abe8469d8dd36fe62"},{"start":"1724.77","dur":"1.545","text":"Okay. All right.","_id":"c31480649e2b46a798d13e6acc95c60f"},{"start":"1721.74","dur":"3.03","text":"No?","_id":"04fbd128f646497e92f197a7932d00bd"},{"start":"1719.27","dur":"2.47","text":"Makes sense? Any questions about this?","_id":"7a3029615c3e49ab8a3592f40f248065"},{"start":"1714.27","dur":"4.3","text":"on the left-hand side j doesn't actually appear on the left-hand side, right.","_id":"38ec13b259aa4393982b302b05e884bc"},{"start":"1712.98","dur":"1.29","text":"which is why, um,","_id":"e9be482c6a2246288b152652f2fd909b"},{"start":"1711.54","dur":"1.44","text":"is same as the third word being drugs,","_id":"3866ebffae284c3a8c983fd36ee30b0a"},{"start":"1709.44","dur":"2.1","text":"drugs is same as chance of the second word being drugs,","_id":"4642989eed2a461b82f54eeddadf6639"},{"start":"1707.505","dur":"1.935","text":"for the- the chance that the first word being","_id":"38c6418a3e994ca989a6e0ca2aa5c0fd"},{"start":"1705.435","dur":"2.07","text":"That for every position in the email,","_id":"1954319cb3b74560b05202885bc2a434"},{"start":"1699.56","dur":"5.875","text":"is that, uh, we assume that this probability doesn't depend on j, right?","_id":"6b1be6ced88f4d028199e526c092ec7f"},{"start":"1697.995","dur":"1.565","text":"mainly why this is tricky,","_id":"81e3e6efb29040f8a529169d0814a4bb"},{"start":"1696.555","dur":"1.44","text":"why we implicitly assume,","_id":"24132a0c9fdf4b2f919a798e316c5809"},{"start":"1694.245","dur":"2.31","text":"And one part of, um,","_id":"aaca25a263da4d49bd029e26100b267f"},{"start":"1690.63","dur":"3.615","text":"or the chance of the second in the email being buy, or whatever.","_id":"3837d68cb5ca4e738f2a69073dc2e95c"},{"start":"1689.595","dur":"1.035","text":"being the word drugs,","_id":"e1e625b140b24ee2ba793579c7c3fcce"},{"start":"1685.38","dur":"4.215","text":"Yes. Right. So it's the chance of the third word in the email,","_id":"a6f77191fab748cabb44cb9ccdf10a05"},{"start":"1682.78","dur":"2.6","text":"[inaudible].","_id":"d0fd703695a64117941ecfe1f4c1150c"},{"start":"1663","dur":"19.78","text":"Well- well, yeah?","_id":"daec34e0944d4d25bb4904133e190d71"},{"start":"1661.455","dur":"1.545","text":"Uh, let's see.","_id":"df55ae5ae177490dbd934903c7caf070"},{"start":"1658.88","dur":"2.575","text":"Actually, what goes in the second blank?","_id":"1691a6e2e3674900be3a225e59ed0229"},{"start":"1649.455","dur":"2.515","text":"So what goes into those two blanks?","_id":"d4be0076eaad4ada9d04d2d354037f6d"},{"start":"1640.215","dur":"9.24","text":"blank being blank if label y equals 0.","_id":"23cf7fe0833c45ff83ac3a8d7082caab"},{"start":"1634.86","dur":"5.355","text":"So this probability is the chance of word","_id":"a8082f26152846adb43e651c5cb9f252"},{"start":"1633.66","dur":"1.2","text":"See if this makes sense.","_id":"701c57f9ab1a4940992dd0e11bcfb5c7"},{"start":"1631.305","dur":"2.355","text":"Right? And- and just to make sure you understand the notation.","_id":"800db6076df3489da313b116f977954a"},{"start":"1628.88","dur":"2.425","text":"given y equals 0.","_id":"7d4da3cdfcb14aa0b925b0f500dafd00"},{"start":"1621.915","dur":"5.395","text":"is a chance of xj equals k,","_id":"6ac1ca5119524600892885230747970f"},{"start":"1620.16","dur":"1.755","text":"given y equals 0,","_id":"5520635db9d6479ab846252d21aaa826"},{"start":"1614.67","dur":"5.49","text":"and also, um, the other parameters of this model, phi k,","_id":"4f503a84160143aa8463834f212af793"},{"start":"1612.54","dur":"2.13","text":"Phi y is probability of y equals 1,","_id":"263cc0085d574f99b3a88b8aa2b29316"},{"start":"1609.39","dur":"3.15","text":"the parameters are same as before.","_id":"c8efdd1134194d24bc17c68dd7235f67"},{"start":"1606.03","dur":"3.36","text":"[NOISE] with this model,","_id":"208d622b6d52463cad5f016ad7c1a51e"},{"start":"1600.975","dur":"5.055","text":"Okay? Um, and it turns out that, uh, well,","_id":"a801b42c3dfb4b0cb261d79780a5d17c"},{"start":"1597.63","dur":"3.345","text":"Rather than a binary or Bernoulli probability.","_id":"6695546e766d4834a9de5e6f302d0396"},{"start":"1594.675","dur":"2.955","text":"and this is now instead a multinomial probability.","_id":"1986437164f54b05bec358e33a9e5a53"},{"start":"1591.825","dur":"2.85","text":"there's a product from 1 through the number of words in the email,","_id":"6ea2ea2ef86947bb934e0a7ec3e77622"},{"start":"1588.315","dur":"3.51","text":"So instead of a product from 1 through 10,000,","_id":"6f0981159b0440abb7ab37c39e962477"},{"start":"1582.48","dur":"5.835","text":"the definition of xj and the definition of n is very different, right?","_id":"bec292c7278d4e4a991f15d5811f885a"},{"start":"1578.55","dur":"3.93","text":"the confusingly named Multinomial Event Model, um,","_id":"789a5cfd5e964576988e8d6e923837f3"},{"start":"1576.045","dur":"2.505","text":"but with this new model, the second model,","_id":"8c509f1ac1cd45bf974a6b8a35e09977"},{"start":"1572.97","dur":"3.075","text":"this equation looks cosmetically identical,","_id":"f4ffdce4b339409b9cbda4710126fb05"},{"start":"1571.35","dur":"1.62","text":"Right? So this is exactly, uh, so this,","_id":"8b0d42c007734210827c9ede83cbf40e"},{"start":"1569.355","dur":"1.995","text":"p of x given y is part of probabilities.","_id":"385f267ec16f46afb52cd94f48296bb7"},{"start":"1567.78","dur":"1.575","text":"that, you know, this, you know,","_id":"1ed13a7e659c4f73abfff5749c18846d"},{"start":"1564.51","dur":"3.27","text":"when we described Naive Bayes for the first time, um,","_id":"eb1cc8e7d9384664849f0a3148c7c28f"},{"start":"1563.07","dur":"1.44","text":"you know, you saw on Monday,","_id":"88b430ead94e4e3bbf5614d223f60a48"},{"start":"1560.085","dur":"2.985","text":"is because this is exactly the equation [NOISE] that,","_id":"fe9a421d617d4a7fb63b26f9f9826bd7"},{"start":"1557.085","dur":"3","text":"were frankly actually very confusing to the machine learning community,","_id":"fad1f9b36ecb43efb4c15075126850a5"},{"start":"1554.49","dur":"2.595","text":"one of the reasons these two models were very-","_id":"9a1ded668f1a4975aff4e28567dfee7e"},{"start":"1551.715","dur":"2.775","text":"Now, one of the, uh, uh, one,","_id":"d980c35481794309b42ce14cc918582a"},{"start":"1549.57","dur":"2.145","text":"Is that second term, right?","_id":"395279156a5e47e3959040b6d2c7dc62"},{"start":"1545.18","dur":"4.39","text":"given y, and then times, you know, p of y.","_id":"18ef7714614c46ee90f9961316eb8cec"},{"start":"1543.08","dur":"2.1","text":"of p of xj,","_id":"9328c2ded71e45d2a051d3b84c9996ed"},{"start":"1540.89","dur":"2.19","text":"of j equals 1 through n,","_id":"59c2802b937d416697e521f1fb07a442"},{"start":"1533.905","dur":"6.985","text":"we're going to assume that p of x given y is product from i equals 1 through n,","_id":"cd9e33e46ad04383906be4223c1d7b6e"},{"start":"1525.855","dur":"8.05","text":"y which can be factored as follows and using the Naive Bayes assumption,","_id":"ca989702d6b747e1aa059055efccf4d9"},{"start":"1524.58","dur":"1.275","text":"or model p of x,","_id":"e7a5aa5d334a4e91a2306ddf91cc8f33"},{"start":"1522.045","dur":"2.535","text":"and because it's a generative model,","_id":"677e4ca2f5f94fa9802f741c4df2ec6c"},{"start":"1518.16","dur":"3.885","text":"um, we're gonna build a generative model,","_id":"8db0490b6d1546aebda62d6e8dc1ac23"},{"start":"1512.63","dur":"5.53","text":"Um, and so, with this new model,","_id":"0d301dedf7224d6a81ec9c57554ff637"},{"start":"1496.89","dur":"6.04","text":"But- but I think these are- these are the names we seem to use.","_id":"f773634c7ffe4d8c873fd6e5ff305a7c"},{"start":"1494.25","dur":"2.64","text":"wrote the paper that named these two algorithms.","_id":"69ac6554061549c7b19c0d41828c5330"},{"start":"1491.76","dur":"2.49","text":"one of my friends Andrew McCallum, uh, as far as I know,","_id":"5be8bc629bcb49df88d97d114e2ce784"},{"start":"1489.105","dur":"2.655","text":"But these are the names that, uh, I think- actually,","_id":"bea9bcd962844a71afe2eb50740fe0b6"},{"start":"1487.5","dur":"1.605","text":"these two names are quite confusing.","_id":"6ec02744bb824fec8b5d8785acdff424"},{"start":"1484.95","dur":"2.55","text":"Uh, these two names are- are- are- frankly,","_id":"f6859a090e464b5786280cbb496e24ca"},{"start":"1477.33","dur":"7.62","text":"the [NOISE] Multinomial Event Model.","_id":"3b3c6a8ecfd741b495c6063df5b69ee0"},{"start":"1473.58","dur":"3.75","text":"Um, and the new representation we're gonna talk about is called","_id":"1197f95498614b9287e0745829d02441"},{"start":"1471.855","dur":"1.725","text":"An event comes with statistics I guess.","_id":"4ab0a5a573c54bbc9b0ff14b74c045a7"},{"start":"1469.02","dur":"2.835","text":"this model whereas as a Multivariate Bernoulli event model.","_id":"7ad30eea52e74033b9cd9023736ed955"},{"start":"1466.32","dur":"2.7","text":"there are 10,000 Bernoulli random variables in","_id":"a3f0e1412d954cc6802772ec596e35a2"},{"start":"1464.565","dur":"1.755","text":"so multivariate means, you know,","_id":"7ef6a3b0e47f4d48ae8495d9ab6fb00f"},{"start":"1462.84","dur":"1.725","text":"so Bernoulli means coin tosses,","_id":"d3745303fc1c4550afb1f76174c2c10b"},{"start":"1460.67","dur":"2.17","text":"And that model, uh,","_id":"cf1bcf51703849ef9927349928fe93a3"},{"start":"1451.02","dur":"8.98","text":"called the Multivariate Bernoulli.","_id":"49470087d7dc4d2196a81f19016ef12f"},{"start":"1447.885","dur":"3.135","text":"That the, the model we've talked about so far is sometimes","_id":"62e75301ad8046139bd29ef35b2618e1"},{"start":"1445.35","dur":"2.535","text":"But this is what the community calls them.","_id":"c8027bb0db6c4bc68efec6b2ed043f63"},{"start":"1441.81","dur":"3.54","text":"these are- these are really very confusing, very horrible names.","_id":"99cc496ff6d346698ffb30855c257734"},{"start":"1437.7","dur":"4.11","text":"Uh, just to give names to the algorithms we're gonna develop,","_id":"9e5d01653fec449c99293cfc8e7b0b96"},{"start":"1432.945","dur":"4.755","text":"So, um, let's see.","_id":"c8122689135e4b47b1f9b1b1f62c9daf"},{"start":"1427.245","dur":"5.7","text":"this feature vector will be shorter, okay?","_id":"3a4bd30de81b490c85b901e66401b560"},{"start":"1425.835","dur":"1.41","text":"and the shorter email,","_id":"2013f7e1cfd946ad9652cd4a6d6e4709"},{"start":"1421.8","dur":"4.035","text":"this vector, the feature vector x will be longer,","_id":"fc1277b6b62e4f12ac2c13c91608a8e5"},{"start":"1420.45","dur":"1.35","text":"So the longer email,","_id":"0eb5af1d0e794639803e42dfd70838a9"},{"start":"1410.595","dur":"9.855","text":"length of email i.","_id":"6a377622706d4e0f912c2220912ab989"},{"start":"1407.91","dur":"2.685","text":"So ni is the, uh,","_id":"efcbcb9cc37e46c39886ac2b16e899f7"},{"start":"1401.07","dur":"6.84","text":"Okay? And, uh, n is- and I guess n varies by training example.","_id":"2e0930a12b7e484c82ed016f1a24a90a"},{"start":"1395.3","dur":"5.77","text":"um, an index from 1 to 10,000 instead of just being 0 or 1.","_id":"50afdf718b8b453394ff7c69e202851f"},{"start":"1385.995","dur":"6.805","text":"but now xj is,","_id":"fe748ce7ab9248ab8f8a22cd390b787e"},{"start":"1382.68","dur":"3.315","text":"we now have a four-dimensional feature vector,","_id":"63e1fc3651e043a2a97bf1c59e2c9977"},{"start":"1379.125","dur":"3.555","text":"So rather than a 10,000-dimensional feature vector,","_id":"c561fd3b4f6b40648383e28575919e8c"},{"start":"1375.72","dur":"3.405","text":"n-dimensional for an email of length n.","_id":"6d82b2182a6e4ce5a52d31ad8bd9b5e8"},{"start":"1373.29","dur":"2.43","text":"And so this is going to be, um,","_id":"cbd261a57c5c44ad97cc10413f6c6c4e"},{"start":"1364.095","dur":"9.195","text":"we're gonna represent it as a four-dimensional feature vector, [NOISE] right?","_id":"8cb5e7e1a18e4dad8d38dfcfa2755562"},{"start":"1363","dur":"1.095","text":"\"Drugs, buy drugs now\",","_id":"3368655b03b746929d34728178718aa8"},{"start":"1361.665","dur":"1.335","text":"for that email that says,","_id":"405ad944622c4959839962e37bb1c6f5"},{"start":"1359.415","dur":"2.25","text":"this email, which is, uh,","_id":"39b083688a65420da60f52781ace4f7e"},{"start":"1354.915","dur":"4.5","text":"Just a different representation [NOISE] for, um,","_id":"929d6b3bd2ea4835a47abdc16756b2b9"},{"start":"1351.45","dur":"3.465","text":"emails and just mapping them to a feature vector that's always the same length.","_id":"cfdddc4a78fc41a08511ae7687b07d85"},{"start":"1348.465","dur":"2.985","text":"and somehow you're taking very short or very long","_id":"5541603b30884054aa37b867736ad7a3"},{"start":"1346.08","dur":"2.385","text":"or a 1,000-word email, um,","_id":"91f5cd453e0543bb96f299873c3a9bf1"},{"start":"1344.235","dur":"1.845","text":"You can have a five-word email,","_id":"23aeed7fa5554a38a8217dd8441bf4bb"},{"start":"1341.715","dur":"2.52","text":"has a property that they can be very long or very short.","_id":"40eebd122d544101a7ca0339e7afca47"},{"start":"1340.02","dur":"1.695","text":"And I think text data has a,","_id":"931a4c4df87241f483a9895d142710a8"},{"start":"1332.58","dur":"7.44","text":"uh, which is specific to text.","_id":"4a9d6c61e4204f30aaf62e33419fa56a"},{"start":"1329.87","dur":"2.71","text":"Um, [NOISE] there's a different representation,","_id":"32b7d8404df54a34a9930e909b8fa59e"},{"start":"1325.68","dur":"3.76","text":"and maybe should be given more weight for your- in your classifier.","_id":"b12418d827674c19a40ea216edc2c2b1"},{"start":"1323.805","dur":"1.875","text":"where the one-word drugs appear twice,","_id":"ba827737afa24ee09a2eac2f570f94bf"},{"start":"1320.1","dur":"3.705","text":"And that's part of why it throws away the information that's, uh,","_id":"eaf802c677a74741a83ff5932ac32c0e"},{"start":"1317.19","dur":"2.91","text":"you know, each feature is either 0 or 1, right?","_id":"2de379eada3c49f8a23d14b97841c3d7"},{"start":"1314.46","dur":"2.73","text":"and in this feature representation, um,","_id":"b2c7dfb9da7d460c8dfaaf529d7ccd9a"},{"start":"1309.945","dur":"4.515","text":"So that's losing a little bit of information, um, uh, and,","_id":"22f972e7a2c6425582fa78816e2d17de"},{"start":"1306.675","dur":"3.27","text":"the fact that the word drugs has appeared twice, right?","_id":"3bec394fdd5943c9b4fd8d309499108f"},{"start":"1303.66","dur":"3.015","text":"one interesting thing about Naive Bayes is that it throws away","_id":"b641467deabe47c0b1e866fbde098471"},{"start":"1300.915","dur":"2.745","text":"Okay? Now, one, one- so, um,","_id":"8b757d2ff22243389568e543f412ca2d"},{"start":"1298.35","dur":"2.565","text":"And they put a 1 there, and a 1 there, and a 1 there.","_id":"79bc54d483104d22b3e96daa34488d62"},{"start":"1296.4","dur":"1.95","text":"you know, 0, 0, [NOISE] right?","_id":"a725c1c798d24f7fb8e1152ff5c31e3f"},{"start":"1292.605","dur":"3.795","text":"Um, then the representation for x will be,","_id":"9311efb9dce042b5a174b165faa01ac6"},{"start":"1289.86","dur":"2.745","text":"uh, 10,000 words in the sorted dictionary.","_id":"be4007a659894f7aaa3e7a8b613b8d96"},{"start":"1285.93","dur":"3.93","text":"and let's say now is the word- is the 6,200th word in your,","_id":"f94f22c31b6d4bf6b6693b6be85446ec"},{"start":"1283.845","dur":"2.085","text":"drugs is word 1,600,","_id":"700cdc109edb47d29add7dddc1e22e89"},{"start":"1281.385","dur":"2.46","text":"Let's say the word buy is word 800,","_id":"7b17051092444ba38bc8a529d8676736"},{"start":"1278.895","dur":"2.49","text":"uh, just to, you know, make this example concrete.","_id":"66ae2055854640bfb1f84f6098437ed0"},{"start":"1277.2","dur":"1.695","text":"aardvark is worth 2,","_id":"36c4c1a22ccf4edea9db47868c308e80"},{"start":"1275.34","dur":"1.86","text":"then I guess- let's say a is worth 1,","_id":"4d171669c3084264aa32b78a116a3d16"},{"start":"1273.12","dur":"2.22","text":"uh, if you have a dictionary of 10,000 words,","_id":"710647b38e244f6ab033367c3b0ec0dd"},{"start":"1264.99","dur":"8.13","text":"[LAUGHTER] Um, so if,","_id":"dddda4ea077d4059972b390a841ce007"},{"start":"1263.49","dur":"1.5","text":"I'm not telling any of you to buy drugs.","_id":"25197d6dca17400685f421599d7a6806"},{"start":"1257.61","dur":"5.88","text":"[LAUGHTER] This is meant as an illustrative example,","_id":"f1d34ce7445443018a2ed749287d7995"},{"start":"1253.725","dur":"3.885","text":"you know, a very spammy email that's \"Drugs, buy drugs now\",","_id":"223b115f3d504688863a7fd9bcee16c1"},{"start":"1250.22","dur":"3.505","text":"So let's say you get an email that's,","_id":"01c5419f126741a2affb6b01370f55ac"},{"start":"1238.53","dur":"5.95","text":"With a dictionary a, aardvark, buy,","_id":"3f4a66f2654e4a82913fe6bd40c2cbfa"},{"start":"1231.88","dur":"6.65","text":"Uh and so our feature representation for x so far was the following, right?","_id":"64222773048b4d9b961495bf64dd1af6"},{"start":"1226.66","dur":"5.22","text":"you that is actually much better for the specific problem of text classification.","_id":"faaa69997ddb41e4bd620ea5dd19f31c"},{"start":"1221.32","dur":"5.34","text":"there's a different variation on Naive Bayes that I want to describe to","_id":"4e9d1d8d699349ecb6fd4f34678fb4aa"},{"start":"1217.345","dur":"3.975","text":"Now, um, there's, uh,","_id":"c2812460d4c242b59459cdee2bb2e0ec"},{"start":"1213.49","dur":"3.855","text":"for example, if a house is likely to be sold in the next 30 days.","_id":"dfc70f2556294ee69249d4e63fc947f7"},{"start":"1209.82","dur":"3.67","text":"And so this is how you can apply Naive Bayes on other problems as well including cost line,","_id":"e617b7b460a64dbb8d0293f070121b42"},{"start":"1200","dur":"9.82","text":"All right. Now, uh, right.","_id":"06d16e95bf7c49a5903e5933df01b5f1"},{"start":"1186.18","dur":"6.46","text":"you know, most people will start off with discretizing things into 10 values.","_id":"488b24efedfd4cdfaaccfa648f274207"},{"start":"1183.57","dur":"2.61","text":"But if you ever discretize var- variables,","_id":"d023c379c6b84c8aa0ae7cd253b45bfd"},{"start":"1181.11","dur":"2.46","text":"I- I drew 4  here so I don't have to write all 10 buckets.","_id":"cbb17e220c244beabeb70c2827997e7b"},{"start":"1178.005","dur":"3.105","text":"Uh, just as a- it often seems to work well enough.","_id":"19824e625c7c417abe6c3a53c38f8f06"},{"start":"1174.9","dur":"3.105","text":"discretize variables into 10 values, into 10 buckets.","_id":"49a26ac2d70f4bbbbca576b94a11f573"},{"start":"1172.23","dur":"2.67","text":"typical rule of thumb in machine learning often we","_id":"815484ba0ee844689c2b51cb1a8af4db"},{"start":"1169.425","dur":"2.805","text":"Um, and if you ever discretized variables, a","_id":"771c5d5057b74a1fa4d16ef664fa4410"},{"start":"1165.54","dur":"3.885","text":"discretize the size of a house into four values.","_id":"d566eb0be1ff4e43b85aead2b3cf1c3d"},{"start":"1162.495","dur":"3.045","text":"probability mass function probably over four possible outcomes if you","_id":"fc4833d642ae425ba36c98f6aba22e78"},{"start":"1161.13","dur":"1.365","text":"this can be a probably, uh,","_id":"86bb0e8769e94db2abc2e3a24654c853"},{"start":"1157.17","dur":"3.96","text":"So instead of a Bernoulli distribution over two possible outcomes,","_id":"6d5664cb7a164253ad7182c0dbd708d6"},{"start":"1155.28","dur":"1.89","text":"uh, estimators and multinomial problem.","_id":"701aa35294864f90ade1b542b0518212"},{"start":"1153.375","dur":"1.905","text":"um, this can be a,","_id":"573c804ff8c04e10ae799fb093e5d832"},{"start":"1149.325","dur":"4.05","text":"Right? Where if- if X now takes on one of four values there then,","_id":"43edc970cb22486b815d8df0894dac73"},{"start":"1140.4","dur":"8.925","text":"xj given Y where now this can be a multinomial probability.","_id":"acfd85bf7b5c4f77aa91fa1b9bf9271d"},{"start":"1132.42","dur":"7.98","text":"Product from i equals 1 through n of p of","_id":"39878cb09ce04a51b9bec2460cb0dd76"},{"start":"1130.11","dur":"2.31","text":"this is just the same as before.","_id":"5e4e0a784c424706ab91890427efeaa8"},{"start":"1127.2","dur":"2.91","text":"then probability of x given y,","_id":"f1fa422d941f4e8b9149770c646262be"},{"start":"1122.865","dur":"4.335","text":"Um, and if you want to apply Naive Bayes to this problem,","_id":"4d390d256aca4fd299db2f61fe5ef130"},{"start":"1118.65","dur":"4.215","text":"So that is how you discretize a continuous valued feature to a discrete value feature.","_id":"32f5e1de32964d64848c1ec5cefa8777"},{"start":"1113.58","dur":"5.07","text":"Then you can set the feature XI to one of four values, right?","_id":"3c2e04b1c71b4235a8b42ca9ede6374e"},{"start":"1106.755","dur":"6.825","text":"you know, 400 to 800 or 800 to 1200 or greater than 1200 square feet.","_id":"75332ee8786f48e78560647b8260f9e6"},{"start":"1100.47","dur":"6.285","text":"assert the size is less than 400 square feet, uh, versus,","_id":"3770922669c1438ab207d4826a3def73"},{"start":"1096.54","dur":"3.93","text":"a discrete feature would be to choose a few buckets,","_id":"107396962d3540d3879f23468a97a125"},{"start":"1093.585","dur":"2.955","text":"then one way to turn the feature into","_id":"55a5071d4400431680b82b5ae91bf1f9"},{"start":"1087.99","dur":"5.595","text":"Um, so if one of the features is the size of the house x, right,","_id":"145da3bad27d4fd8bd646b30ee98b836"},{"start":"1086.385","dur":"1.605","text":"So it's a classification problem.","_id":"70cc22ae683341ce8ed21dc780a7dccf"},{"start":"1083.535","dur":"2.85","text":"what is the chance of this house to be sold within the next 30 days?","_id":"334b5b1a85894dd7af23ad3b24eeab35"},{"start":"1081.69","dur":"1.845","text":"which is you're listing a house you want to sell,","_id":"c66a37094fa8429c831dd6a0e309fb82"},{"start":"1078.66","dur":"3.03","text":"Let's say you have a classification problem instead,","_id":"adedfc0d92344a1fbeff7e35a92debff"},{"start":"1076.62","dur":"2.04","text":"That was our very first world meaning example.","_id":"5b517eb1da624d2db3013e5705192972"},{"start":"1074.1","dur":"2.52","text":"We talked about predicting housing prices, right?","_id":"6ba83675ffaf4bbfb7aef438dd18609a"},{"start":"1069.825","dur":"4.275","text":"um, then the generalization- actually here's one example.","_id":"bb06822521494734928bbb89c6d1f51b"},{"start":"1063.87","dur":"5.955","text":"when the features are multinomial valued,","_id":"5b5c65c9ca9e42adb676a752049005a3"},{"start":"1061.05","dur":"2.82","text":"actually one quick generalization, uh,","_id":"4cc2bd0485d24124afc1d408f0fa46bc"},{"start":"1057.825","dur":"3.225","text":"Um, and so, um,","_id":"17638a6c40a04a0ab92cac3b13f1edd3"},{"start":"1055.47","dur":"2.355","text":"the features were binary valued.","_id":"94cf39d2407c41a38fecc9cac7377928"},{"start":"1049.897","dur":"5.573","text":"So, um, in- in the- examples we've talked about so far,","_id":"47a173a127ba496387fe9502f83a9922"},{"start":"1040.48","dur":"9.417","text":"[NOISE]","_id":"3d04621e8e1343f292632a03f3928762"},{"start":"1039.6","dur":"0.88","text":"Um,","_id":"5b569338a7974ce8b37d8785c0c184b1"},{"start":"1039.09","dur":"0.51","text":"All right.","_id":"7cd3d112c8954304a37b7f5012dd32f9"},{"start":"1032.231","dur":"6.859","text":"[NOISE].","_id":"c6e5bc6b686646bfb12abacf76a2acc7"},{"start":"1030.78","dur":"1.451","text":"[LAUGHTER].","_id":"e6bc9e60e1dc42209ce5d0f32e53dc28"},{"start":"1028.845","dur":"1.935","text":"They're doing much better right now. That was a few years ago.","_id":"f4043066ecbb4a4a95a4024b40cf4983"},{"start":"1027.66","dur":"1.185","text":"Because, okay, I love our football team.","_id":"a367c50db93146c783315b359014852e"},{"start":"1025.92","dur":"1.74","text":"[LAUGHTER].","_id":"9c85bd80667d4782a4970f0f9fc77b9f"},{"start":"1024.255","dur":"1.665","text":"you know, they lost.","_id":"b2b6de5d1b414912bca11bc9423a53ae"},{"start":"1020.595","dur":"3.66","text":"I- I was actually following the Stanford football team that year so,","_id":"68af6a8beaf54cc785e7009df5f1d0b9"},{"start":"1018.02","dur":"2.575","text":"All right. Oh, by the way,","_id":"1c27fc689e0f45498e70534187c016ad"},{"start":"1006.57","dur":"3.34","text":"Thank you. Er, yes thank you.","_id":"54775913501c4e31a1c5ef6144c28ae0"},{"start":"1003.51","dur":"3.06","text":"Oh sorry. This is y. Er, oh yes.","_id":"ffe5c8bb00c6410094f89b8b29044421"},{"start":"1000.21","dur":"3.3","text":"[inaudible]?","_id":"048d9d0785e14044a6771819f5bfeed4"},{"start":"994.075","dur":"6.135","text":"Any questions about this? Yeah.","_id":"237dfbb237f344e1a9311f0d40ae1d03"},{"start":"989.695","dur":"4.38","text":"Uh, this is very confusing first algorithm. All right.","_id":"7748751bc350403092a0d1952f22e24b"},{"start":"985.39","dur":"4.305","text":"uh, and then- and classification time is just multiplying a bunch probabilities together.","_id":"656a0bdd2ad44ac0899931086d7e672d"},{"start":"982.39","dur":"3","text":"very efficiently, right, just- just by counting,","_id":"733f6f39be154d2191144b4d9ee98708"},{"start":"980.31","dur":"2.08","text":"uh, uh can be done, you know,","_id":"c552b613e0f746f3888eff476c516bc6"},{"start":"977.98","dur":"2.33","text":"Estimated parameters is just counting ,um,","_id":"0fd51e71701f4e8f8720b1a25e82992f"},{"start":"974.215","dur":"3.765","text":"And one nice thing about this algorithm is is so simple, right?","_id":"23e842f872784a71a4b9d3de84448c90"},{"start":"970.27","dur":"3.945","text":"it's not- it's not like a great spam classifier but it's not terrible either.","_id":"16eb3fd6c98c4f7499c275f1f339aee4"},{"start":"966.31","dur":"3.96","text":"Okay. Um, and so if you implement this algorithm,","_id":"89f878b7456c4564b6ae69e6d8eca98e"},{"start":"964.21","dur":"2.1","text":"you know, the 0 over 0.","_id":"3e657644046f454a8415439e8edabb52"},{"start":"962.23","dur":"1.98","text":"which takes away that problem of,","_id":"10bd2678570e4ad986cc8bad02490a3a"},{"start":"959.62","dur":"2.61","text":"exactly 0 or exactly 1,","_id":"a1abd28215f64d919d1ea58656971686"},{"start":"957.19","dur":"2.43","text":"means that your estimates are probably- these probabilities they're never","_id":"12432af55bb542cfa071ac75b90bb0d5"},{"start":"952.06","dur":"5.13","text":"you add one to the numerator and add two to the denominator and this","_id":"0e2013e464854d8d8c523756609eca7f"},{"start":"950.32","dur":"1.74","text":"And with Laplace smoothing,","_id":"ef10116ce4fa41bbbe3a06ed68787a5b"},{"start":"944.37","dur":"5.95","text":"Um, so that's the maximum likely estimate.","_id":"a3d1746637e34fde88b641139201eec7"},{"start":"934.375","dur":"8.285","text":"Right?","_id":"73e0a665a9ee4c92a9e8a8cc1e9e4f01"},{"start":"929.755","dur":"4.62","text":"Um, I'm just gonna copy over the formula from above.","_id":"364ed09b7eb843e79b73568acf714b04"},{"start":"924.505","dur":"5.25","text":"the way this mod- modifies your parameter estimates is this.","_id":"458342bd8d0c4bc5be1b10cccb327828"},{"start":"914.905","dur":"9.6","text":"Okay? So for Naive Bayes,","_id":"3fdba67d74de465da8bbb22e05f97578"},{"start":"911.485","dur":"3.42","text":"um, you add k to the denominator.","_id":"b7be5ae87bf44b888ccbbbeeb1e4ed0a"},{"start":"908.44","dur":"3.045","text":"you'd add one to the numerator and,","_id":"f4aa4658a62a4025bb1e5715d7eee09c"},{"start":"906.47","dur":"1.97","text":"And for the fast-moving,","_id":"1e8742671ed542948fcdd4eacde1b15f"},{"start":"903.27","dur":"3.2","text":"the maximum likelihood estimate.","_id":"88394fbee2cc46aa9d100f6368def69f"},{"start":"894.011","dur":"9.259","text":"um, so- so that's","_id":"b2d5726a195b457caacc35961c77ac21"},{"start":"880.56","dur":"7.76","text":"then you estimate the chance that X being i to be equal to,","_id":"7fe0a2b66a974b34a58bc207a88996d0"},{"start":"872.29","dur":"8.26","text":"If- if you're estimating probabilities for a k way random variable, um,","_id":"a96f3f8c731a4434a0a3aa6374a781c4"},{"start":"864.88","dur":"7.41","text":"uh, if y, er, excuse me.","_id":"c4be718bbf4441879ca9f01fbe232715"},{"start":"862.885","dur":"1.995","text":"Um, and more generally,","_id":"50aaa6257c7248ae9b1ca84f95f4dd1b"},{"start":"859.57","dur":"3.315","text":"counts you actually saw for each of the possible outcomes.","_id":"fb06d7f6598643aba1fa12d32adcc602"},{"start":"856.51","dur":"3.06","text":"take this formula and add 1 to the number of","_id":"33e15412e7074fa0a71d9e859a0c4b64"},{"start":"854.14","dur":"2.37","text":"But mechanically, what you should do is, uh,","_id":"79e5124c729944298b2597b32f7e37a2"},{"start":"851.2","dur":"2.94","text":"a Bayesian statistics- advanced Bayesian statistics classes.","_id":"267aab656b0f4666a7638670ec4a1627"},{"start":"849.805","dur":"1.395","text":"Um, uh, it's taught in sort of","_id":"e988c7c464a84115a6e198b932fae47c"},{"start":"846.805","dur":"3","text":"If you don't understand what I just said in the last 30 seconds, don't worry about it.","_id":"24b94bb14e874639b3bff74b24f97e44"},{"start":"843.07","dur":"3.735","text":"this is actually a Bayesian optimal estimate of the chances of the sun rising tomorrow, okay?","_id":"0737a40bbf0d4fdf8547394fb033b29a"},{"start":"838.345","dur":"4.725","text":"then after a set of observations of this coin toss of whether the sun rises,","_id":"2a407a8712de4705879beaf45bec41e1"},{"start":"835.735","dur":"2.61","text":"you know, in the unit interval anywhere from 0 to 1,","_id":"600d4619bfef49a4a38771369f643ca9"},{"start":"832.18","dur":"3.555","text":"So if the chance the sun rising tomorrow is uniformly distributed,","_id":"4a72860be5eb41eb9bb4e757ff9a331f"},{"start":"827.725","dur":"4.455","text":"where the uniform Bayesian prior on the chance of the sun rising tomorrow.","_id":"c40fafff897c427d9eb4b02deac7c6e9"},{"start":"825.04","dur":"2.685","text":"But it turns out that if you assume that you are Bayesian,","_id":"78a92fcdc1dc43b0b9a9c9d76d2c9fc0"},{"start":"823.99","dur":"1.05","text":"we don't need to worry about it.","_id":"e252f75b9dcc40aca38126c3a0f8ae61"},{"start":"819.475","dur":"4.515","text":"And this is actually an optimal estimate under I'll say- I'll say the same assumptions,","_id":"9cdadd2e3b8c4dba8f32098e8eb97605"},{"start":"817.12","dur":"2.355","text":"you know, really the chance the sun will rise tomorrow.","_id":"361b65ed210f473aad01dd8a283c03f5"},{"start":"813.895","dur":"3.225","text":"was- he derived the optimal estimate- way of estimating,","_id":"3f14a18318e0498dadc6a52ef07714a3"},{"start":"810.685","dur":"3.21","text":"Um , uh, uh, and so his reasoning","_id":"6cfccaf046e04bfd998aa6a6c5fe0bf9"},{"start":"808.27","dur":"2.415","text":"or who- who knows what will happen in this galaxy?","_id":"d363ce1ae39e4ddf94ab4f876c23de5a"},{"start":"806.47","dur":"1.8","text":"certain because maybe something will go wrong","_id":"7b016fc815fd46059c490ca4706aec8f"},{"start":"802.96","dur":"3.51","text":"we can be really certain the sun will rise again tomorrow but maybe not absolutely","_id":"ac71aebf2ae040a09c12ba4b85f5fac8"},{"start":"800.53","dur":"2.43","text":"we've seen the sunrise 10,000 times, you know,","_id":"3cf00f2ded944b6dbc82e85c14ff4f5c"},{"start":"799.135","dur":"1.395","text":"And so his reasoning was, well,","_id":"5ff9a36224e94ca5b91f24c25e208e44"},{"start":"796.81","dur":"2.325","text":"the sun will still rise tomorrow, right?","_id":"40001623145a447c9576184ee1573c83"},{"start":"794.515","dur":"2.295","text":"but tha- that doesn't mean we should be absolutely certain","_id":"aa1c0e5f42034721b70224a4161aacd6"},{"start":"790.12","dur":"4.395","text":"And the reasoning was, well, we've seen the sunrise all times and so, uh,","_id":"8ae4e01159024a9e847e797965b47f1c"},{"start":"787.27","dur":"2.85","text":"He actually tried to estimated the chance of the sun rising the next day.","_id":"2b2f7b5e57f84e9eb785a6ca3ad27646"},{"start":"784.195","dur":"3.075","text":"uh, very influential mathematician.","_id":"24900a2845b34a5f926a85eb2aee370c"},{"start":"781.54","dur":"2.655","text":"it's an ancient that -- well known,","_id":"9be427d3d0964621ab269f4517eee4b6"},{"start":"778.42","dur":"3.12","text":"Uh, Laplace, um, uh, you know, uh,","_id":"76c3cfd4a0494fcaa4599b37190d8548"},{"start":"776.98","dur":"1.44","text":"I didn't just make this up in thin air.","_id":"527e700be711477cb3a4a3397f4fea15"},{"start":"773.74","dur":"3.24","text":"a cert- certain- certain set of circumstances under which there's more estimates.","_id":"63e4446adf0f4a2d987e2c32ee6a0494"},{"start":"771.415","dur":"2.325","text":"Um, uh, and the- there's actually","_id":"97a979ec138a492681157de1e91b8859"},{"start":"768.46","dur":"2.955","text":"uh, them winning or losing the next game.","_id":"11d195b4a4f84b869ae3734c29ba7c7d"},{"start":"765.73","dur":"2.73","text":"it is a more reasonable estimate for the chance of,","_id":"c1375399d3d640d1930a19014ff2588e"},{"start":"762.475","dur":"3.255","text":"And that's actually a more reasonable may- maybe","_id":"a3afc3b8d52a4c00b7070b181483fa5f"},{"start":"758.53","dur":"3.945","text":"And so this ends up being 1 over 6, right?","_id":"8fb3a7880b624239812337f534bbf670"},{"start":"752.68","dur":"5.85","text":"you're gonna end up adding 1 to the numerator and adding 2 to the denominator.","_id":"b5ada555326d4bcea40a5491d00ef20d"},{"start":"751.27","dur":"1.41","text":"And so Laplace smoothing,","_id":"90fbcea4d9ba41579e3a3f97d3a937aa"},{"start":"748.99","dur":"2.28","text":"pretend you saw 1 more than you actually saw.","_id":"ada8630c0e0e4cdd83232b14653fda08"},{"start":"744.625","dur":"4.365","text":"Right? So if you actually saw 0 wins, pretend you saw one and if you saw 4 losses,","_id":"6e72d70add624837ae2117862697e64b"},{"start":"736.975","dur":"7.65","text":"just add 1 to the number of wins we actually saw and also the number of losses add 1.","_id":"8daa278b75a945e8bf0a8c9e05f59bf4"},{"start":"734.83","dur":"2.145","text":"the number of wins, you know,","_id":"316a182957e24491b8e2641032b3d57e"},{"start":"730.285","dur":"4.545","text":"imagine that we saw the positive outcomes,","_id":"6f4742b1e9fc4f859363f81694359f71"},{"start":"727.71","dur":"2.575","text":"what we're going to do is, uh,","_id":"b9264d2e5cfe4478b3903999f6969c60"},{"start":"719.995","dur":"3.925","text":"Um, and so what Laplace smoothing,","_id":"6d84b302cc0343f2ac48e86c13949c44"},{"start":"716.965","dur":"3.03","text":"um, this is not a good idea.","_id":"0339c665dfa841929da1535c66563aee"},{"start":"714.91","dur":"2.055","text":"And- and- and just statistically, this is not,","_id":"bed1c96073564896b7ef7329d406d034"},{"start":"713.77","dur":"1.14","text":"Absolute certainty.","_id":"7e8121bd50514ad1a0f1e8715a69a2a1"},{"start":"710.44","dur":"3.33","text":"They lost 4 games, but you say, no, the chances of their winning is 0.","_id":"97c39645cc6840238fcdccb25519ca41"},{"start":"709.48","dur":"0.96","text":"[LAUGHTER].","_id":"4afd52ead0934a7ba8cd366756d40c09"},{"start":"707.08","dur":"2.4","text":"Um, that's kinda mean, right?","_id":"3c8b84a69cb84b4b86504d3af0d30431"},{"start":"705.115","dur":"1.965","text":"Which is equal to 0, okay?","_id":"aaefd1924b854d57aaeba81f79a334b4"},{"start":"702.085","dur":"3.03","text":"number of losses was 4, right?","_id":"b0b0128e767c4df28168df46d2f1beca"},{"start":"696.19","dur":"5.895","text":"you estimate this as 0 divided by number of wins with 0,","_id":"d4ea78f848484c659ffb1c4132989289"},{"start":"693.625","dur":"2.565","text":"And so in this case, um,","_id":"c0b384225ef14787bc33fc9fef6a7316"},{"start":"686.005","dur":"7.62","text":"and divide that by the number of wins plus the number of losses.","_id":"7c356d2d1c0848c7b2930558afd1a13a"},{"start":"681.085","dur":"4.92","text":"Well, maximum likelihood is really count up the number of wins, right,","_id":"9cd0b41447804f92a5f07f6083536e3a"},{"start":"677.575","dur":"3.51","text":"you would estimate the probability of their winning.","_id":"f95efaeef4d64f0fbeaa62328386516c"},{"start":"675.699","dur":"1.876","text":"so let's say this is the variable x,","_id":"161e34bcb69f4b248e86e191a197d1f4"},{"start":"672.085","dur":"3.614","text":"Right? Now, if you use maximum likelihood,","_id":"3e76854a664d41bda0b64eefe5c3c92e"},{"start":"667.93","dur":"4.155","text":"what's your estimate of the chances of their winning or losing?","_id":"623ecc78012e472e842c63cbec6c0cc9"},{"start":"665.14","dur":"2.79","text":"you follow them to their over home game,","_id":"37782d32c4da48c4b247c58506a90620"},{"start":"661.195","dur":"3.945","text":"when you go on- there's actually a game on New Year's Eve,","_id":"6a277452548046828035cda85fa356f6"},{"start":"658.135","dur":"3.06","text":"The question is, after this unfortunate streak,","_id":"b78dfb952697461586b315b64edbc82c"},{"start":"655.195","dur":"2.94","text":"You followed them to every single out of state game and watched all these games.","_id":"3aba41722b974762933940f1dc9b9be8"},{"start":"653.05","dur":"2.145","text":"And so you're, you know, Stanford football team's biggest fan.","_id":"c3f2525a3e0f4a7b894dc77bd034478f"},{"start":"648.685","dur":"4.365","text":"these are all the away games- almost all the out of state games we played that year.","_id":"26e19cc2ebab4fa6b1ae9b378c34f2ec"},{"start":"646.42","dur":"2.265","text":"And the question is,","_id":"99a5174889b244d5aed8af6326dd3541"},{"start":"643.39","dur":"3.03","text":"[LAUGHTER].","_id":"9fc930c5d43349dc96f253472fec2d69"},{"start":"639.75","dur":"3.64","text":"We played Caltech, we did not win that game.","_id":"1fa71f38647c4e00abb2a05d57cc844e"},{"start":"634.17","dur":"4.43","text":"Arizona, we did not win that game.","_id":"e55b8e64e4f747f98dd3d2829c4b4ed8"},{"start":"624.295","dur":"7.855","text":"Then on 10/10, we played Oregon State and we did not win that game.","_id":"4e05d823e671457d923c4016744584a0"},{"start":"619.87","dur":"4.425","text":"And, um, uh, we did not win that game.","_id":"11cfc01d84df41c19166c2aca49b07e9"},{"start":"617.29","dur":"2.58","text":"all the stay games we played that year, right?","_id":"56eb9869225f4dc78921f57fd193da52"},{"start":"615.07","dur":"2.22","text":"you know, actually these are all the, uh,","_id":"fcb2e8d188204f14ad3072e98df59eb3"},{"start":"609.88","dur":"5.19","text":"our football team played to Wake Forest and,","_id":"da6e1effcca2433bbb253e36bc3d69c0"},{"start":"606.19","dur":"3.69","text":"But that year on 9/12, um,","_id":"127f44de9c524a97ac2be1e472af3a52"},{"start":"603.88","dur":"2.31","text":"um, just a few years ago now.","_id":"bd4b7275181641038c0c3804fcd64754"},{"start":"599.605","dur":"4.275","text":"I was tracking the progress of the Stanford football team,","_id":"ccc3c94865c945d29ede6b7298f1ed7d"},{"start":"598.36","dur":"1.245","text":"So several years ago,","_id":"f2ef6b2bb6384364986ae21372003c54"},{"start":"596.23","dur":"2.13","text":"We will come back to apply Laplace smoothing in Naive Bayes.","_id":"739bddc3397842c4b59cc531c9d1faf4"},{"start":"595.18","dur":"1.05","text":"I want to talk about Laplace smoothing.","_id":"12b1fe9e670f4c4ca92a40224aa11355"},{"start":"592.87","dur":"2.31","text":"but several years ago- so- so let me put aside Naive Bayes,","_id":"d84f1637f066493881417ad295f5b5e3"},{"start":"591.55","dur":"1.32","text":"this is- this is all the data,","_id":"9bb0a36396b94bf9a376cc7f5be5d78f"},{"start":"589.69","dur":"1.86","text":"So, you know, several years ago,","_id":"1d4acfbe682341c79997f25d763b5c44"},{"start":"587.25","dur":"2.44","text":"Let's see. All right.","_id":"a7fbdd0b9d204f1f96e91a2119caaeff"},{"start":"579.27","dur":"4.63","text":"Let me use a different example for now. Right? Um.","_id":"c0a2732789574a759209f4717eee1b56"},{"start":"574.905","dur":"4.365","text":"use a- a- a- Yeah,","_id":"72595c57751a4bd783654011097a90fe"},{"start":"568.39","dur":"6.515","text":"Okay? And, um, let's- let's- In order to motivate Laplace smoothing, let me, um,","_id":"3187aad55f38456b96a7ff92c72b85b6"},{"start":"566.315","dur":"2.075","text":"um, address this problem.","_id":"1602dcb7643b410fbf6c586ec4eb664f"},{"start":"563.735","dur":"2.58","text":"which is a technique that helps,","_id":"252faf42d71947649a85dbe2b7564431"},{"start":"557.175","dur":"6.56","text":"Um, so [NOISE] what I want to do is describe to you Laplace smoothing,","_id":"c00f5ad694a54857820276f4a79aaa9d"},{"start":"553.76","dur":"3.415","text":"because you have not seen it once yet, right?","_id":"1f415274eced4ce9952388eed3c03944"},{"start":"550.68","dur":"3.08","text":"To estimate the probability of something as 0 just","_id":"5682225da1344313a7c8126d6cf37fe9"},{"start":"548.61","dur":"2.07","text":"statistically, it's just a bad idea, right?","_id":"c1033b6482974b04b6117544a2f7ded3"},{"start":"546.57","dur":"2.04","text":"it turns out that, um,","_id":"d645761811dc4fb0a41f0289eec3950c"},{"start":"542.815","dur":"3.755","text":"Now, apart from the divide by 0 error, uh,","_id":"58479d752105461fbf8d5feead289b16"},{"start":"535.17","dur":"7.645","text":"your spam classifier will estimate this probability as 0 over 0 plus 0, okay?","_id":"b780520bb3a843b787aafb3459fcffd6"},{"start":"530.345","dur":"4.825","text":"The first time you get an email from your teammates that has the word NIPS in it,","_id":"73ad2c9d5249423e9b2ded5c9deb9b6e"},{"start":"528.275","dur":"2.07","text":"you know, or two months from now, whenever.","_id":"82a544982ed6422d8cf48018b58ac24d"},{"start":"526.46","dur":"1.815","text":"and if tomorrow or- or,","_id":"e21668e053604db3aa12cfc93e030011"},{"start":"521.33","dur":"5.13","text":"today using all the data you have in your email inbox so far,","_id":"803f180ca37a40fe98ecf1115e334fd9"},{"start":"518.66","dur":"2.67","text":"And so what that means is if you train a spam classifier","_id":"74c23f5812f24b9d8761aa15b64a057f"},{"start":"511.195","dur":"7.465","text":"And this is also 0 because there'll be that one term in that product over there.","_id":"5784abcd0f19473f909ba8e5b90d0b80"},{"start":"508.46","dur":"2.735","text":"well, this is also 0, right?","_id":"6aee9de734934034a384a860ad4857c7"},{"start":"505.31","dur":"3.15","text":"Um, and in the same way this,","_id":"4fb04b4e55db4593827cd106e1344346"},{"start":"502.17","dur":"3.14","text":"0  because you multiply a lot numbers, one of which is 0.","_id":"33f6a305d7cb477db321e29f6b35cd5c"},{"start":"499.245","dur":"2.925","text":"And so this thing that I've just circled will evaluate to","_id":"a898bca4fb8848daa7e492b2f20d6fa1"},{"start":"492.8","dur":"6.445","text":"um, this product of probabilities will have a 0 in it, right?","_id":"f4e982eb24414d429f508c6e6592bcd3"},{"start":"487.2","dur":"5.6","text":"then when you start to see the word NIPS in your email maybe in March of next year,","_id":"247b130a16bf4699b3b48451ae6e28c4"},{"start":"484.98","dur":"2.22","text":"let's submit our papers to NIPS conference,","_id":"126c036fc7724d029f14fad04e2d0779"},{"start":"483.105","dur":"1.875","text":"When you start getting emails from your friends saying,","_id":"30fb44ae11b745f298b4bcf162f9a18b"},{"start":"478.5","dur":"4.605","text":"March, April next year and then maybe submit it to the conference May or June of 2019.","_id":"f146549751174c60919cbebd7f4aaa11"},{"start":"476.1","dur":"2.4","text":"work on it some more by January, February,","_id":"f9e71993f0674249be36a7fda01d7355"},{"start":"474.195","dur":"1.905","text":"finish your class project this December,","_id":"2ca285cf46d54c729f75524372bed4a2"},{"start":"470.06","dur":"4.135","text":"um, sort of May or June most years so, you know,","_id":"3daf42785ff14f99ad5848b6fbb8480d"},{"start":"468.38","dur":"1.68","text":"The NIPS conference deadline is usually in,","_id":"36960db5772a4cbcaf8b9c48820ace88"},{"start":"464.305","dur":"4.075","text":"Shall we consider submitting this class project to the NIPS conference?","_id":"b4fcff5e20f14d008571d98bcb686021"},{"start":"461.94","dur":"2.365","text":"hey, you know, we like the class project.","_id":"a4fb8a08e33344429d2ddb3fb49c9d17"},{"start":"457.34","dur":"4.6","text":"and then after CS229, your project teammates sen- starts sending you emails saying,","_id":"5e5f4017799b4f77b47336d3818d1229"},{"start":"454.11","dur":"3.23","text":"you train your spam classifier on the emails you've gotten up until today,","_id":"67782ef938ff456eaeb0ff3d318059d4"},{"start":"452.28","dur":"1.83","text":"And so if, um,","_id":"029b9ca876a840ed92798598a9f7d49b"},{"start":"446.055","dur":"6.225","text":"p of X_i given y, right?","_id":"dedc71ab4d084f9a947ec55818798310"},{"start":"439.53","dur":"6.525","text":"have 10,000 words appear of X_i equals 1,","_id":"eb2c4b97b7734c41b1a05aab21c4cfcc"},{"start":"434.2","dur":"5.33","text":"that is equal to a product from I equals 1 through n. Let's say you","_id":"9f93fcb53e6b4693b25d64aa096c3517"},{"start":"430.73","dur":"3.47","text":"Yes? And if you ever calculate this probability,","_id":"eeb58d57bf454bd49c8f88638e973372"},{"start":"426.465","dur":"4.265","text":"This is phi subscript 6017 given y equals 0.","_id":"8c3a9584cfeb4f2ebf2468e33dd5eb86"},{"start":"420.02","dur":"6.445","text":"so this is your estimates parameter phi subscript 6017 given y equals 1.","_id":"a5510098cd5f4652b12e0131dc395664"},{"start":"416.09","dur":"3.93","text":"if you use these as estimates of the, of the parameters,","_id":"a946552d23c0416aa18ced2a622dc498"},{"start":"410.885","dur":"5.205","text":"yet and where this will cause the Naive Bayes algorithm to break down is,","_id":"93b57201fefa48d79d8e482a6a98228e"},{"start":"407.87","dur":"3.015","text":"the chance of something is 0 just because you haven't seen it","_id":"59bdcc86d74547d3acf76f6f898c498a"},{"start":"399.66","dur":"8.21","text":"Right? And, um, statistically it's just a bad idea to say that","_id":"860ba655dae141d78320558a8d10862e"},{"start":"397.815","dur":"1.845","text":"is just this formula.","_id":"63fa42ab43854434ba0bf60df474b596"},{"start":"395.91","dur":"1.905","text":"Right. So that's what this is,","_id":"4e539015eb4446b582ab086d318c6d6a"},{"start":"393.555","dur":"2.355","text":"you know, the number of non-spam emails I guess.","_id":"faeb5046ed6845f797aa5edc5e420b1a"},{"start":"390.86","dur":"2.695","text":"this is also 0 over,","_id":"39509c5dae5e422eac1ed1a74ab4d08b"},{"start":"384.98","dur":"2.59","text":"Um, and then similarly,","_id":"28de671651f5465fb723658ee133186f"},{"start":"381.77","dur":"3.21","text":"the numerator is 0 and so your estimate of this is probably 0.","_id":"e78e2ad9c8c244a6890962b3a02b24c0"},{"start":"377.75","dur":"4.02","text":"Uh, and so if you plug in this formula for maximum likelihood estimate,","_id":"7e2b21eafc4441f1afb39f619245edf3"},{"start":"374.66","dur":"3.09","text":"the last few ones had the word NIPS in it, um, maybe.","_id":"6c7ac6dc3a094a93b02b30bd08c67ac7"},{"start":"372.645","dur":"2.015","text":"probably none of the emails you've received for","_id":"3098be6667a84da5ad11e586e720d77f"},{"start":"369.71","dur":"2.935","text":"So if, if you train up this model using your personal email,","_id":"7802060437aa44c2b32f31c26c9e5ddc"},{"start":"365.515","dur":"4.195","text":"examples that you've labeled as spam in your email.","_id":"6bbb481c59bc4d38842a6dd4fdeece5b"},{"start":"362.24","dur":"3.275","text":"Right? Zero over the number of, ah,","_id":"dc4bcef6f08049fe9cd6c72700bfa9bc"},{"start":"352.895","dur":"9.215","text":"probability of seeing this word given that it's spam email, is probably zero.","_id":"0536324ece4343ac8c3d1eafe0086b1c"},{"start":"350","dur":"2.895","text":"you will probably estimate that, um,","_id":"46501df097e849deb45c9f6aa2e1dd1a"},{"start":"345.585","dur":"4.415","text":"email, set of emails to find these maximum likelihood estimates of parameters,","_id":"ca5e2d99e6204c9492c2683a65141e67"},{"start":"341.225","dur":"4.36","text":"Um, and so if you use your current, you know,","_id":"e791f40a8499448da17c14e76c770efb"},{"start":"337.49","dur":"3.735","text":"\"Hey, do you want to submit the paper to the NIPS Conference or not.\"","_id":"42b0ee47e6624101876a27bf78534441"},{"start":"333.04","dur":"4.45","text":"presumably you've not had a lot of emails from your friends asking,","_id":"1fee2f53658548f688d6397649a6dbc2"},{"start":"330.645","dur":"2.395","text":"But up until now,","_id":"3aaf458c3c8f4ad68b8caa515fb73871"},{"start":"327.32","dur":"3.325","text":"In your, in your 10,000 word dictionary.","_id":"daeee408e4d349f6a9828a14edae9874"},{"start":"322.7","dur":"4.62","text":"the word NIPS corresponds to word number 6017, right?","_id":"4033c9635c894a3d94ffef5adce7fd14"},{"start":"320.66","dur":"2.04","text":"Let's say that the NIPS conference,","_id":"5f8cac573cb6431990b7a7020ac22f1c"},{"start":"318.855","dur":"1.805","text":"you know, you have 10,000 words in your dictionary.","_id":"d027f6606c72444e9011e0b33dc6f997"},{"start":"314.48","dur":"4.375","text":"ah, and let's say that in your dictionary,","_id":"2f5c260f39c14d9884ec97ab4de0a93b"},{"start":"311.84","dur":"2.64","text":"NIPS stands for Neural Information Processing Systems, um,","_id":"98cdcdcd7fe04bd69ced6db82062f089"},{"start":"310.475","dur":"1.365","text":"is the conference NIPS.","_id":"2590cd55b0d84c079d5ab72293349459"},{"start":"307.73","dur":"2.745","text":"One of the top machine learning conferences,","_id":"f0093167cb344621a948f9256b9658b1"},{"start":"304.94","dur":"2.79","text":"you know, as conference papers pretty much every year.","_id":"3164fe5f8d614367a7584fbd6502b07e"},{"start":"302.465","dur":"2.475","text":"some of CS229 class projects get submitted,","_id":"0782a760b3a14e66816e54a2dbbee53c"},{"start":"301.115","dur":"1.35","text":"Right? Some, some- actually some,","_id":"d41c335c3f064d54905b57061c7bc18a"},{"start":"298.55","dur":"2.565","text":"end up submitting this to an academic conference.","_id":"b0b9b138e8314e798df9c6d2a60c480d"},{"start":"296.19","dur":"2.36","text":"they will do a class project and some of you will","_id":"ec67b335206e478e90c01ee9d5ffd8d3"},{"start":"293.43","dur":"2.76","text":"there are some CS229 students and some machine learning students,","_id":"c490c0bf7b494c22882f8ea8d5939069"},{"start":"291.495","dur":"1.935","text":"so actually eve- every year,","_id":"9b096bac0cc048bc82e9305785afe96f"},{"start":"289.425","dur":"2.07","text":"which is, um, you know,","_id":"7d343b36cc254c5284e90831fc5ac437"},{"start":"282.975","dur":"6.45","text":"So it turns out this algorithm will almost work and here's where it breaks down,","_id":"b64e0ecbe1d74b78ab0c5bc152eb92b4"},{"start":"280.02","dur":"2.955","text":"Um, all right.","_id":"e463939d3bb14d539dcb4e719cc19cb4"},{"start":"265.535","dur":"1.465","text":"Okay?","_id":"16404e1316d7450fadbefcad012396aa"},{"start":"261.58","dur":"3.955","text":"This is kinda according to Bayes rule.","_id":"ada25b132f8d449bba2b82002bc828b1"},{"start":"250.65","dur":"6.73","text":"you will calculate p of y equals 1 given X.","_id":"4e573771542b4f64a85f7aa096e3dccf"},{"start":"245.378","dur":"5.272","text":"um, let's see,","_id":"5e10990ae7884961b9215bb2f1a28cff"},{"start":"233.325","dur":"6.505","text":"Right? Um, and then finally at prediction time,","_id":"b204e3577ed3438298bd4e706ff884f2"},{"start":"231.45","dur":"1.875","text":"Did this word X_j appear?","_id":"2ce9c6af54e244a49f80f4685c0be477"},{"start":"229.275","dur":"2.175","text":"did this feature X_j appear?","_id":"2c848a2b9dbc481eab61901f1859a9b7"},{"start":"224.34","dur":"4.935","text":"uh, emails with label y equals 0 and contact y fraction of them,","_id":"14b2ea6364dc4e33a9fe04744d5f22c9"},{"start":"223.035","dur":"1.305","text":"look, at all of your,","_id":"bf1b57e99af64b448acbeb1bad8a514f"},{"start":"221.37","dur":"1.665","text":"way of writing, um,","_id":"6b0c912ead724f299aaec66025c720ec"},{"start":"216.53","dur":"4.84","text":"and this is just an indicator function notation,","_id":"baea0256f8b14900a434fb90412f53d5"},{"start":"190.695","dur":"8.145","text":"that was equal to spam and maximum likelihood estimates of this-","_id":"6c419fec3098447199bb6cbb9d825c3a"},{"start":"185.81","dur":"4.885","text":"Right? Just a fraction of training examples, um,","_id":"2e7102be6b8243c092f47d02974f2978"},{"start":"179.21","dur":"3.38","text":"you know, phi y is this.","_id":"f631d3ab939143c48e413286f9d4e675"},{"start":"176.075","dur":"3.135","text":"you will find that the maximum likelihood estimates of,","_id":"684ec8b5f5fb4ceca6c1f76ae33e014c"},{"start":"168.63","dur":"7.445","text":"Okay? Um, and so if you derive the maximum likelihood estimates,","_id":"8edca9d1a20c4f55916f6274913b9d95"},{"start":"165.14","dur":"3.49","text":"that word appearing in spam email.","_id":"0b8e59cff7a149faaba41962ee73b16e"},{"start":"161.825","dur":"3.315","text":"as well as phi subscript J given y equals 1 which is a chance of","_id":"02d06bb6c3174342842f1cc8505cba2b"},{"start":"157.82","dur":"4.005","text":"which is a chance of that word appearing in a non-spam,","_id":"d07a559268424d8ebed0d8c2680b73a2"},{"start":"153.615","dur":"4.205","text":"As well as phi subscript J given y equals 0,","_id":"5223a70fdf514ad0abd6e9684ef5ceb3"},{"start":"151.995","dur":"1.62","text":"before you've seen any features?","_id":"8f1526c679234aeca0f870dbb32bf1fa"},{"start":"150.18","dur":"1.815","text":"What's the chance that y is equal to 1,","_id":"4f60e060da0a4b3eb64285540fd17ccf"},{"start":"147.21","dur":"2.97","text":"um, phi subscript y is the class prior.","_id":"9a9549baf64945a3865cc42f1b6fdc0b"},{"start":"144.36","dur":"2.85","text":"And so the parameters that Naive Bayes model are,","_id":"40b96baedbe242d4b8d9c8a98ee0cc83"},{"start":"137.6","dur":"6.76","text":"product of the conditional probabilities of the individual features given the class label y.","_id":"a8ba02f66f22420c9fa2ef8cdcbdd6a2"},{"start":"133.085","dur":"4.515","text":"And with Naive Bayes in particular p of x given y is modeled as a, um,","_id":"f8b38e2261af484eb9d9ff16e6f8a905"},{"start":"128.63","dur":"4.455","text":"with a Gaussian and the Bernoulli respectively and Naive Bayes uses a different model.","_id":"f7de6762b75f49a3a0a03e68d7c5120e"},{"start":"124.41","dur":"4.22","text":"Uh, so Gaussian distribution analysis models these two terms","_id":"d9d557b64c8744b798f7278ebfd62734"},{"start":"118.59","dur":"5.82","text":"uh, we need to model these two terms p of x given y and p of y.","_id":"6ef6a97ab5834de9bece95211f9e6589"},{"start":"114.335","dur":"4.255","text":"And so, um, to build a generative model for this,","_id":"bab32c3e20e74be296a39de43c00cbeb"},{"start":"107.535","dur":"6.8","text":"So X_j is whether or not the indicator for whether words j appears in an email.","_id":"971d7d52738c4479b324122e90b51b3c"},{"start":"103.86","dur":"3.675","text":"and ith index in the training examples and you'll see I'm not being consistent with that.","_id":"8ef862faeae34b659a88dfda3e796a12"},{"start":"101.96","dur":"1.9","text":"to denote the indexes and the features","_id":"bccff33adc634bafaf13910432024fd8"},{"start":"97.145","dur":"4.815","text":"X_j- I've been trying to use the subscript J not consistently","_id":"892f9f7875524e649dd7e1b85af76713"},{"start":"90.44","dur":"6.705","text":"Um, so using the indicator function notation, um, X_j-, uh,","_id":"f6548f63f6934f479a7d07032f890ac0"},{"start":"87.38","dur":"3.06","text":"say, an email that you're trying to classify as spam or not spam.","_id":"9acba0734bd4490499406ed4f4f554ba"},{"start":"84.05","dur":"3.33","text":"a particular email and so this becomes your feature representation for,","_id":"c6484afca1ae4155bebd9770aa91cc6b"},{"start":"81.305","dur":"2.745","text":"whether different words appear in","_id":"76afda6f2ceb4b998174f456e6410de5"},{"start":"77.54","dur":"3.765","text":"take a dictionary and put in zeros and ones depending on","_id":"73fae9237421493397c79561426f1220"},{"start":"74.12","dur":"3.42","text":"or Twitter message or some piece of text, um,","_id":"4925b1f101ef4bc2a81868451cb7288a"},{"start":"70.19","dur":"3.93","text":"a generative learning algorithm in which given a piece of email,","_id":"938fb22562b74b039ecb1a76c59b1fe8"},{"start":"67.83","dur":"2.36","text":"the Naive Bayes Algorithm is","_id":"3fb3cb31465449fdbcf2170900f45a5d"},{"start":"63.42","dur":"4.41","text":"Okay? Um, so to recap, uh,","_id":"a956467c8041407a92e7a0671c1dd670"},{"start":"60.64","dur":"2.78","text":"um, intro to support vector machines.","_id":"57f5c208ec804daebb60b09635d45407"},{"start":"58.19","dur":"2.45","text":"uh, and then we'll start with,","_id":"ec91b5a6ce4b40ea9819c5ec73ba2906"},{"start":"54.665","dur":"3.525","text":"This is a strategy of how to choose an algorithm and what to do first, what to do second,","_id":"693f76b092644af3b3be1f6518993b89"},{"start":"52.82","dur":"1.845","text":"ah, CS229 class projects as well.","_id":"5ee4b30d141a4ddaa21aefd1e97310c8"},{"start":"49.37","dur":"3.45","text":"So this would be useful to you as you get started on your,","_id":"8ae8be49acde4ae18e26255180cf3309"},{"start":"46.585","dur":"2.785","text":"ah, advice for applying machine-learning algorithms.","_id":"fde3dfa1f96247c4af49a620614fbb5f"},{"start":"44.24","dur":"2.345","text":"Um, talk a little bit about,","_id":"d136b1104480468687eec6cc8af3f1a0"},{"start":"40.74","dur":"3.5","text":"Naive Bayes that's even better than the one we've been discussing so far.","_id":"1fba45fd458c4065b3b611ac744bb031"},{"start":"38.85","dur":"1.89","text":"Uh, and then we'll talk about the different version of","_id":"a481619e331241f9814b08ccca6c9eb0"},{"start":"37.23","dur":"1.62","text":"or for text classification.","_id":"7157e0e6f2cf4c07bb48712cc786a5dd"},{"start":"35.31","dur":"1.92","text":"email spam classification, or,","_id":"ace2d110eda84ab0a8d924436ff6b3e6"},{"start":"32.85","dur":"2.46","text":"to really make it work, um, for, say,","_id":"41b9321857bb4d839f220db8b8f0bbdc"},{"start":"28.845","dur":"4.005","text":"you need to add to the Naive Bayes algorithm we described on Monday,","_id":"fa42e44e921a492c9336204e74d5ebb3"},{"start":"24.075","dur":"4.77","text":"And, and, and so today you see how Laplace smoothing is one other idea, uh,","_id":"968ad8ee1dcc4c34aa69d38f6ec15b3a"},{"start":"20.7","dur":"3.375","text":"to build a spam classifier that will almost work, right?","_id":"73125e36888d41199dbb78dc531b0699"},{"start":"15.795","dur":"4.905","text":"um, we've described how to use Naive Bayes in a generative learning algorithm,","_id":"5e4db2c254344515a5cee01d2fea290c"},{"start":"11.685","dur":"4.11","text":"is continue our discussion of Naive Bayes and in particular,","_id":"c2c11e84504e4ea3a00536b7d76b9c23"},{"start":"7.68","dur":"4.005","text":"Um, so what I'd like to do today","_id":"1474d79e2a5e4629bd90b8042a99dba8"},{"start":"5.7","dur":"1.98","text":"Morning and welcome back.","_id":"88d5da78d8e64ec99c28e3ab5a424327"},{"start":"3.47","dur":"2.23","text":"All right. Hey, everyone.","_id":"39e15956f6dc4f65b19be229cd360091"}]