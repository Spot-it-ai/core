[{"start":"4725.77","dur":"2.44","text":"So let's break, I'll see you on Wednesday.","_id":"291667cebe66413da77531c29315f87e"},{"start":"4722.95","dur":"2.82","text":"some more fine details how to make this work even better.","_id":"7491e4d3eccb42b38152cec40caf6e2c"},{"start":"4721.135","dur":"1.815","text":"we'll come back on Wednesday and learn about","_id":"52a0f8574ff34110a32280bb28cb5f1a"},{"start":"4718.135","dur":"3","text":"So now you've learned about generative learning algorithms, um,","_id":"298980217c474e62939108e0992af403"},{"start":"4711.98","dur":"6.155","text":"All right, any quick questions before we wrap up? Okay, okay good.","_id":"9ab5ee879d534ac78ba6444c4f7ae151"},{"start":"4705.83","dur":"6.15","text":"But we'll come back to that when we talk about Laplace moving on Wednesday, okay?","_id":"d1dfb04b635a4d32a87de6cd065c7d99"},{"start":"4701.33","dur":"4.5","text":"this is zero or if- if you get zeros in some of these equations, right?","_id":"bfbda7d2705540268733cfcc48f85b3a"},{"start":"4700.205","dur":"1.125","text":"what happens if, uh,","_id":"276eae55480a40d3b0ff19d44492ac8f"},{"start":"4697.4","dur":"2.805","text":"uh, actually, the biggest problem with this algorithm is,","_id":"0e77a574571a4d088916b43f9154f6ce"},{"start":"4694.49","dur":"2.91","text":"Um, but it turns out that,","_id":"9f940c5478fb4852951d2ec873b3863e"},{"start":"4691.445","dur":"3.045","text":"even as you get new data, you can update this model very efficiently.","_id":"dd154085b92d4268b593e88330727e46"},{"start":"4687.245","dur":"4.2","text":"even as you get new- new- new uses hits mark or spam or whatever,","_id":"ed77856b2394411a937dbd8e18b467e6"},{"start":"4684.44","dur":"2.805","text":"also keep on updating this model even as you get new data,","_id":"a65061ee030a4e7a8a49fee80ac463a4"},{"start":"4681.995","dur":"2.445","text":"So you can fit this model very efficiently and","_id":"6fbfcc23769b44a19c732eb11a1fae2f"},{"start":"4680.27","dur":"1.725","text":"So there's nothing iterative about this.","_id":"5813c96d2cd443e9afbad20810b421bd"},{"start":"4677.195","dur":"3.075","text":"and then computing probabilities is just multiplying a bunch of numbers.","_id":"dbf8cddc72ef4bfb801e3a05276d28b6"},{"start":"4674.36","dur":"2.835","text":"because estimating these parameters is just counting,","_id":"c0799ad82d6a4f9e9d1d89c6ccac70d8"},{"start":"4672.26","dur":"2.1","text":"But this is a very efficient algorithm,","_id":"0ce8158affb64780a7b3cdb98b66f5b7"},{"start":"4668.57","dur":"3.69","text":"for spam classification you do better than this almost all the time.","_id":"50e2613def234a1bab4302705644c2fc"},{"start":"4665.75","dur":"2.82","text":"It turns out that if you used logistic regression","_id":"dec8e0759a4e4efe8cd93563f2bf7937"},{"start":"4661.205","dur":"4.545","text":"this is actually, it's actually a not too horrible spam classifier.","_id":"0a48440d3baf4a9d8952d83932073163"},{"start":"4658.4","dur":"2.805","text":"which we'll talk about on Wednesday, um,","_id":"13c3173b2d424a54a40d61981a76403d"},{"start":"4655.1","dur":"3.3","text":"uh, and it turns out that what one fixed to this algorithm,","_id":"70853aea62a94a5f8c12ac1c05d78cd2"},{"start":"4652.04","dur":"3.06","text":"And I mentioned, uh, one reason this,","_id":"32513345793e4d718388c41bd4dbe23b"},{"start":"4648.77","dur":"3.27","text":"um, for e-mail spam classification, right?","_id":"d5ce973451b6443ab76bdd57ac0315d3"},{"start":"4646.505","dur":"2.265","text":"but this is Naive Bayes for,","_id":"3ced816ba3a24c1596a8751c2c9be32f"},{"start":"4645.11","dur":"1.395","text":"I guess, uh, uh,","_id":"4743c6e262fa441b839472ce203c43c8"},{"start":"4643.055","dur":"2.055","text":"it will- it will nearly work,","_id":"88c62dee2bf74bfa98eb89804d64ad60"},{"start":"4637.97","dur":"5.085","text":"Um, and so it turns out that if you implement this algorithm,","_id":"054616fdc96d450d87c6a1f68487ee9e"},{"start":"4636.16","dur":"1.81","text":"the word by, okay?","_id":"ffe2a236725a4475bd7d3fa05240aa16"},{"start":"4634.705","dur":"1.455","text":"x_j equals 1 for say,","_id":"a8c6c24e07a64ff09e8a8ee6fa05d808"},{"start":"4632.665","dur":"2.04","text":"What- what fraction of them had, you know,","_id":"b97b33c9e2934a0e990ed516a57b8960"},{"start":"4629.89","dur":"2.775","text":"what fraction of them contain the word by?","_id":"dfdff72088eb4443adf24e00bf624936"},{"start":"4627.07","dur":"2.82","text":"just we have all the spam e-mails in your training set,","_id":"223833db774f4ea8b4a056f69f6ad950"},{"start":"4624.97","dur":"2.1","text":"the word by appearing in a spam e-mail is","_id":"9911fd6b0e3c4fdb833e204c4e9a009c"},{"start":"4620.69","dur":"4.28","text":"So you estimate that the chance of word j appearing- you estimate the chance of","_id":"8d94415e84c74cd58dc05b6d7b13c7ea"},{"start":"4614.765","dur":"5.925","text":"i.e., examples of y equals 1 count up what fraction of them had word j in it, right?","_id":"3cc54daa5aa5436dae1588f3515ccfed"},{"start":"4611.6","dur":"3.165","text":"find all the spam e-mails and of all the spam e-mails,","_id":"e96eb0f4653f4cc38812a26b5c17af0c"},{"start":"4609.095","dur":"2.505","text":"Look through your, uh, training set,","_id":"8d680321ca214c46a2d6bd1c5bdfdd9d"},{"start":"4603.88","dur":"5.215","text":"Okay. So that's the indicator function notation of writing notes.","_id":"9b84951bd1274a2eb6bb03c4c241772a"},{"start":"4579.97","dur":"2.96","text":"Oh, shoot, sorry.","_id":"c5c68353ed4948c2ba284e00477a727a"},{"start":"4570.89","dur":"2.17","text":"function notation.","_id":"247c059b046c47ad929961bf8bc36c67"},{"start":"4569.57","dur":"1.32","text":"I'll write this out in indicator","_id":"af3b8f00a6e942c18acaf14c7de22fad"},{"start":"4564.575","dur":"4.995","text":"Phi of j given y equals 1 is, um, well,","_id":"6acbecd1d9044ebba5be741225d9f612"},{"start":"4560.84","dur":"3.735","text":"It's just a fraction of spam e-mails and, uh,","_id":"6e637b2de90b4b8a9e90449b8b9e00d3"},{"start":"4552.2","dur":"8.64","text":"this is pretty much what you'd expect, right?","_id":"bd2684e209e244d4a48ecfc990cc1c29"},{"start":"4547.595","dur":"4.605","text":"you find that the maximum likelihood estimates of the parameters are, Phi_y,","_id":"8f97abe124264a29a97c6f4ec1f5eaeb"},{"start":"4544.55","dur":"3.045","text":"set derivatives to 0, solve for the values that maximize this,","_id":"187db97133c74683a414deff6e713162"},{"start":"4543.11","dur":"1.44","text":"take logs, take derivatives,","_id":"62cb836d0436427b890d3142383660d9"},{"start":"4538.73","dur":"4.38","text":"And the maximum likelihood estimates, um, if you take this,","_id":"71447e1eafb04b7ba0323340ea157c46"},{"start":"4534.77","dur":"3.96","text":"Similar to what we had for Gaussian discriminant analysis.","_id":"b37241554db943a3818013732f2504c9"},{"start":"4528.8","dur":"5.97","text":"given these parameters, right?","_id":"375d39dca6894fa39fe9d73565137e88"},{"start":"4522.89","dur":"5.91","text":"Is a product, you know,","_id":"af2fb48116454dc3a5c6201f6ce77e49"},{"start":"4515.87","dur":"7.02","text":"So the joint likelihood of these parameters, right?","_id":"5d782dd18e2c42e2bbe3c3c51f7b62f1"},{"start":"4512.53","dur":"3.34","text":"write down the John- joint likelihood.","_id":"fa094bf9f7da49b9962b69389e845244"},{"start":"4505.76","dur":"5.62","text":"you would s- similar to Gaussian discriminant analysis,","_id":"dea13f8ecc9f4aa5881dc99a75658708"},{"start":"4500.62","dur":"5.14","text":"And so to fit the parameters of this model,","_id":"19e43528f5e346ac94ce25d80719b87f"},{"start":"4492.29","dur":"4.57","text":"uh, in your- in your inbox is spam e-mail?","_id":"c31f4e34468244b8be88a8aa73b1ebf0"},{"start":"4489.2","dur":"3.09","text":"what's the prior probability that the next e-mail you receive in your,","_id":"376e173375b8458e90c60d71cb282135"},{"start":"4487.325","dur":"1.875","text":"Then also, what's the cost prior,","_id":"916256921c054b4ba48ac8af8b0d7b5c"},{"start":"4483.335","dur":"3.99","text":"If it's not spam e-mail what's the chance of word j appearing in the e-mail.","_id":"8fc06edf0fa649e1981dc74a28db3c4e"},{"start":"4478.82","dur":"4.515","text":"If it's a spam e-mail, what's the chance of word j appearing in the e-mail?","_id":"7feecb07be714a07b3a82c9d0bf9433e"},{"start":"4475.94","dur":"2.88","text":"if y equals 1 is spam and y equals 0 is not spam.","_id":"5d6070cb478445bbba60c631a21724d2"},{"start":"4474.305","dur":"1.635","text":"if a spam e-mail,","_id":"b39801330d4c46568376bc76f436c4e9"},{"start":"4472.01","dur":"2.295","text":"So this parameter says,","_id":"81ee05b506764b16b4802638b7ab7a95"},{"start":"4468.5","dur":"3.51","text":"we can just call this Phi subscript y, okay?","_id":"2e333bfa81454254bf70f0ecb3869130"},{"start":"4464.89","dur":"3.61","text":"And just to distinguish all these Phi's from each other,","_id":"0c1e72bbb2c14bbf9df651199f3f6938"},{"start":"4452.51","dur":"10.03","text":"0, and then Phi.","_id":"e3a1150a225f40c0b4e7e25ed09b7de9"},{"start":"4449.795","dur":"2.715","text":"phi subscript j given y equals","_id":"96047d3065e045f790b5feae6f7173ce"},{"start":"4443.33","dur":"6.465","text":"1 as the probability that x_j equals 1 given y equals 1,","_id":"21c05231845b444fb590c88ffb968e36"},{"start":"4437.75","dur":"5.58","text":"um, j given y equals","_id":"8e9048a8177e4b13a95c2c01963e2e13"},{"start":"4431.945","dur":"5.805","text":"um, are, I'm going to write it, Phi subscript,","_id":"9ef45c081d0240348739b512e6cea05b"},{"start":"4426.67","dur":"5.275","text":"All right, so the parameters of this model,","_id":"e669af99529a4006a25c2171c55a8c49"},{"start":"4389.81","dur":"7.21","text":"this is product from i equals 1 through n of p of X_i given y.","_id":"9c44d4bf0af64c88be79bda88a8ad92a"},{"start":"4387.73","dur":"2.08","text":"So just to summarize,","_id":"104e5e423b1c47eab02b2433ad197ee2"},{"start":"4381.95","dur":"4.9","text":"mortgage or discount or whatever spammy words appear, right?","_id":"7c1b547562ab405b8bbae0410666ccb6"},{"start":"4378.305","dur":"3.645","text":"whether the word by appears in it doesn't affect you believes that what- whether the word","_id":"34620e63ea1c447aac63f287da7d6ef3"},{"start":"4374.78","dur":"3.525","text":"this piece- if I tell you that this piece of e-mail is spam then","_id":"92b58e8228e0442386baab4e2ca1bef1"},{"start":"4371.345","dur":"3.435","text":"But intuition is that if I tell you whether","_id":"77e379f2d7bd48279e99ea7fc14741c1"},{"start":"4369.23","dur":"2.115","text":"that's all you need to derive Naive Bayes.","_id":"8a050c2f0f8d4aecaad605f4600b627b"},{"start":"4366.905","dur":"2.325","text":"um, and you just use this equation,","_id":"6a46c10994134f9c94460483d558c7b0"},{"start":"4361.805","dur":"5.1","text":"So the mechanics of this assumption is really just captured by this equation,","_id":"8bf35862af6549b2b8149539ed2df081"},{"start":"4360.575","dur":"1.23","text":"So this is called conditional.","_id":"58c76225f31d4fd5b87e1a207452f348"},{"start":"4356.78","dur":"3.795","text":"or not each word appears or does not appear is independent, okay?","_id":"e27611d111e0437ba20632e2a7832ceb"},{"start":"4352.94","dur":"3.84","text":"once you know the class label is a spam or not spam whether","_id":"0ccd9e49b65a401b98b2b2c0684ec800"},{"start":"4348.68","dur":"4.26","text":"Um, right, that, uh,","_id":"1628e4093fdf4f798591333477e5a606"},{"start":"4344.24","dur":"4.44","text":"and if you haven't taken CS-228 this picture won't make sense, but don't worry about it.","_id":"01e4d950268d4feea006095cee39e253"},{"start":"4341.539","dur":"2.701","text":"this assumption is summarizing this picture,","_id":"e4df9d7d2676425f8d1d6a4d75f14a5d"},{"start":"4339.05","dur":"2.489","text":"if you've taken CS-228, uh,","_id":"985fbf7187bf4a0082d8a61cb9bd4d76"},{"start":"4336.275","dur":"2.775","text":"if you- if any of you are familiar with probabilistic graphical models,","_id":"4a89ba736b964ecfba81f6f2170ae38a"},{"start":"4333.2","dur":"3.075","text":"Um, and so- so- so it's like,","_id":"e1ee6c30149b4f279eef6319cfc56e24"},{"start":"4328.76","dur":"4.44","text":"but it may be not so horrible that you can't get away with it, right?","_id":"c7093b6415eb4c258d25fafdae306daf"},{"start":"4327.14","dur":"1.62","text":"um, in a mathematical sense,","_id":"4fd246a3a2034fdaa06a2d80d0f2ecde"},{"start":"4324.515","dur":"2.625","text":"So this assumption is not true,","_id":"c970e0d2a1f145a4b151188edd18b913"},{"start":"4321.89","dur":"2.625","text":"but if it was Gaussian you can kind of get away with it.","_id":"05e9baa46f114aefa035edbc45079887"},{"start":"4319.52","dur":"2.37","text":"Just that sometimes your data isn't perfectly Gaussian,","_id":"4317601ad0fc478eb772b3e9d2475e5b"},{"start":"4315.815","dur":"3.705","text":"a true assumption and that is just not mathematically true assumption.","_id":"c2557ead2f13421da36bf582550d0fdd"},{"start":"4313.25","dur":"2.565","text":"Um, and this is one of those assumptions that is definitely not","_id":"48aab36f0911493da5ed3eb559c67734"},{"start":"4309.47","dur":"3.78","text":"depend on whether the word \"A\" appears in your e-mail, right?","_id":"39c4858f272f47028b1f09af514912f4"},{"start":"4306.215","dur":"3.255","text":"um, aardvark in your e-mail does not","_id":"479f71f7bfd54454a9cd997778bba614"},{"start":"4301.895","dur":"4.32","text":"so long as you know why the chance of seeing the words,","_id":"8411384cf3c642618dd97374c1f00775"},{"start":"4299.42","dur":"2.475","text":"But you're assuming that, um,","_id":"6e93cbcfb103401ea86d5a759b2d9dbd"},{"start":"4294.905","dur":"4.515","text":"conditional independence assumption it's also sometimes called the Naive Bayes assumption.","_id":"b47f313696dc482fa6eed10746ae9247"},{"start":"4291.44","dur":"3.465","text":"So this assumption is called a","_id":"0a18c45ef02a46bcb3f9a6abdf020e20"},{"start":"4286.04","dur":"5.4","text":"p of X_10,000 given y, okay?","_id":"eeaad21f79a945a79ec127ee78b21418"},{"start":"4280.97","dur":"5.07","text":"x_2 given y p of x_3 given y and so on,","_id":"20ce34525ac042a1a2b55fcbf008f296"},{"start":"4275.86","dur":"5.11","text":"is that this is equal to this first term no change the","_id":"63928dc0152a409f9de55c956560b2b8"},{"start":"4264.58","dur":"5.47","text":"Um, and what we're going to assume which is what this assumption is,","_id":"50016ff757d740349aa7aefa4c153ec3"},{"start":"4261.85","dur":"2.73","text":"true by the- by the chain rule of probability.","_id":"6421eddf3a444716bf2bd7e6a43c0920"},{"start":"4259.9","dur":"1.95","text":"This is just a true statement of fact as always","_id":"9b388ceb890b490c869eb74f546ab960"},{"start":"4257.77","dur":"2.13","text":"So I haven't made any assumptions yet.","_id":"d77ebf0b0b7c460f985c6487533a9616"},{"start":"4250.33","dur":"7.44","text":"x_2 Y up to your p of x_10,000 given, and so on, right?","_id":"0f0cf79fe823478a9167ec734bb7ea45"},{"start":"4243.95","dur":"6.38","text":"um, x_1 and y times p of x_3 given x_1,","_id":"1864e8ce093843118fc12d3fb55f7443"},{"start":"4232.925","dur":"11.025","text":"this is equal to P of x_1 given y times P of x_2 given,","_id":"61745668af2f43169cdaf3f33c98f94a"},{"start":"4223.565","dur":"9.36","text":"but so P of x_1 up to x_10,000 given y by the chain rule of probability,","_id":"f4b229557ed941fa96f51045746f184b"},{"start":"4221.15","dur":"2.415","text":"Uh, let me just write out what this means,","_id":"b592998d0fb64f7ba9fd8fcb08dee74e"},{"start":"4208.87","dur":"12.28","text":"conditionally independent given y, okay?","_id":"3204f9035a58412e884390c9a1b3b2fa"},{"start":"4202.43","dur":"6.04","text":"we're going to assume that X_i's are","_id":"0afa43a441d240a9842f864f92880a54"},{"start":"4200.48","dur":"1.95","text":"So in the Naive Bayes algorithm,","_id":"dd05d28328934cf98fef5e7950e8d125"},{"start":"4196.67","dur":"3.81","text":"right, because of the excessive number of parameters.","_id":"7e44c8bb44f34c32a2949a20dbf9ad7b"},{"start":"4192.56","dur":"4.11","text":"But so, modeling this without additional assumptions won't- won't work,","_id":"8b619dced7ac4c8f86a80e3f4df1dbc6"},{"start":"4189.44","dur":"3.12","text":"1 parameter because that adds up to 1, and you can see one parameter.","_id":"5919e498580d4067b21e9b088461b035"},{"start":"4187.43","dur":"2.01","text":"you need 2 to 10,000 minus","_id":"1ed0c14378984c0ca7802b80d5e54b0a"},{"start":"4185.015","dur":"2.415","text":"Which is a lot, or technically,","_id":"f3ed9a0c6b814ef99c4c79fa489081e8"},{"start":"4181.475","dur":"3.54","text":"you know 2 to the 10,000 parameters, right?","_id":"8d5231d502114bbb97bfdab66412a7ff"},{"start":"4178.715","dur":"2.76","text":"Then you need, right, uh, uh, you need,","_id":"b363911b21be43d6bb704f415a6afa75"},{"start":"4175.325","dur":"3.39","text":"you know, 2 to the 10,000 possible outcomes.","_id":"9502d66a0bad4c1e83571a087a176e91"},{"start":"4168.575","dur":"6.75","text":"So we try to model P of x in the straightforward way as a multinomial distribution over,","_id":"974b958e02e345e3a76c768118eef257"},{"start":"4165.005","dur":"3.57","text":"Because x is a binary vector of this 10,000 dimensional.","_id":"fe83541c3b2b4e09998215d0406f172f"},{"start":"4155.12","dur":"9.885","text":"2 to the 10,000 possible values of x, right?","_id":"66a54c8b728a4b45aca3fc82af1ccdd0"},{"start":"4151.985","dur":"3.135","text":"But there are, uh,","_id":"7cf5243be38f47d0a03d7c9515bd5780"},{"start":"4149","dur":"2.985","text":"As well as P of y, okay?","_id":"ecfff36d89eb4c16b7a567199df5b27a"},{"start":"4141.11","dur":"7.89","text":"model P of x given y, right?","_id":"ba09b4bcd6e64158a416d645c232b1c0"},{"start":"4138.245","dur":"2.865","text":"Um, and so we want to","_id":"485c184e79824c3baf18f8783a83c916"},{"start":"4134.69","dur":"3.555","text":"we're going to build a generative learning algorithm.","_id":"6a45c1bf16bf470faab91a3a4cbc2ebd"},{"start":"4129.71","dur":"4.98","text":"Now, um, in the Naive Bayes algorithm,","_id":"2416d2d446fa4ca0bd0905fb66ad71bb"},{"start":"4125.195","dur":"4.515","text":"i from this list appears in your e-mail.","_id":"7c70cd5ef05444baabd5059d5fe3ce01"},{"start":"4122.615","dur":"2.58","text":"So it's either 0 or 1 depending on whether or not that word","_id":"925b8b46f7034426add97936fdb2439d"},{"start":"4114.275","dur":"8.34","text":"indicator word i appears in the e-mail, right?","_id":"80451810c0794f4eb03300ad621cd910"},{"start":"4109.76","dur":"4.515","text":"So in other words, X_i is","_id":"5915cc9feffb4307a70ea16569f25da3"},{"start":"4096.92","dur":"12.84","text":"So, um.","_id":"6431886761ba4ecea75e644310b65f8e"},{"start":"4091.25","dur":"5.67","text":"that appear in your e-mail training set as the dictionary that you will use.","_id":"b674ee94e6d64c9886c3b793eb5d1635"},{"start":"4089.315","dur":"1.935","text":"you know, take the top 10,000 words, uh,","_id":"f8734dfaa8854d118ef3744202e32be3"},{"start":"4087.305","dur":"2.01","text":"n is 10,000 because you're using,","_id":"6fddd730e24a4d9ab237049eb221262d"},{"start":"4084.605","dur":"2.7","text":"where- where for the purpose of illustration, let's say,","_id":"b45ea5a30c27401f8c3b408a94c25d8b"},{"start":"4081.26","dur":"3.345","text":"because there's a n-dimensional binary feature vector,","_id":"ac86e602327c4ad2acdc8a9983a4e17a"},{"start":"4073.67","dur":"7.59","text":"Um, and so here the feature vector is 0, 1 to the n,","_id":"f2c2d2a58d8241978f3717231efd803a"},{"start":"4066.605","dur":"7.065","text":"Right? So you take a- take an e-mail and turn it into a binary feature vector.","_id":"2f9014d179bf44959750da359f438942"},{"start":"4061.61","dur":"4.995","text":"so 0 there, buy and so on.","_id":"e7b02daf8d27427f88c0c1c941950e49"},{"start":"4059.48","dur":"2.13","text":"Did not try to sell aardvark or aardwolf,","_id":"cde581ca1ed8472bb6351b3d40bba038"},{"start":"4055.835","dur":"3.645","text":"buy some stuff and then the word A appears in e-mail, you put a 1 there.","_id":"e8725a318a4f4876b84852301e4533a0"},{"start":"4054.155","dur":"1.68","text":"that asks you to, you know,","_id":"fd4c006878344425bed880027326d403"},{"start":"4051.755","dur":"2.4","text":"Right? So if you've gotten an e-mail, um, uh,","_id":"7d884141b21f4d56860a118e287c1f43"},{"start":"4047.675","dur":"4.08","text":"if a word appears in the e-mail and puts a 0 if it doesn't.","_id":"6cced80def8845ffa1682b865e586465"},{"start":"4045.485","dur":"2.19","text":"that puts a 1,","_id":"239840c6265a4623bed03e3209c324e4"},{"start":"4042.8","dur":"2.685","text":"um, you can create a binary feature vector,","_id":"e5d4a09441d640f4a0c30e3aedddfb29"},{"start":"4040.46","dur":"2.34","text":"And so one way to do this is,","_id":"7b1428434cf34dceb5c58a542ed5eab7"},{"start":"4037.445","dur":"3.015","text":"take this piece of text and represent it as a feature vector.","_id":"d022a6015268404faad4b9fb18fafb68"},{"start":"4034.64","dur":"2.805","text":"what we would like to do is then, um,","_id":"3bfb55ec2f5e43e5995efeecb6abc167"},{"start":"4028.245","dur":"6.395","text":"All right. Um, and so given an e-mail,","_id":"635cd7edbc334977a9327c68f762be96"},{"start":"4025.925","dur":"2.32","text":"we'll- we- we'll get CS229 there someday.","_id":"dc19eb5648cf41afadc1976dd0a46ce2"},{"start":"4024.21","dur":"1.715","text":"just yet just- just- just- you wait,","_id":"12b7bcee7db549e8a713938dc31b1f98"},{"start":"4022.24","dur":"1.97","text":"was it like the Oxford dictionary,","_id":"c37a0d609fd244818a8f30baa8e5d4f3"},{"start":"4019.135","dur":"3.105","text":"even if it doesn't appear in the- in the official, uh,","_id":"1a339cdd48374a03a9a845d216b8e3c9"},{"start":"4016.72","dur":"2.415","text":"your dictionary of building your e-mail spam filter for yourself,","_id":"73a14ad027fa4ca6b530646ffc724c50"},{"start":"4015.34","dur":"1.38","text":"So CS229 might appear in","_id":"a3eee45ee112463195292c86b0e90407"},{"start":"4012.625","dur":"2.715","text":"e-mail about- from us or maybe others about CS229.","_id":"75fcbbe0142944d592e61601bc8f68ce"},{"start":"4010.59","dur":"2.035","text":"Right? And your e-mails, I guess you're getting a bunch of","_id":"bf1002d8f4f344978408de1b2b3fc0e2"},{"start":"4009.11","dur":"1.48","text":"and so I don't know.","_id":"fdfe46b7a07d42febbefed90975b4bbf"},{"start":"4004.55","dur":"4.56","text":"co-pairs and just find the top 10,000 occurring words and use that as a feature set,","_id":"5558a4c4f99f4049acf72788cb124d4e"},{"start":"4001.28","dur":"3.27","text":"but where- the other way to do this is to look through your own e-mail","_id":"599f29e04b564ad68b262995a30c6244"},{"start":"3998.01","dur":"3.27","text":"in practice, well, you- the other thing that's- dictionary has too many words,","_id":"4f7debfbc04f4906a4c3e4ed82403fb2"},{"start":"3994.92","dur":"3.09","text":"it's easier to think about it as if it was a dictionary but, you know,","_id":"a947a937a1894208bf7cae0084f08d6a"},{"start":"3993.03","dur":"1.89","text":"Right? So maybe you have 10,000,","_id":"91850f3f953a40b6b26b39981fe999bf"},{"start":"3991.89","dur":"1.14","text":"you know, in your training set.","_id":"0d24d07481e64db5b36ecdd697578c11"},{"start":"3988.95","dur":"2.94","text":"actually look at the dictionary but look at the top 10,000 words,","_id":"bd275d23b43642a0b43f4159e053b79e"},{"start":"3987.36","dur":"1.59","text":"what you do is not, uh, uh,","_id":"18285e04df4a4654986b6ed1e6dbeaea"},{"start":"3983.37","dur":"3.99","text":"Um, So- so- I think it is a useful way to think about it, in- in- in- practice,","_id":"3a0984ca444d4de38df24f888e34de9d"},{"start":"3976.664","dur":"6.706","text":"which is the technological chemistry that refers to the fermentation process in brewing.","_id":"fe13594d4a0c40758686019cf205b21c"},{"start":"3973.32","dur":"3.344","text":"and then the last word in my dictionary is zymurgy,","_id":"16e9bc878dc3476c8e3bbfa7f21b4556"},{"start":"3971.565","dur":"1.755","text":"And then, um, uh,","_id":"f53481bda6314a1595a8ae81c7adc4fe"},{"start":"3966.96","dur":"4.605","text":"e- e-mail spam lot of people asking to buy stuff so that they would buy, right?","_id":"5265c91321bf4bc39e1735e2552c7962"},{"start":"3964.89","dur":"2.07","text":"Um, and then, you know, uh, uh,","_id":"68ab59fa2f1645cbac30acdc99ae3498"},{"start":"3962.67","dur":"2.22","text":"[NOISE] [LAUGHTER]","_id":"79e8c894a12947ee880a600a44ad5ede"},{"start":"3961.74","dur":"0.93","text":"No, it's easy, look it up.","_id":"efc4201bb0e34f1a96665367e4569f50"},{"start":"3959.22","dur":"2.52","text":"[BACKGROUND]","_id":"06098d88730a4761bc0ce4c34932ddaa"},{"start":"3957.045","dur":"2.175","text":"Third word is aardwolf.","_id":"d405e9463ed044fe8c809ae3d05bb6e7"},{"start":"3954.36","dur":"2.685","text":"second word in the English edition is aardvark.","_id":"2c5aac98253c4e7b95ea69afa915270a"},{"start":"3952.2","dur":"2.16","text":"So first of all there's the English dictionary as A,","_id":"3b9aba98267248df819e0ad5c89cd955"},{"start":"3947.01","dur":"5.19","text":"the English dictionary and make a list of all the words in the English dictionary, right?","_id":"ab5a6d4bae964c3aadbeb6c0a0ff64b5"},{"start":"3942","dur":"5.01","text":"which is first, um, let's start with a- let's start with","_id":"d045cf23ab744bdda0bac22ffdd8529e"},{"start":"3940.365","dur":"1.635","text":"And we'll do so as follows,","_id":"dd06299e53504bd38f0d60b8b009ba07"},{"start":"3934.545","dur":"5.82","text":"take a piece of e-mail and first map it to a feature vector X.","_id":"7d61a31276494a64a98ab12158cf4572"},{"start":"3927.41","dur":"7.135","text":"And so, um, in Naive Bayes what we're going to do is take your e-mail,","_id":"62d8a79bdbd048d7b5d76e8958fc1053"},{"start":"3918.99","dur":"4.57","text":"how do you represent it as a feature vector?","_id":"54ee00300e13464bb0b4de9cb0440ab9"},{"start":"3917.1","dur":"1.89","text":"given the e-mail classification problem,","_id":"3784b9b04fde4fb0b6fdb6391a7ca720"},{"start":"3914.67","dur":"2.43","text":"given the e-mail problem, uh,","_id":"f60a66b7302648eb8f7439cdf03cbdf4"},{"start":"3908.07","dur":"6.6","text":"Um, and so the first question we will have is, um, uh,","_id":"1adb9122fc07462087dc316a56f79684"},{"start":"3904.05","dur":"4.02","text":"and they're trying to take a product description and classify it into one of the classes.","_id":"5a39bbd8fcd94f55ad9c17b97bd0c5d2"},{"start":"3899.955","dur":"4.095","text":"two categories for spam or not spam or one of maybe thousands of categories,","_id":"4e491afaad544d37917c2055b72104e2"},{"start":"3896.34","dur":"3.615","text":"we have a piece of text and you want to classify into one of","_id":"16bc3c74edbb43b8af88cbc16d156ce7"},{"start":"3893.22","dur":"3.12","text":"Uh, but these- these examples of text classification problems,","_id":"d8320e2b43af4cb89c4822068a492a8a"},{"start":"3892.17","dur":"1.05","text":"Are they trying to sell clothing?","_id":"960b669c5ecc45b492f65e74deeea021"},{"start":"3889.41","dur":"2.76","text":"categorize it, is it an electronic thing or are they trying to sell a TV?","_id":"1acce1b7a2c1462b8434ca9f6be12433"},{"start":"3886.725","dur":"2.685","text":"How do you take that text that someone wrote over the description and","_id":"1a5ae5ac4b954e5589fdae985b0b92ae"},{"start":"3884.61","dur":"2.115","text":"Roomba, I'm trying to sell it on Ebay.\"","_id":"33aee817384140a8a697b9b6d8d53b43"},{"start":"3882.57","dur":"2.04","text":"\"Hey, I have a secondhand, you know,","_id":"3b48895b9ade4bc8bfea546f53491411"},{"start":"3879.06","dur":"3.51","text":"the- if someone's trying to sell something and you write a text description, right?","_id":"91f7aa09f37b44bebc83c0b31998e987"},{"start":"3877.74","dur":"1.32","text":"used to have a problem of, you know,","_id":"da7549b8132045a7b40d32654e7553ed"},{"start":"3873.765","dur":"3.975","text":"Or, uh, other examples, uh, uh, actually several years ago, Ebay,","_id":"cbdcbd9e69f44099b9966bcd362d26ab"},{"start":"3871.185","dur":"2.58","text":"can you classify this as spam or not spam?","_id":"7220b1143152472680e48065bb94690f"},{"start":"3869.04","dur":"2.145","text":"But given in a piece of text, like given a piece of email,","_id":"7b4efa40690a4359aa10f467e78dae4c"},{"start":"3866.01","dur":"3.03","text":"I guess this is our first foray into natural language processing, right?","_id":"e1aa2064a53e46ae9b11a1e1f9e75e2a"},{"start":"3863.01","dur":"3","text":"e-mail spam classification, but this- this is- this-","_id":"e6fa4b86ca714a70ba671136b425f1a6"},{"start":"3858.765","dur":"4.245","text":"um, and I'm gonna use as most of the example;","_id":"011ca7f2cab3423e8a54960aa2dc05aa"},{"start":"3852.6","dur":"6.165","text":"is talk about one more generative learning algorithm called Naive Bayes,","_id":"63cea9a962c6488da2f3c1574a77f37e"},{"start":"3848.775","dur":"3.825","text":"The last thing I want to do today, um,","_id":"2cedef4845524c8d8a812847eca144f4"},{"start":"3843.75","dur":"5.025","text":"continuous valued, uh, features x.","_id":"483289b192d940fcb2f029a4db34465b"},{"start":"3839.39","dur":"4.36","text":"So you've seen GDA in the context of, um,","_id":"f8a5b93794c74f5bbf02c20974497362"},{"start":"3828.96","dur":"4.6","text":"So, uh, I want a fresh board for this.","_id":"f67b29722e2d48ac94fdb583e737cf89"},{"start":"3823.71","dur":"5.25","text":"as well. All right.","_id":"01bc33e375134b0697b707909548c87a"},{"start":"3821.58","dur":"2.13","text":"talk about much more complicated learning algorithms","_id":"27f62db4cd034acf9f320c722e053be6"},{"start":"3819.405","dur":"2.175","text":"Uh, this is a theme we'll come back to you when we","_id":"f226cf52ae914e0db2174e3fac99905d"},{"start":"3814.14","dur":"5.265","text":"that when you have less data your skill at coding and your knowledge matters much more.","_id":"76c2a860e17d473a9e0b79c9e10b6637"},{"start":"3811.485","dur":"2.655","text":"This is one of the important principles of machine learning,","_id":"22860a7459c648599715fc680b5d5d25"},{"start":"3808.74","dur":"2.745","text":"This is a recurring theme that we'll come back to it as well.","_id":"2d9edaf84ece44239d71f72c3f7127d6"},{"start":"3803.37","dur":"5.37","text":"that when you have less data the algorithm needs to rely more on assumptions you code in.","_id":"a78ed8f60f0d465d861c1b843b1e788f"},{"start":"3797.685","dur":"5.685","text":"Cool. Um, and this- this theme","_id":"205a006b611f490b82246a2396278b68"},{"start":"3794.4","dur":"3.285","text":"Yeah. Oh yes, right. You saw Softmax the other day.","_id":"1589083f8125435ab247129088d6361e"},{"start":"3790.59","dur":"3.81","text":"uh, like a GDA with three classes and Softmax.","_id":"e5e4820b69a247749b5bbee8a1fc8a79"},{"start":"3787.08","dur":"3.51","text":"But, uh, but yes, similar- similar things holds true for,","_id":"659971a172504183bd148b12f1bdce73"},{"start":"3785.58","dur":"1.5","text":"whether you have more than two classes.","_id":"48246a8f1d5b4b109a08d340739df359"},{"start":"3782.34","dur":"3.24","text":"GDA for multiple- and we have so far we're going to talk about Binary Classification,","_id":"9dfd83eb84d44069a740de57b0976c5e"},{"start":"3779.4","dur":"2.94","text":"I think it's a similar thing holds true for, um,","_id":"9bf7e4820d2a4958b857cca3383ae6f0"},{"start":"3776.64","dur":"2.76","text":"the Softmax Regression which I didn't talk about. But yes.","_id":"aae31d1c59b94bf180aa80e4b15cbd74"},{"start":"3775.14","dur":"1.5","text":"uh, and the generalization of this would be","_id":"5c100ed0f6b84eb2852f58bdcd52bc03"},{"start":"3769.8","dur":"5.34","text":"Uh, ye- I think so yes,","_id":"8407404591e64a02a9ef8b61fed86910"},{"start":"3766.815","dur":"2.985","text":"improvement happen even as you increase the number of classes?","_id":"6b88064a5e344573acf47b06f9b56bc8"},{"start":"3765.06","dur":"1.755","text":"Oh, uh, does performance","_id":"8064d3fa6fa44107a250efc12a6e474a"},{"start":"3762.29","dur":"2.77","text":"Uh, if performance [inaudible]","_id":"bac9205840ee4067935e75d75232fafb"},{"start":"3754.32","dur":"3.7","text":"uh, actually let's take one last question then we move on, go ahead.","_id":"8776ae4ccb8e48d5a467f513c9c74536"},{"start":"3752.265","dur":"2.055","text":"But, yeah. All right,","_id":"01d4758e039a43558957b42a6e107e0d"},{"start":"3747.18","dur":"5.085","text":"Yeah. Um, I think this was once a midterm homework problem to prove this actually?","_id":"0a60654cf88d411098069a0843a95e67"},{"start":"3745.485","dur":"1.695","text":"then this will be logistic.","_id":"389a5c28dfef4529813bc38f96825a74"},{"start":"3741.39","dur":"4.095","text":"vary only by the natural parameter of the exponential family distribution,","_id":"16a09157db484133892152fdb0d8e61d"},{"start":"3739.02","dur":"2.37","text":"it's the same exponential family distribution and if they","_id":"45dc752453644ec09f13930cc32e4d18"},{"start":"3736.095","dur":"2.925","text":"x given y equals 0 comes to an exponential family distribution,","_id":"69b7708120bb4e9cbabedabcac1ccf55"},{"start":"3733.785","dur":"2.31","text":"uh, it comes from an exponential family distribution,","_id":"5e25bca2060d4b10bfaf23a0bbae8a9c"},{"start":"3732.09","dur":"1.695","text":"x given y equals 1,","_id":"a98dfaa7931342a794db04b555934687"},{"start":"3730.53","dur":"1.56","text":"Yes, so if, uh,","_id":"a079f49855284b89a15d7ddedee431e4"},{"start":"3728.67","dur":"1.86","text":"yes, so what's the general statement of this?","_id":"7bb33ae0ae1b455e91f29e1676ac6be2"},{"start":"3727.35","dur":"1.32","text":"Oh, sure. So does this, uh,","_id":"3db6b9a5628248878d5599d81cd62ed3"},{"start":"3723.68","dur":"3.67","text":"Um, what's the implication when [inaudible].","_id":"0fca7777b3894bafbfea7fc14dff92c6"},{"start":"3715.005","dur":"2.995","text":"coul- could- I should still take questions from all of you. Yeah, go ahead.","_id":"d5a491ede36b482480a360063b59cc7c"},{"start":"3712.71","dur":"2.295","text":"All right. This is- uh- uh-","_id":"9acd0c189e724bc098bb0b177056503a"},{"start":"3708.315","dur":"4.395","text":"much bigger performance than a lower-skill team would be able to.","_id":"f9877a0834dd459c8fd1c51a54f8f1f5"},{"start":"3705.6","dur":"2.715","text":"That- that skill allows you to drive","_id":"8503e3f8c9c14f5a86c65691dadf3623"},{"start":"3704.34","dur":"1.26","text":"is it Gaussian, is it Poisson?","_id":"3a17c2d999534fcf9d5e3294446fa629"},{"start":"3701.97","dur":"2.37","text":"is the assumptions you code into the algorithm like,","_id":"0d54f099667746e1917f16bfeab0333a"},{"start":"3699.06","dur":"2.91","text":"but it just- this is a very clear where you don't have much data,","_id":"c1907ad165cf408f8cd57307845ce259"},{"start":"3696.48","dur":"2.58","text":"a- it makes a difference from big data and small data,","_id":"504ba4aa0c3a40b183463a53faa1b511"},{"start":"3694.065","dur":"2.415","text":"really makes a bigger difference when you have a- make- makes","_id":"aa7c46050dd94cbf9ed76d1cb706c053"},{"start":"3692.025","dur":"2.04","text":"your skill at designing a machine learning system,","_id":"5ce3c17d9e884a849ea3caaef9390795"},{"start":"3689.79","dur":"2.235","text":"Uh, but- but I think there are a lot of applications where","_id":"1bcbe6cc9aa847409f79240d9da065f1"},{"start":"3688.38","dur":"1.41","text":"Uh, well, I don't know.","_id":"b6e351b82da74ae8bb56b3d78d715b5a"},{"start":"3686.58","dur":"1.8","text":"don't do it because I can make it work.\"","_id":"7143b984bf1c4ceaa9417745668b2818"},{"start":"3685.275","dur":"1.305","text":"then I'll say, \"Great, you know,","_id":"78cffffea1a4417da90c8d46e274d0f7"},{"start":"3683.52","dur":"1.755","text":"if- if there's a competitor saying that,","_id":"d3fe2282f95d45c4989b501cf15072bc"},{"start":"3682.425","dur":"1.095","text":"Uh, then I don't know,","_id":"3cabfb93b5b8460d848b3ff013dbbf7e"},{"start":"3681.21","dur":"1.215","text":"you'll never do anything.\"","_id":"46002d1d4f7b421283df9a8932e875a6"},{"start":"3679.575","dur":"1.635","text":"\"Oh you only have a hundred examples,","_id":"447c4caf0f764cce9ddae95b8cd6317e"},{"start":"3676.8","dur":"2.775","text":"Oh, and if someone goes to you and says,","_id":"ad17c287c29141f3a9eb789f3bd25dc2"},{"start":"3674.01","dur":"2.79","text":"the performance differences when you have small data.","_id":"b208b81923a54a7d85af2229f6300980"},{"start":"3671.67","dur":"2.34","text":"and the less experienced teams and drives a lot of","_id":"61bcaea5776a4e349e0d3e059ca7f7c1"},{"start":"3669.315","dur":"2.355","text":"distinguishes the high-skilled teams and, uh, and, uh,","_id":"7aa4db44e0074a27aecb6e30600e72fa"},{"start":"3667.41","dur":"1.905","text":"generative or discriminative, that actually","_id":"86c23b6a78744f5d9b9e43d82c32fc5c"},{"start":"3665.985","dur":"1.425","text":"you know, what assumptions you use,","_id":"5c1fe5000644461b9aa1e6f78271c8dc"},{"start":"3663.045","dur":"2.94","text":"so and I think that it's these types of intuitions,","_id":"87e78cd6b38d4474a7fb171c71b691bb"},{"start":"3659.28","dur":"3.765","text":"whereas the performance gap is smaller when you have giant data sets I think,","_id":"854a827fd5b8400fa074c2d631580a9d"},{"start":"3657.18","dur":"2.1","text":"much, much, much better than the low skilled teams,","_id":"e58c0d956c1841589240bcbeb310c999"},{"start":"3654","dur":"3.18","text":"then the high-skilled teams will actually do much,","_id":"83bd6d446cde40e18af0dc6e0d54ab98"},{"start":"3651.645","dur":"2.355","text":"But if you have only a hundred examples,","_id":"6fddb32c942c4769b1ab6c6debe33f78"},{"start":"3649.365","dur":"2.28","text":"for- for- for image classification, for ImageNet.","_id":"77ecb105c5b5403bb20f58f94dbc632f"},{"start":"3647.82","dur":"1.545","text":"if a million examples, uh,","_id":"af4c740f660e40bbb24d6e6f181210f1"},{"start":"3645.705","dur":"2.115","text":"there are now dozens of teams that get great performance,","_id":"4acd7987961c461aa21612ec055196d3"},{"start":"3642.6","dur":"3.105","text":"and so the performance difference between teams, you know,","_id":"b88e49e83e5e4dc58529c4df3323b823"},{"start":"3640.74","dur":"1.86","text":"They give a million examples, right?","_id":"6d64d530c9a44e24a01c601c8fb032f9"},{"start":"3639.405","dur":"1.335","text":"They can get great results.","_id":"2d3a772ad6954377ab398d1489301bc2"},{"start":"3638.07","dur":"1.335","text":"maybe hundreds of teams, I don't know.","_id":"20b15797788a4615b3adaf1e2bf8a0bd"},{"start":"3636.63","dur":"1.44","text":"there are now dozens of teams,","_id":"b4cfef4c6913473e91bb51a1cd129431"},{"start":"3635.415","dur":"1.215","text":"mi- million in- in- images,","_id":"5f8d65d0898849be8e8beedd39575037"},{"start":"3633","dur":"2.415","text":"Um, so if you take something like ImageNet,","_id":"4c7b669fae5e4daea1d6a5f5d4826eff"},{"start":"3629.625","dur":"3.375","text":"it's then the skill in designing your learning algorithm matters much more.","_id":"bc5773edec02403dbe04fa160ee6af2d"},{"start":"3626.64","dur":"2.985","text":"you have a hundred examples and, um,","_id":"fa51b9c87d4145fd99229ca45a1bdc15"},{"start":"3624.255","dur":"2.385","text":"where you just don't have a million examples, uh,","_id":"96d755acdb8649819c03edc5a7453e97"},{"start":"3622.44","dur":"1.815","text":"So there are lots of machine learning applications","_id":"0ad3662061f744b8b3f372af7f3930b2"},{"start":"3620.67","dur":"1.77","text":"even you don't have a hundred million examples.","_id":"16a96f1ffbe94221ba9166bd1ca9c42f"},{"start":"3618.435","dur":"2.235","text":"even when you don't have a million in examples,","_id":"70ba3f9e0b6b4eeba4200aae54c078bd"},{"start":"3616.26","dur":"2.175","text":"these days is getting your algorithms to work","_id":"bf85b6395c5a4ad39661e29f1fe7749d"},{"start":"3612.33","dur":"3.93","text":"But, um, uh, I think a lot of the skill in machine learning","_id":"fd3684f2ec2a4673832084caf858c09c"},{"start":"3610.515","dur":"1.815","text":"you know, more data almost always helps.","_id":"4945bbf9db7c4fcdaba54d518744f136"},{"start":"3607.68","dur":"2.835","text":"And I think we did a good job telling people that high-level message,","_id":"f34b6b28a3d5440185c0fb23041e261c"},{"start":"3605.355","dur":"2.325","text":"usually the more data the better, so all that is true.","_id":"7b4729a1eb76441688f4c83e1ebdb731"},{"start":"3603.36","dur":"1.995","text":"having more data pretty much never hurts and","_id":"c32136e6f7ff4634b54418231df6ceaa"},{"start":"3601.02","dur":"2.34","text":"it's great and I love data and a- um,","_id":"4a5ce3ab2a4146a88e8973c8fa70ad71"},{"start":"3598.77","dur":"2.25","text":"And- and yes it's true that when we have more data,","_id":"67bee2cf296f4326992157c1b843161d"},{"start":"3595.5","dur":"3.27","text":"frank- you know, a little bit overhyped big data, right?","_id":"286015fae53c4d1dbe75da11a08cbc81"},{"start":"3592.875","dur":"2.625","text":"uh, the machine learning world has,","_id":"731213962bc14144b223ecd794769d7b"},{"start":"3590.985","dur":"1.89","text":"um, I think that,","_id":"09cd302501cf42a285c186a22cb31b55"},{"start":"3587.88","dur":"3.105","text":"Right? I think- by the way a- another philosophical point,","_id":"f95b3469495a49b6b6970df456821531"},{"start":"3585.045","dur":"2.835","text":"relatively Gaussian,\" and use domain knowledge like that.","_id":"59fb80e1b69e4bed838f59a142b6d3f5"},{"start":"3583.32","dur":"1.725","text":"do you think the distribution is rath- rath-","_id":"0593a67efb5d462b900e537c17fd21f9"},{"start":"3580.155","dur":"3.165","text":"then you just have to ask some doctors and ask, \"Well,","_id":"5d2c7aaeba6f467091af22e685e676a9"},{"start":"3576.735","dur":"3.42","text":"you know, I don't know, 50 examples of healthcare records,","_id":"9de7981ec9884fbb8372fb1075c3e6e1"},{"start":"3574.65","dur":"2.085","text":"Like if you have, you- if you have, uh, uh, uh,","_id":"426e48bb92354940a356aeb1f112a097"},{"start":"3572.16","dur":"2.49","text":"set and it's just a matter of judgment, right?","_id":"75b901d9bb3b4e8f9b6ca274c91e6a68"},{"start":"3569.055","dur":"3.105","text":"uh, uh, yeah sometimes you just have a very small training","_id":"97a801767c4546a884810e7c9cdf35e2"},{"start":"3565.215","dur":"3.84","text":"But what happens often is that um,","_id":"1c6d88ea73264f97bdcca778816cfaeb"},{"start":"3562.8","dur":"2.415","text":"then, you know, there will be reasons not to use GDA.","_id":"e5524e0e0dd949afa0934bbb9d48346f"},{"start":"3560.55","dur":"2.25","text":"and if it looks clearly non-Gaussian,","_id":"9c2d042d6d0d4f7aab8fad32ad942ba5"},{"start":"3558.705","dur":"1.845","text":"is people just plot the data,","_id":"d80200dc43f144139267489f8be99c13"},{"start":"3556.905","dur":"1.8","text":"I- I think what often happens more,","_id":"8935dbc7c278418786f21eef7bf6f264"},{"start":"3555.36","dur":"1.545","text":"If they're very high dimensional data,","_id":"8fb4e668f54a4c7f92c2c85cc3798cc5"},{"start":"3552.93","dur":"2.43","text":"[LAUGHTER] Well no, that's not really fair. I don't know.","_id":"6f556f4bc0cf4d2b90a52bc5ae6c27ea"},{"start":"3550.53","dur":"2.4","text":"um, uh, the- the- I- I don't know.","_id":"6df3481acb3e42e0a9e3fa7080c4cb7d"},{"start":"3547.08","dur":"3.45","text":"you probably have enough data to just use logistic regression,","_id":"a92f88f7f67640c19450e74f74dbb130"},{"start":"3543.195","dur":"3.885","text":"if you have enough data to do a cyclical test and gain conviction,","_id":"d5122756d7a34ad5973ab93e9ec98bb0"},{"start":"3542.115","dur":"1.08","text":"I think in practice,","_id":"4ac50a79fbb6442db80fa4c2e09556d7"},{"start":"3538.38","dur":"3.735","text":"um, I can tell you what's done in practice.","_id":"d13a1605eaf34ec19d20d52e23eb5f93"},{"start":"3535.305","dur":"3.075","text":"Yeah. It's recommended that you do some cyclical tests to see if it's Gaussian,","_id":"77bbb6539b524624b7605b57b1eef033"},{"start":"3530.21","dur":"5.095","text":"sure that the plot distribution results have a equal variance before we do GDA?","_id":"b9bddcbd98bf4e539a09d9726c49691e"},{"start":"3525.15","dur":"5.06","text":"Is it recommended that we use some kind of statistical global test to make","_id":"d6763ae701914023b27d053ef1158660"},{"start":"3522.075","dur":"3.075","text":"play around their parameters and plot this for yourself, uh, questions?","_id":"10658cbfe0a14fc5b44fc402ef0c12c0"},{"start":"3519.63","dur":"2.445","text":"fire up Python NumPy and- and","_id":"a421656a091745d3bc5d69d7b928beaf"},{"start":"3516.75","dur":"2.88","text":"I- if you're curious, I encourage you to, you know, uh, uh,","_id":"f7cff9fc6ccc485da1b1b5fc33ef3970"},{"start":"3514.32","dur":"2.43","text":"Uh, you- you could- you could- you could fig- actually,","_id":"203d40f765d04b3e8ed1aeced8977ea6"},{"start":"3511.62","dur":"2.7","text":"some- by some other shape from a linear decision boundary.","_id":"a8ccc8a9042249bfa372c0ce19c93dc4"},{"start":"3509.79","dur":"1.83","text":"With positive and negative examples separated by","_id":"03f3d373f559484e9833c50c063c9807"},{"start":"3507.54","dur":"2.25","text":"you know, that- that- that looks like this, right?","_id":"1d978d6b206948af9a3511baf25f5291"},{"start":"3505.245","dur":"2.295","text":"You can end up with a decision boundary,","_id":"ab363c0b77774771a7924dc038c08c63"},{"start":"3503.49","dur":"1.755","text":"So it's not a linear decision boundary anymore.","_id":"ae982d028a84494185ab394bffa58896"},{"start":"3501.48","dur":"2.01","text":"a bunch of quadratic terms in the logistic function.","_id":"f9737a3e37424449a9a0aa70105413cb"},{"start":"3499.53","dur":"1.95","text":"uh, trying to remember, it still ends up being a logistic function but with","_id":"dd22cf32a105459cb07173586b22dd30"},{"start":"3497.055","dur":"2.475","text":"It turns out that, uh,","_id":"9e4cd9a10163415d88a371b319e8ac36"},{"start":"3493.39","dur":"3.665","text":"Oh, right, ah, so what happens when the co-variance matrices are different?","_id":"765d293006594edabcd365bba15bca43"},{"start":"3490.01","dur":"3.38","text":"that we just use the same program for performance-?","_id":"1b4aac26234c4ad2bea5b912b05aa245"},{"start":"3487.31","dur":"2.7","text":"my program synthesis are different with the assumption","_id":"cf949c270c2d4e259db4f6f5cb13c05d"},{"start":"3482.81","dur":"4.5","text":"Uh, if the data is generated from a Gaussian but","_id":"b5a6aee1af60469c9473c163ad0b3bd0"},{"start":"3479.72","dur":"3.09","text":"when we develop more sophisticated learning algorithms. Yeah.","_id":"a9f6d6231d994a5994fb2de3cf86fd4d"},{"start":"3476.45","dur":"3.27","text":"But this general principle is one that we'll come back to again later","_id":"297db7e30f1f463c8a8748f9571bc0e9"},{"start":"3471.695","dur":"4.755","text":"is more motivated by computation and less by performance.","_id":"e36561e7df254523a4f36190d3e1d30a"},{"start":"3468.5","dur":"3.195","text":"So these days when I use these models, um,","_id":"38bb2adb9d6146a9ae9de2357b69575e"},{"start":"3466.1","dur":"2.4","text":"done and there's no iterative process needed.","_id":"f472100d3ecf454d935d8c99d7947d53"},{"start":"3464.03","dur":"2.07","text":"We just compute the mean covar- covariance and we are","_id":"b8dfa06d4dea4b148e391aad0f8b2128"},{"start":"3461.69","dur":"2.34","text":"an accuracy point of view but there's actually a very efficient algorithm.","_id":"cc5c9f5be42f420aa44fbfffbdaef10e"},{"start":"3459.725","dur":"1.965","text":"less that I think I perform better from","_id":"54668b9a92714917ba32f69ade4a811c"},{"start":"3455.81","dur":"3.915","text":"But the very concrete- the other reason I tend to use GDA these days is","_id":"149f7f9e53dd470bba6421fd5c1fdf04"},{"start":"3452.03","dur":"3.78","text":"This is a general principle in machine learning that we'll see again in other places.","_id":"39b1d892cf9f4a79a8edeee9124b1bfb"},{"start":"3449.18","dur":"2.85","text":"Right. Th- this idea about do you make strong or weak assumptions?","_id":"7459f096c8bb4ffcb112e0ffd4abd507"},{"start":"3447.665","dur":"1.515","text":"We'll see again later in this course.","_id":"fb3615ca3fe94fc6a5865b71fa29ca62"},{"start":"3445.76","dur":"1.905","text":"uh, which is a general philosophical point.","_id":"ed2cf3e80e244f35a424ed1c3f848b02"},{"start":"3442.22","dur":"3.54","text":"there's actually apart from the assumptions type of benefit,","_id":"e78c6ba909834be39a63df032ed2bd4d"},{"start":"3439.43","dur":"2.79","text":"covariance matrices is very efficient and so","_id":"6cbc77c446384c5980221486ecbe533c"},{"start":"3435.755","dur":"3.675","text":"And it turns out computing mean and variances of, um,","_id":"331e2bb0c35a4c83a7aa1f7afc7f890a"},{"start":"3432.83","dur":"2.925","text":"don't have the patience to run the GC progression over and over.","_id":"f0f9fc01478d4951b86ca3494822681a"},{"start":"3430.34","dur":"2.49","text":"on where we just need to fit a ton of models and","_id":"3297b11f16274762a0af096a6746a2ee"},{"start":"3427.88","dur":"2.46","text":"the- there's actually one use case at Landing. AI that we're working","_id":"1d35a71b8dcb4c459f33a42a1d13a59a"},{"start":"3425.69","dur":"2.19","text":"it's actually quite computationally efficient and so","_id":"e0a04839087847b7891f7dad16879c3a"},{"start":"3423.455","dur":"2.235","text":"um, uh, is that,","_id":"00557fb4b041466ebc435d4567c5bdc9"},{"start":"3421.16","dur":"2.295","text":"general discriminant analysis, so algorithms like this,","_id":"4a90e35ac683422eb9f4c7a4054d3a82"},{"start":"3416.255","dur":"4.905","text":"Right. Now, one practical reason why I still use algorithms like the GDA,","_id":"aaffe0b610214865902118ec3aafbe42"},{"start":"3413.99","dur":"2.265","text":"figure out whether it wants to figure out from the data.","_id":"996d865cd0c64dbd8e10910a7c361052"},{"start":"3412.07","dur":"1.92","text":"less assumptions and just lets the algorithm","_id":"92d905bb002d45e09259a2e52ecd6cd9"},{"start":"3409.1","dur":"2.97","text":"there is a strong trend to use logistic regression which makes","_id":"b9d137b3c52e46d69e768303daaaede8"},{"start":"3407.21","dur":"1.89","text":"we have a lot of data, you know,","_id":"e6ebd9ff5cc44ca7afebd06db540c057"},{"start":"3404.63","dur":"2.58","text":"the data and in this era of big data,","_id":"7ea3817cf09945a79091a33c1f745196"},{"start":"3402.35","dur":"2.28","text":"And the second source of knowledge is learned from","_id":"060a003c57ca46bf8f77ce9f8fa173e9"},{"start":"3400.625","dur":"1.725","text":"what are the assumptions you told it to make?","_id":"18cdcd196d364f37bbbe17d37bd9dc38"},{"start":"3397.85","dur":"2.775","text":"Uh, one source of knowledge is what did you tell it,","_id":"b069e79d33d44e1daa9b705607a5a4fc"},{"start":"3394.73","dur":"3.12","text":"Right. So- so the algorithm has two sources of knowledge.","_id":"9ec48c9f28814d0fa6a73ad403ec2216"},{"start":"3391.07","dur":"3.66","text":"you could overcome telling the algorithm less about the world.","_id":"49b505ad0afe45f79dc749bca68e520e"},{"start":"3389.18","dur":"1.89","text":"Because with more data,","_id":"94717e2f4304436f96ea963471f0721a"},{"start":"3386.78","dur":"2.4","text":"I would probably use logistic regression.","_id":"d9f5a787791a4aac9015eb2db3be27df"},{"start":"3383.735","dur":"3.045","text":"and so for a lot of problems we have a lot of data,","_id":"24dc82d78ad7440685adb1e09d886a85"},{"start":"3381.35","dur":"2.385","text":"Right. Digital Civil Society which is a lot of data","_id":"1568c491ffef481e981c34244fdaa5ca"},{"start":"3377.27","dur":"4.08","text":"So I think that the whole world has moved toward using bigger than three datasets.","_id":"83964904da8c4f1ea62f7bba187b81ac"},{"start":"3375.44","dur":"1.83","text":"um, these two algorithms.","_id":"0df528b963a740aab742ba7b5e613554"},{"start":"3373.115","dur":"2.325","text":"I- I- I actually tell you the way I choose to use,","_id":"64fb6147873a44ac99bd6826012b1a7d"},{"start":"3371.6","dur":"1.515","text":"So I think a lot of it is amount of degree.","_id":"94da70f1b6834d30b3a5b67513bf109a"},{"start":"3369.26","dur":"2.34","text":"it you can convince yourself is vaguely Gaussian.","_id":"3c599cdd5e5049f59a4b21a7b555ee3d"},{"start":"3366.995","dur":"2.265","text":"will not really be Gaussian but a lot of","_id":"4eb18645f7ee407ab2c0217ae3c8d7f7"},{"start":"3364.43","dur":"2.565","text":"You could plot it and most data that you plot, you know,","_id":"70dc9fd650f04c6588808a2235febe9d"},{"start":"3361.22","dur":"3.21","text":"continuous value data- no, ther- ther- there are exceptions.","_id":"db3b23368cd24887897f0b6ad4d098ac"},{"start":"3359.165","dur":"2.055","text":"Right. If- if you plot- actually if you take","_id":"80d38c7b058d48e6992224f80478d75f"},{"start":"3355.535","dur":"3.63","text":"Yeah, but- but, um- I think it's actually a- a matter of degree.","_id":"583a8b12685d404da8eb32dad097fbaa"},{"start":"3352.52","dur":"3.015","text":"uh, uh, except at this feed data, I guess.","_id":"bad957a1106e47ecacc6977bab456c93"},{"start":"3348.71","dur":"3.81","text":"Right. Most data on this universe is Gaussian [LAUGHTER] uh,","_id":"7051a853832d47a8a6c737356c807987"},{"start":"3347.555","dur":"1.155","text":"it's a matter of degree.","_id":"a5e1c1fb80a24c7caaf34aa73c48cff9"},{"start":"3345.56","dur":"1.995","text":"uh- yeah, you know,","_id":"6e6f5cc5eda84461b582b5781e8a308a"},{"start":"3344.22","dur":"1.34","text":"you know, it's, uh,","_id":"c9ffe80b672740d4b636299b3dccd4f4"},{"start":"3341.8","dur":"2.42","text":"Practical sample without data is a Gaussian probably,","_id":"69891159211840d2ae23e6179d412ea6"},{"start":"3340.6","dur":"1.2","text":"Oh, oh, yeah.","_id":"6932612151ad4a14b800dc2878010bc4"},{"start":"3335.36","dur":"5.24","text":"of data it usually has a Gaussian problem?","_id":"d433075986854f21bc878737aca59181"},{"start":"3330.14","dur":"5.22","text":"Just from that, is there a point do you know like what sort","_id":"e02fc4191f83401aa5b66902ea0e7b7d"},{"start":"3327.17","dur":"2.97","text":"Okay. Your question at the back or a few questions. Go ahead.","_id":"ae294ff05e474199a0fc9703cda695ef"},{"start":"3324.92","dur":"2.25","text":"then it will actually do- do- do better.","_id":"dc14700829b4491ea1ad8cd034b6d232"},{"start":"3321.635","dur":"3.285","text":"you know, \"Hey, algorithm, the world is Gaussian,\" and if it is Gaussian,","_id":"41026e0054074f15a260ffb5bea65659"},{"start":"3318.965","dur":"2.67","text":"telling the algorithm more truth about the world which is,","_id":"5d2ba66e8b2c449187f200beb39a48b7"},{"start":"3316.415","dur":"2.55","text":"because by making more assumptions you're just","_id":"21cea1da2ef94c9797d8e93ff209bf41"},{"start":"3311.375","dur":"5.04","text":"using a model that makes more assumptions will actually allow you to do better","_id":"b21436b64da64df6a4dc1dd31b8d8bcb"},{"start":"3308.315","dur":"3.06","text":"if you have a very small dataset, then, um,","_id":"5f398e7cb537477bbc56dc9f59fdcea5"},{"start":"3306.56","dur":"1.755","text":"Uh, but on the flip side,","_id":"647abc6ad605418da4be6b7b4cdd7094"},{"start":"3303.2","dur":"3.36","text":"assumptions such as accidentally assuming the data is Gaussian and it is not.","_id":"1bccf60d19f048a384bb0524c7075949"},{"start":"3300.35","dur":"2.85","text":"then your algorithm will be more robust to modeling","_id":"f992734de3a748a996daf93197760342"},{"start":"3295.72","dur":"4.63","text":"if you make weaker assumptions as in logistic regression,","_id":"d90480fd9c734bec910fd6ce88573a55"},{"start":"3286.91","dur":"7.27","text":"Okay. So the key high level principles when you take away from this is, um, uh, uh,","_id":"f0e20a2241b14c1cac30f2394b2c43ce"},{"start":"3284.119","dur":"2.791","text":"then your model might do quite poorly.","_id":"50bf8e2e355543edb94d935f753947e7"},{"start":"3279.395","dur":"4.724","text":"Right. But if your data was actually Poisson but you assumed it was Gaussian,","_id":"060720a9b2864146be7d0d34033b901b"},{"start":"3276.56","dur":"2.835","text":"it- it'll do fine under all of those scenarios.","_id":"840f2d5bf8df483fb064567afebf2020"},{"start":"3274.685","dur":"1.875","text":"But if you're fitting logistic regression,","_id":"53f0ccd39856487589c7091b905cf1b2"},{"start":"3270.575","dur":"4.11","text":"Is it Poisson? Is this some other exponential family model? Maybe you just don't know.","_id":"32ab771a33274d9c86e461decbf5dd96"},{"start":"3268.43","dur":"2.145","text":"And you don't know, is a data Gaussian?","_id":"4fd51fea0a21460cba44e6e817a8b571"},{"start":"3265.37","dur":"3.06","text":"uh, a model, binary classification model to some data.","_id":"eb836e23b32940ad9cb83ce7225c08d5"},{"start":"3262.64","dur":"2.73","text":"you are fitting data to s- maybe a fitting, uh,","_id":"7fc07d7d2c49477b871af1820e25ba2c"},{"start":"3260.34","dur":"2.3","text":"Right. So- so, you know, maybe, um,","_id":"074cf0f8fcb043db89f5fa90c9f3b56c"},{"start":"3258.865","dur":"1.475","text":"It'll work fine either way.","_id":"7c8f3f5a49ba4cd8b729ad0715fd53bd"},{"start":"3254.98","dur":"3.885","text":"um, if you're using logistic regression you don't need to worry about it.","_id":"6a4d7f99a14143ae935a047038370fd8"},{"start":"3251.785","dur":"3.195","text":"if you don't know if your data is Gaussian or Poisson,","_id":"5fc96c8e67a94b39b696cfaa3bf70a7d"},{"start":"3246.83","dur":"4.955","text":"Right. And so what this means is that, um,","_id":"4e32758d7797448e8d4b538819cc90be"},{"start":"3244.565","dur":"2.265","text":"Excuse me, of the exponential family distribution.","_id":"28f066b860d842dca0d5efa133dd7899"},{"start":"3242.03","dur":"2.535","text":"according to the natural parameter as a generalized name.","_id":"54960a549cc043c3a59659f0a9d8d44f"},{"start":"3239.93","dur":"2.1","text":"these two distributions varies only","_id":"a4b3906747924397993c3d3418194fbb"},{"start":"3237.77","dur":"2.16","text":"where- where, uh, the difference between","_id":"1c07979452054a498c5658c4770422a8"},{"start":"3234.605","dur":"3.165","text":"any generalized linear model, actually where, uh,","_id":"956a65b98cc849b8b668eae8f18e07e0"},{"start":"3232.97","dur":"1.635","text":"And this is actually true for, um,","_id":"bd9bc8ed1b5d47acb644d47e9a17b46e"},{"start":"3227.6","dur":"5.37","text":"This is logistic, okay, and you can prove this.","_id":"f6b02b66b6bb4d01845bfb572b01caa4"},{"start":"3219.085","dur":"8.515","text":"Right. It turns out that this set of assumptions also imply that p of y equals 1 given x.","_id":"32c7ca61a3d34c68ae8681593cb179f2"},{"start":"3215.68","dur":"3.405","text":"as before, is Bernoulli 5x.","_id":"8f777904775446bdbd82b2467d12b075"},{"start":"3211.49","dur":"4.19","text":"uh, Lambda_0, or lambda_1 not 0 and y,","_id":"0ec5626b69cd43d387cc2000728842c9"},{"start":"3202.07","dur":"9.42","text":"parameter Lambda_1 and x given y equals 0 is Poisson with mean,","_id":"e72f206231664da097402944cad15f1d"},{"start":"3194.975","dur":"7.095","text":"let's say that x given y equals 1 is Poisson with, uh,","_id":"64dd478c665d4337982e2efb28ce8666"},{"start":"3191.225","dur":"3.75","text":"which is let's say the following are true;","_id":"dfd3fb0e0bea487a85f003784fd10503"},{"start":"3188.96","dur":"2.265","text":"Here's another example, get to your question in a second,","_id":"86815b28cc4948a1895c60fe7c0c46e4"},{"start":"3184.865","dur":"4.095","text":"Okay. So here's one fun fact.","_id":"25533a4543b04c0badb7d38df9853627"},{"start":"3181.01","dur":"3.855","text":"at all Gaussian and then GDA would do more poorly.","_id":"b29076fe18d942f3a67e0040efaace38"},{"start":"3177.86","dur":"3.15","text":"You might be trying to fit a Gaussian density to data that is not","_id":"32d754b7282f403598a44c49ac301c4b"},{"start":"3175.445","dur":"2.415","text":"then this might be a very bad set of assumptions to make.","_id":"5097f14e9ec148738fc7a8dec9bab8a1"},{"start":"3172.94","dur":"2.505","text":"So if x given y is not at all Gaussian,","_id":"f30199e81c904fb7a214916c50b4c967"},{"start":"3170.81","dur":"2.13","text":"if these assumptions turn out to be wrong.","_id":"93f08b68816f4dc590820d920e957ba9"},{"start":"3168.545","dur":"2.265","text":"And the problem with GDA is,","_id":"f8ed1012865741c5a2322d004bb259a9"},{"start":"3166.595","dur":"1.95","text":"then GA will do better.","_id":"a52af3f1d34a4e8195f5b9e03aa05404"},{"start":"3164.84","dur":"1.755","text":"if these assumptions are roughly correct,","_id":"0affb34ee09746479ea834e0556a786d"},{"start":"3161.63","dur":"3.21","text":"And so even if a very small dataset, um,","_id":"6c32cbf888a94ea182b64984d170eab6"},{"start":"3158","dur":"3.63","text":"the algorithm x given y is Gaussian and so it can be more efficient.","_id":"f08554c29f67496095b07ed54e918ee1"},{"start":"3154.655","dur":"3.345","text":"then GDA will do better because you're telling","_id":"cb4ff466431b488a831704f0ca19f550"},{"start":"3150.41","dur":"4.245","text":"So if indeed x given y is Gaussian,","_id":"4264316ba1bc41ba9e6681c7f26b7eb6"},{"start":"3145.205","dur":"5.205","text":"then your model will do better because you're telling more information to the algorithm.","_id":"20a5dd8333d243dd99b17c371397bd3a"},{"start":"3142.64","dur":"2.565","text":"and if your modeling assumptions are roughly correct,","_id":"3ca5a5ee86ae44609977e94ae6fd8ebf"},{"start":"3140.135","dur":"2.505","text":"if you make strongly modeling assumptions","_id":"0da0412fc4a5487aa5a5857e027e06d0"},{"start":"3135.305","dur":"4.83","text":"And so what you see in a lot of learning algorithms is that, um,","_id":"03a1c84b084d4ab6bf31442c8afe2415"},{"start":"3131.87","dur":"3.435","text":"as- as- uh, let's see.","_id":"1e482a3f1da2451f8c335e5abe805411"},{"start":"3125.465","dur":"6.405","text":"Okay. Um, and by the way as- as- uh,","_id":"3dc279977f464962aaeea33c5522bf25"},{"start":"3120.74","dur":"4.725","text":"assumptions because you can prove these assumptions from these assumptions.","_id":"ad3e310c3cdb4bba9d700cd46844cd5e"},{"start":"3115.01","dur":"5.73","text":"a weaker set of","_id":"ba8bc05c1b414598ade0ee99268bc790"},{"start":"3105.605","dur":"9.405","text":"this makes a stronger set of assumptions and which this regression makes","_id":"32a77fd616b94eea987e97a944a507c0"},{"start":"3103.205","dur":"2.4","text":"the generative learning algorithm in this case,","_id":"c8175789d6c44368af657c0d29c2a513"},{"start":"3097.91","dur":"5.295","text":"Right. So what this means is that GDA,","_id":"805c75b1631d459eb4c6293194e4a0e3"},{"start":"3093.695","dur":"4.215","text":"uh, uh, x given y equals 0 is Gaussian x given y equals 1 is Gaussian.","_id":"6ed20fa612d7439a9b60551a531cec4c"},{"start":"3088.625","dur":"5.07","text":"this does not in any way shape or form assume that x given y is Gaussian,","_id":"130407e9b6614151a9d4dffcaeb5b848"},{"start":"3085.1","dur":"3.525","text":"given x is governed by logistic function by- by this shape,","_id":"f57d02c675e14581badff533d759a867"},{"start":"3081.265","dur":"3.835","text":"Right. So if you assume p of y equals 1","_id":"40d11f17a1e74a2bb2fe42004d898afa"},{"start":"3074.065","dur":"7.2","text":"Right. But it turns out that the implication in the opposite direction is not true.","_id":"f7249a0dd3a94243888f38d399ae360e"},{"start":"3067.41","dur":"6.655","text":"implies that p of y equals 1 given x is governed by a logistic function.","_id":"45f04f896d7e4708b8e24f559b1b5426"},{"start":"3064.69","dur":"2.65","text":"this set of assumptions","_id":"31de9b73c7e74c4c9308ccd1e29121c1"},{"start":"3062.98","dur":"1.71","text":"But what that illustrates is that,","_id":"6b4c649a1a444f359dd1c57e35fac5dc"},{"start":"3061.18","dur":"1.8","text":"You prove it yourself in a homework problem.","_id":"28398ce4a36e461d9879a30a128ba22a"},{"start":"3059.56","dur":"1.62","text":"Um, it doesn't prove it.","_id":"fd2905c29137455fa3c066adccd02d23"},{"start":"3057.66","dur":"1.9","text":"What that illustrates.","_id":"fbf2fd044a33421385309f73b0cd0848"},{"start":"3053.63","dur":"4.03","text":"point-by-point to really the sigmoid curve I drew on the other board.","_id":"88c9353efcec4b9b80677c5f8e46d898"},{"start":"3050.63","dur":"3","text":"plotting you know p of y equals 1 given x","_id":"083450cf96b04d0eaeaa0663e03c4039"},{"start":"3045.31","dur":"5.32","text":"Okay. And the argument that I just described just now, uh,","_id":"e660028ce02f439cb965fb566cb811fb"},{"start":"3038.21","dur":"6.64","text":"um, p of y equals 1 given x is logistic.","_id":"cd47e214855b45f4b4767cb666bc52ff"},{"start":"3036.68","dur":"1.53","text":"uh, it's assumed that this is,","_id":"8934d007b7ad4a169e0158e060234455"},{"start":"3035.465","dur":"1.215","text":"So- so in other words,","_id":"79617e907c94461cb4b0cc91167a1d62"},{"start":"3032.69","dur":"2.775","text":"Right. So just- just- okay.","_id":"d841dc6c195c431a87d44aebe7301e94"},{"start":"3026.72","dur":"5.97","text":"We-where some details about x_0 equals 1 and so on.","_id":"5b31aa3ee3404239bc0d9250a2792cca"},{"start":"3022.985","dur":"3.735","text":"Right. So this is really 1 over 1 plus e is a negative Theta transpose x.","_id":"195aad3aceac4a28b1d2d8822682a9a1"},{"start":"3020.105","dur":"2.88","text":"You know, that this is, uh, governed by logistic function.","_id":"c8773ef3bf3b4a01b39982e4cac73a65"},{"start":"3009.95","dur":"10.155","text":"Uh, so what logistic regression assumes is p of y equals 1 given x.","_id":"4940ff1d2ca14d9eaa2bec2f92c2dc72"},{"start":"3003.59","dur":"6.36","text":"[LAUGHTER] Um, let's see.","_id":"9e34360ceb044632924373776cde0eaf"},{"start":"3001.445","dur":"2.145","text":"Good. Hurricane stopped.","_id":"14a0b561c5c84f1fbc8337cefd3a7688"},{"start":"2998.55","dur":"2.895","text":"the UN report on global warming but hopefully- all right.","_id":"14fe8dd5a4c44732ab8f2b2eafaf0b03"},{"start":"2996.39","dur":"2.16","text":"It's slightly scary actually wa- the-","_id":"ce18e604f06d40f2a32f7a947541f3e4"},{"start":"2992.475","dur":"3.915","text":"Okay. It's okay. Did you guys see the UN report?","_id":"81a66d149f5e4498aa09449d47773d25"},{"start":"2989.64","dur":"2.835","text":"I hope we don't already have storms here, um.","_id":"0ab5148748ed4a51b54db89666ce8b4c"},{"start":"2986.13","dur":"3.51","text":"the-there's just a scary UN report on global [LAUGHTER] warming over the weekend.","_id":"5a7787c126d540dba70715e7e986a9a6"},{"start":"2982.425","dur":"3.705","text":"Yeah. Why? You know","_id":"b61c395f12584463bdc34d0588480b44"},{"start":"2980.76","dur":"1.665","text":"Okay. Cool. All right.","_id":"4b3ba150ca0047f7a23c8c571f0edea7"},{"start":"2980.26","dur":"0.5","text":"I see.","_id":"f50bba190b4443a9842a1d2942ac5c9a"},{"start":"2979.545","dur":"0.715","text":"Yeah.","_id":"6e9728995735409eaae5352d9e5f6ebf"},{"start":"2970.38","dur":"9.165","text":"uh, there is some [LAUGHTER] strange wind at the back, is it?","_id":"d12f629dd6db495e8a7b367dfd9eaa27"},{"start":"2963.9","dur":"6.48","text":"[NOISE] This is a discriminative algorithm,","_id":"3b1b25d82beb4b36b4b0ce237b046087"},{"start":"2960.765","dur":"3.135","text":"Right. And what logistic regression does.","_id":"740e16144d3347229244aea0ce4d552d"},{"start":"2952.635","dur":"8.13","text":"and y is Bernoulli with, um, parenthesis Phi.","_id":"e46935301cfc4b0398da2b8f2de5f54e"},{"start":"2949.17","dur":"3.465","text":"this is Gaussian with mean Mu_1 and covariance Sigma,","_id":"24410ff0a92a4ddc8fee16c58985b49a"},{"start":"2946.875","dur":"2.295","text":"It assumes x given y equals 1,","_id":"3df8dfbf00954858b53ec6aaae42f823"},{"start":"2942.9","dur":"3.975","text":"this is Gaussian, with mean Mu_0 and co variance Sigma.","_id":"70493992e448412bb850a92ddc8876c1"},{"start":"2937.46","dur":"5.44","text":"This assumes that x given y equals 0,","_id":"88a3a2bf7264442bac27ffedb33f8d40"},{"start":"2933.96","dur":"3.07","text":"So the generative approach.","_id":"ba555afdec0a4b6a8449094255ab04a5"},{"start":"2928.17","dur":"5.79","text":"All right. So GDA, Gaussian Distributed Analysis.","_id":"31260167eb1149fd9f70e101919b350a"},{"start":"2912.88","dur":"15.29","text":"[BACKGROUND]","_id":"7e37ffbf363f4363b07849584a4cf85b"},{"start":"2905.29","dur":"7.59","text":"Um, let's see if I can get rid of this.","_id":"c90ead112ca04addb42136be297ff9b3"},{"start":"2901.69","dur":"3.6","text":"a distributed algorithm like logistic regression is superior.","_id":"09f47bad26de4163b2537003c6cf1d76"},{"start":"2898.075","dur":"3.615","text":"a genitive algorithm like GDA is superior and when","_id":"fd889a3d453644b2be35173f08527754"},{"start":"2893.53","dur":"4.545","text":"Right. So, um, let's discuss when","_id":"dcc1c4034b1f41c28578dde7c78bc164"},{"start":"2888.67","dur":"4.86","text":"that the two algorithms actually come up with two different decision boundaries.","_id":"fcbe4c8ff9094b9c836cda43fe25cee5"},{"start":"2885.43","dur":"3.24","text":"projecting the results on the display just now in PowerPoint,","_id":"380fac486f614c149067685201b04863"},{"start":"2882.61","dur":"2.82","text":"choosing are quite different and you saw when I was","_id":"406ca524385341bfb2996755f245cd01"},{"start":"2877.48","dur":"5.13","text":"Right. But, um, the specific choice of the parameters they end up","_id":"540e73bc677942148c61e3204e074c48"},{"start":"2874.27","dur":"3.21","text":"you actually use this calculation rather than compute a sigmoid function.","_id":"482051d838534d4d8b72a3fb11b0a954"},{"start":"2873.145","dur":"1.125","text":"I guess the mechanics is,","_id":"6c23120896cd4e1c9bbc47225abc2f73"},{"start":"2870.895","dur":"2.25","text":"the outcome ends up being a sigmoid function.","_id":"c339770537a247e181e3c48c15d68090"},{"start":"2863.725","dur":"7.17","text":"a sigmoid function to calculate P of Y equals 1 given X or- or the,","_id":"08513b24e1be43e481d379b9e490f706"},{"start":"2858.16","dur":"5.565","text":"both logistic regression and Gaussian discriminant analysis actually end up using","_id":"0316c35b0ba04c4eb2b442dc40af42d4"},{"start":"2847.06","dur":"11.1","text":"Right. Um, so, um,","_id":"616ff18fb7914046bdf0a2e407dec830"},{"start":"2842.26","dur":"4.8","text":"exactly a shaped sigmoid function and you prove this in the problem sets as well.","_id":"63642e8f76504af59708e5218c108054"},{"start":"2840.49","dur":"1.77","text":"The shape of that turns out to be","_id":"04823461d39c4184b7a460cce1f31454"},{"start":"2836.23","dur":"4.26","text":"um, then this is exactly a sigmoid function.","_id":"31810f6ad9274dae8716e83fb477bf6b"},{"start":"2833.29","dur":"2.94","text":"It turns out that if you connect up the dots,","_id":"71777d226dd346478aead565ac2eca06"},{"start":"2830.395","dur":"2.895","text":"you know, the values you get a curve like this.","_id":"f02df0582824433bab68af2890ccc5c1"},{"start":"2828.145","dur":"2.25","text":"Is the probability and go ahead and plot,","_id":"23ddd440206c4f5a838800ce3a6661f6"},{"start":"2824.5","dur":"3.645","text":"this formula which will give you a number between 0 and 1.","_id":"a66a187620eb4d53b65bbae8481efba1"},{"start":"2821.35","dur":"3.15","text":"for a dense grid on the x-axis evaluate","_id":"4a0dd52bfd7f47cab4b025d439394b25"},{"start":"2817.555","dur":"3.795","text":"Right, and you do this exercise and actually just for every point, you know,","_id":"1b2b9ad75a934cabbbeeea276c79da4a"},{"start":"2814.45","dur":"3.105","text":"it becomes very very close to 1.","_id":"b73faba0da064d278ad6ccd7e2633a2a"},{"start":"2812.08","dur":"2.37","text":"And then beyond a certain point,","_id":"77f75a41b9244249bfde97f8197108e5"},{"start":"2808.105","dur":"3.975","text":"it increases to 0.5 and it surpasses 0.5.","_id":"9a281e544889465a90a72eae7096b04d"},{"start":"2803.095","dur":"5.01","text":"the Y equals 1 class is very small and as you approach this midpoint,","_id":"f0e06dd6d291466e8d42aca994aa1978"},{"start":"2799.225","dur":"3.87","text":"the chance of this coming from, um,","_id":"30f50da038ef40e6b6956d278343d070"},{"start":"2797.335","dur":"1.89","text":"for points far to the left,","_id":"475f3528c039427f97371c1fcbdc6d3e"},{"start":"2791.83","dur":"5.505","text":"sweeping from left to right for many many points on the X axis you find that,","_id":"eea5acd02be54acd8f7ed6949562f836"},{"start":"2787.03","dur":"4.8","text":"Right. Now, it turns out that if you repeat this exercise","_id":"e6ba74a8b3b2431e98c1012f0f8b1628"},{"start":"2783.865","dur":"3.165","text":"you know, you get a point like that.","_id":"e952a04525694d38a2f68837ca07dd92"},{"start":"2780.205","dur":"3.66","text":"then you'd be pretty sure this came from the positive examples and so,","_id":"e31cea2668d2496d857b65ba5d794a24"},{"start":"2778.87","dur":"1.335","text":"if you get an example way here,","_id":"49c4ca444b6d4bca9dc8fc80bcacb264"},{"start":"2776.215","dur":"2.655","text":"Um, then if you go to a point away to the variance,","_id":"9a715209ce044ff7b0d35869b57a7805"},{"start":"2771.19","dur":"5.025","text":"would have P of Y equals 1 given X is 0.5.","_id":"1782e414e87e407e8b6f82f3f4df84eb"},{"start":"2767.05","dur":"4.14","text":"So I guess if this is 0.5 for that midpoint you","_id":"0d8eb2ed868f48a18dac783db25cd867"},{"start":"2763.87","dur":"3.18","text":"Can't tell. Right. So this is really 50-50.","_id":"384a68f0fda044fdb8c2194d33cb986d"},{"start":"2761.575","dur":"2.295","text":"Did this come from the negative or the positive Gaussian?","_id":"45295f54308143ccbac72cba5124d1cf"},{"start":"2759.595","dur":"1.98","text":"you- you really have no idea. You really can't tell.","_id":"4ffd637576174046adfa10b58f9c1879"},{"start":"2757.615","dur":"1.98","text":"Well, if you're getting example right at the midpoint,","_id":"9e44c586624846a1888e5e651823b005"},{"start":"2755.2","dur":"2.415","text":"Right, how about this point, the midpoint.","_id":"35e71403a18349cf8928eddde13c623c"},{"start":"2753.43","dur":"1.77","text":"Um, let's pick another point.","_id":"8c19f712ffb147adbf29e2e463967cd3"},{"start":"2750.595","dur":"2.835","text":"very close to 0, right.","_id":"109110f5153c49db9d0c4ba01b1761a7"},{"start":"2748.87","dur":"1.725","text":"you end up with a point you know,","_id":"26bc9e9d90f947ecb7e95d0bf4b61278"},{"start":"2747.49","dur":"1.38","text":"So for a point-like that,","_id":"5d8013652388497eb84ab89ad542bbff"},{"start":"2743.53","dur":"3.96","text":"Right, and so chance of P- P of Y equals 1 given X is very small.","_id":"ad6cc00e4eac47f7becadf38defc06a1"},{"start":"2739.84","dur":"3.69","text":"this Gaussian generating example all the way to left is almost 0.","_id":"85106816e9f44786807723a186b663a5"},{"start":"2738.46","dur":"1.38","text":"because the chance of","_id":"03f8854ed3854160a4af6011097ea2da"},{"start":"2734.785","dur":"3.675","text":"you're almost certain it came from the class 0 Gaussian","_id":"f0213dbd7b724bd98944cfb2ba843638"},{"start":"2732.145","dur":"2.64","text":"If- if you have an unlabeled example here,","_id":"fa003bba53f343568f8e164fb3e938ef"},{"start":"2727.96","dur":"4.185","text":"it almost certainly came from this Gaussian on the left.","_id":"abdec3252d2347ada75bdff6715b5a0c"},{"start":"2723.37","dur":"4.59","text":"calculate this ratio you find that if you have a point here,","_id":"8d0d78abb80449db90c194842127c145"},{"start":"2719.77","dur":"3.6","text":"Right. With this model you- if you actually","_id":"d60951534d44403bbb905a4ed34376f4"},{"start":"2714.82","dur":"4.95","text":"So, um, let's pick a point far to the left here.","_id":"0f60ce07a86541e691bf6a7ecf22f8b0"},{"start":"2709.6","dur":"5.22","text":"So the vertical axis here as P of Y equals 1 given different values of X.","_id":"eee3466ed1284e76a705e776aeb91d5b"},{"start":"2703.21","dur":"6.39","text":"plot P of Y equals 1 given X for different values of X.","_id":"c8bb71f8e77d4094a55fd2141d75c331"},{"start":"2697.285","dur":"5.925","text":"Okay. Now, let's go through that exercise I described on the left of trying to","_id":"0cfe5917099c4272ae65d40c17c54a67"},{"start":"2695.23","dur":"2.055","text":"Right, so one half prior.","_id":"e03265f7ae8a45c3807f89512d318d85"},{"start":"2690.235","dur":"4.995","text":"Then because the dataset is split 50-50 P of Y equals 1 is 0.5.","_id":"f412e876d95444ac9b0bf50e78fc59df"},{"start":"2686.605","dur":"3.63","text":"What does class 1 look like with two Gaussian bumps like this?","_id":"a5c8f36f6edc4b7f8d85daa6bd661498"},{"start":"2682.39","dur":"4.215","text":"but you know, you kinda model the Gaussian densities of what does this class 0 look like?","_id":"c002251d718f4b0a8ac900d5afc98814"},{"start":"2679.36","dur":"3.03","text":"we set the same variance to the two Gaussians,","_id":"149508582e98498b8a47f3f3316adc5c"},{"start":"2677.05","dur":"2.31","text":"Right, and- and again just to check on all details that","_id":"8b7b6344ed134f92a66d31eae57d8f6b"},{"start":"2670.42","dur":"6.63","text":"this bump on the right is P of X given Y equals 1.","_id":"769412bfbace402d9307ef95c23ba40f"},{"start":"2663.625","dur":"6.795","text":"Gaussians as follows where this bump on the left is P of X given Y equals 0 and","_id":"053de882c8924da3b8c74fd884099d13"},{"start":"2655.945","dur":"7.68","text":"And if you fit a Gaussian to each of these two data sets then you end up with, you know,","_id":"1723c9a913e3409f9b55b6664c544222"},{"start":"2652.32","dur":"3.625","text":"I just filled this data and mapped it down.","_id":"c25e04fbe62049fda14d34ad092c257c"},{"start":"2639.985","dur":"10.075","text":"So let me map all this data to an x-axis.","_id":"b27e9c16ae7d418991ad8cf09b78ea82"},{"start":"2635.53","dur":"4.455","text":"Um, with just one feature so that's why all the data is parsing on 1D.","_id":"dc868e25ba0340f6a4d99c5a93e39307"},{"start":"2629.67","dur":"5.86","text":"Okay, and let's see what Gaussian discriminant analysis will do on this dataset.","_id":"8bba1a8f0e514026a77f82a2cc373516"},{"start":"2625.345","dur":"4.105","text":"Right. So it's a simple dataset.","_id":"16438453db524d73af9baa3923c4795a"},{"start":"2619.03","dur":"6.315","text":"a few negative examples there and a few positive examples there.","_id":"cb9953ddedd748fa8b19e0969b62e0bb"},{"start":"2614.08","dur":"4.95","text":"so X is a- a- and let's say that you have","_id":"9e64612f76de4972a7ae63cf80591ea6"},{"start":"2611.62","dur":"2.46","text":"Let's say you have just one feature X,","_id":"5b53405c4d5948f5b411d480cb4ffa85"},{"start":"2605.395","dur":"6.225","text":"Okay. So, um, let's see.","_id":"2e09880551cd4ac6b0082ee8f92360d0"},{"start":"2602.74","dur":"2.655","text":"um, different values of X.","_id":"3c672a9cfe9a43bebaeed1f821672f50"},{"start":"2599.17","dur":"3.57","text":"for what function you get for this if you actually plot this for,","_id":"150325cd63e94752899534500d8e63b7"},{"start":"2594.385","dur":"4.785","text":"what function you'd get for P of Y equals 1 given X,","_id":"8f451bf50eff4017813a98e0a06ef4d9"},{"start":"2588.73","dur":"5.655","text":"through one example of","_id":"0eae21ace5754b23b0e845c74dd89d01"},{"start":"2583.72","dur":"5.01","text":"the chance of Y being 1 given X. So I'm gonna go","_id":"6b62355c17e54431ab7ed8e10ed8c4f4"},{"start":"2579.265","dur":"4.455","text":"you can compute this ratio and thus get a number for","_id":"7a581cde951a4e33a215da0f8a0199e5"},{"start":"2577.09","dur":"2.175","text":"But so for every value of x,","_id":"75fe104bd1904085b5020d7816d226ac"},{"start":"2573.76","dur":"3.33","text":"is that second term and you similarly calculate the denominator.","_id":"b0341cec2d514fc7b7c9025ef5e2ec77"},{"start":"2570.639","dur":"3.121","text":"so actually P of Y equals 1 parameterized by phi is just equal to phi","_id":"99faeddb4cbd4866a07cfd65e0a6b193"},{"start":"2566.08","dur":"4.559","text":"Um, this is the Bernoulli probability,","_id":"6956acfc03784c359abaabe63cc2bb43"},{"start":"2562.15","dur":"3.93","text":"just a number you compute by evaluating the Gaussian density.","_id":"79bd67391b1a48569a2e17fdb17fc37a"},{"start":"2560.26","dur":"1.89","text":"Once you have fixed all the parameters that's","_id":"000d0c469add4d5d9b1b3786479a82d4"},{"start":"2554.56","dur":"5.7","text":"this little thing and just as we saw earlier, I guess right.","_id":"a1214b91df304a05907f78cc6be926ad"},{"start":"2551.964","dur":"2.596","text":"you know this formula is equal to","_id":"fe3dc36e02824c789e7a4cdeabcb2f4d"},{"start":"2546.84","dur":"5.124","text":"Right. So by Bayes rule,","_id":"818bedc2fe50452eafd933e74ae97936"},{"start":"2535.675","dur":"7.345","text":"is parameterized by phi divided by P of X which depends on all the parameters, I guess.","_id":"6ddc8693432f4832b442087770336dd2"},{"start":"2531.85","dur":"3.825","text":"the various parameters times p of y equals 1,","_id":"a716a864dc724a4a9be691ab72e9ff86"},{"start":"2528.415","dur":"3.435","text":"which is parameterized by- right well,","_id":"e94e468b5f1f42f8ac531886fc99340b"},{"start":"2521.89","dur":"6.525","text":"this is equal to P of X given Y equals 1, you know,","_id":"1b9b3f0436d7416298045e0d9e2bca96"},{"start":"2519.685","dur":"2.205","text":"um, well, this formula,","_id":"fcc71c058c1b4d7fa88e13c2283203ec"},{"start":"2518.05","dur":"1.635","text":"but what this means is,","_id":"6512aff753084a1299429d63abb3c193"},{"start":"2514.41","dur":"3.64","text":"So I'm gonna do this little exercise in a second,","_id":"94717a4004e44b8d8999dbf350d2f93b"},{"start":"2507.32","dur":"3.59","text":"right, as a function of x.","_id":"570285013ccb4e6ba1d406cf0781f2f9"},{"start":"2503.05","dur":"3.61","text":"you're parameterized by all these things,","_id":"08f255d64d614de6b07c03dfed7fae0b"},{"start":"2498.24","dur":"4.81","text":"P of Y equals 1 given X,","_id":"033730fc87294d6c9d197fb279077b98"},{"start":"2489.655","dur":"5.905","text":"Um, I'm going to do an exercise where we're going to plot,","_id":"9391f4aa12454f9f996285eb517ed1b7"},{"start":"2484.23","dur":"5.425","text":"Right. So let's say you've learned some set of parameters.","_id":"28fa4ece09904762b6da7e2f4587d8b2"},{"start":"2476.005","dur":"5.515","text":"um, for a fixed set of parameters.","_id":"1e6c7b79ffe04cbcab47f7350b51a529"},{"start":"2469.53","dur":"6.475","text":"GDA to logistic regression and,","_id":"e78081cf6bd44d1e8a0742d033b88e2a"},{"start":"2462.52","dur":"3.94","text":"Well, let's- let's compare","_id":"80bb7188cdca49a393e227a477e30e26"},{"start":"2454.285","dur":"8.235","text":"about Gaussian discriminant analysis and it turns out that's- ah.","_id":"b9e5bec82f3a45968c16d157417a7763"},{"start":"2451.18","dur":"3.105","text":"Now, there's one very interesting property, um,","_id":"af7c6c2e818547a282cea0ddb1db0789"},{"start":"2430.33","dur":"20.85","text":"[BACKGROUND].","_id":"115b3346572f4a449c1cdc9384f02cf9"},{"start":"2425.705","dur":"4.625","text":"Um, now, there's one-","_id":"7f39b060c52a4516b8590abba8700a59"},{"start":"2421.345","dur":"3.625","text":"But it is actually not an unreasonable algorithm to do that as well.","_id":"3b531014e73b49e2968686a04e8b5f4c"},{"start":"2417.88","dur":"3.465","text":"you end up with a decision boundary that isn't linear anymore.","_id":"0e6eb4e2a09e4bb5bb07993c076246a5"},{"start":"2414.34","dur":"3.54","text":"but you double the number of parameters roughly and","_id":"28537443e07b413db0b35fd9668cd990"},{"start":"2411.25","dur":"3.09","text":"Right. There's- it is actually very reasonable to do so as well,","_id":"a3fc0c2c1d3a4f4795a6addf791f831a"},{"start":"2409.96","dur":"1.29","text":"and they'll actually work okay.","_id":"7f4785b3e40b48e8bf83fddb9c321e89"},{"start":"2407.02","dur":"2.94","text":"um, covariance matrix sigma 0 and sigma 1,","_id":"fd54e1c11e994e96bf784917f65d0005"},{"start":"2403.735","dur":"3.285","text":"And it turns out you could choose to use two separate,","_id":"9d7eb37eadeb4fbbb90d0f5827da20d6"},{"start":"2401.77","dur":"1.965","text":"um, uh, um, yeah.","_id":"d9525f6c0d7145b19239efd3b4340106"},{"start":"2398.89","dur":"2.88","text":"for a lot of problems if you want to linear decision boundary,","_id":"680f6ffe71d64410b0955b168d2a0509"},{"start":"2396.43","dur":"2.46","text":"the decision boundary ends up being linear and so","_id":"dfabe0b0ee5c494c86b0a5962406c0ae"},{"start":"2392.98","dur":"3.45","text":"It turns out that if you choose to build the model this way,","_id":"454826d94bb84c72a0a84277013c1356"},{"start":"2389.605","dur":"3.375","text":"It turns out that, um-.","_id":"97071bc74489458b9f0762eddd05b654"},{"start":"2385.66","dur":"3.945","text":"mu 0 and mu 1 and a single covariance matrix sigma?","_id":"8c4347934638484ca0bea75677c414d5"},{"start":"2382.54","dur":"3.12","text":"So why- why- why do we use two separate means,","_id":"8355dd58c19941bfa1ec2bb42464d49c"},{"start":"2381.671","dur":"0.869","text":"Oh, sure yes, good question.","_id":"a8598b94d83644ae80e94f9ec6013e8e"},{"start":"2368.36","dur":"13.311","text":"[inaudible].","_id":"adbb2146155042dc8aee709cc7162c90"},{"start":"2367.45","dur":"0.91","text":"[NOISE]","_id":"5cbb0b001b074bf891b5dc8482ff4749"},{"start":"2363.365","dur":"4.085","text":"let's go back to the- Any questions about this? Yeah.","_id":"98b3fedd62ba4ad2a0bbb061058d1c04"},{"start":"2356.86","dur":"6.505","text":"So, um. All right,","_id":"5766f1e5f0fe45aa938907c75eeeae61"},{"start":"2351.715","dur":"4.605","text":"Okay, but the way you arrive at these two decision boundaries are a little bit different.","_id":"1384f9177cf94f529729664ab7dd27bf"},{"start":"2348.67","dur":"3.045","text":"with slightly different decision boundaries.","_id":"09bfada6e72b48f2977c3a70c2a634b3"},{"start":"2346.03","dur":"2.64","text":"So- so- so these two algorithms actually come up","_id":"37776bfe5b5440f0813e0a2479f21df2"},{"start":"2340.045","dur":"5.985","text":"And I've- I've also drawn in green here the decision boundary for logistic regression.","_id":"f0c08175a4964a18bc2eb51bc1718974"},{"start":"2336.895","dur":"3.15","text":"you end there classifying as- as a positive examples.","_id":"f6c01297d9a44161bd8959cfa261e26a"},{"start":"2334.45","dur":"2.445","text":"points to the lower left of that line,","_id":"b9683747e7ab4680baf633d8ddb81db7"},{"start":"2332.47","dur":"1.98","text":"You end up classifying them as negative examples and","_id":"f93fefecaeac4fdfbd524e57f54b6c75"},{"start":"2329.815","dur":"2.655","text":"you are closer to the negative class.","_id":"1501697077964198ac874507f04d1a4b"},{"start":"2328.149","dur":"1.666","text":"to that straight line I just drew,","_id":"cdb8af0038b24934be8ec6d449634b9d"},{"start":"2323.815","dur":"4.334","text":"Right. So points to the upper right of this decision boundary,","_id":"1f5057961a374435aed27671d5e1ba6f"},{"start":"2318.58","dur":"5.235","text":"using that formula and it turns out that this implies the following decision boundary.","_id":"9109256c10164a3d9fea38a69e2a2950"},{"start":"2313.255","dur":"5.325","text":"for each point try to decide whether this is class label using Bayes' rule,","_id":"e81847b1aabc417a89865ed669b59566"},{"start":"2311.11","dur":"2.145","text":"And then what we do is,","_id":"62b75d83162b432e93371637378b5bfd"},{"start":"2306.91","dur":"4.2","text":"fit two Gaussian densities to the positive and negative examples.","_id":"ccb638de77674b4cb1586ee821712545"},{"start":"2303.805","dur":"3.105","text":"We actually don't quite look at them totally separately but we do","_id":"64b9c9d6a54249beb116d13ec8d8ed88"},{"start":"2300.01","dur":"3.795","text":"because we use the same covariance matrix sigma for the positive and negative classes.","_id":"bfc0b34201514b21aa306f0129a31755"},{"start":"2296.56","dur":"3.45","text":"I described this as if we look at the two classes separately","_id":"95d943c12edd4e21aff29b830a4e3983"},{"start":"2293.11","dur":"3.45","text":"Right, and just one- one technical detail, um,","_id":"fc9c8504b0754443856b65c59ac7a0d1"},{"start":"2287.605","dur":"5.505","text":"What we'll do, is fit Gaussians to the positive and negative examples.","_id":"5df19d80863c465286187e990792ab01"},{"start":"2283.915","dur":"3.69","text":"which is fit with Gaussian discriminant analysis.","_id":"23c377a9357c482eba83a4a8571c2832"},{"start":"2281.545","dur":"2.37","text":"What it does is the following,","_id":"c2880387958f4647b6a9a5a1204716e5"},{"start":"2278.695","dur":"2.85","text":"How about the generative learning algorithm?","_id":"690cb0b51ea041a1b1280dff4e05ef9b"},{"start":"2274.885","dur":"3.81","text":"really searching for a line that separates positive and negative examples.","_id":"92de56da01bd40d9bd7e27d03a69cfba"},{"start":"2273.565","dur":"1.32","text":"So that's logistic regression,","_id":"55e5fe5c0dbe4bf4b2e9faa9fb3bb621"},{"start":"2267.805","dur":"5.76","text":"about 20 iterations it will converge to that pretty decent discriminative boundary.","_id":"3064dd1bc6034ff48e0f178fa1a5933d"},{"start":"2264.435","dur":"3.37","text":"four iterations and so on and after","_id":"c014e16508fd40c9af3abd269db17779"},{"start":"2260.985","dur":"3.45","text":"There's two iterations, three iterations, um,","_id":"df7a7859d1f14f1b951bab15aea75ddb"},{"start":"2257.325","dur":"3.66","text":"um, one iteration of logistic regression moves the line there.","_id":"e71116b5e75649c7b4bce3b51b2a05db"},{"start":"2252.2","dur":"5.125","text":"And then if you run one iteration of gradient descent on the conditional likelihood,","_id":"aa922bc3efff41cd97551df0c41c9be6"},{"start":"2250.54","dur":"1.66","text":"with a random line I guess.","_id":"1fa39ec06f054103b5bf285a388e2815"},{"start":"2247.6","dur":"2.94","text":"it's more interesting to start off for the purposes of visualization,","_id":"e75b3155b1824ee3889c6e26bc7a47e6"},{"start":"2243.895","dur":"3.705","text":"I almost always initialize the parameters as 0 but- but this just, you know,","_id":"ecd010a6bee14ee094631c681c6e08a0"},{"start":"2241.87","dur":"2.025","text":"Typically, when you run a logistic regression,","_id":"74ba93c212da4cb5854d095f4d39aa87"},{"start":"2238.495","dur":"3.375","text":"Um, let say you initialize the parameters randomly.","_id":"9e4afae5ba5b4607b060c17ac2cf9bce"},{"start":"2235.78","dur":"2.715","text":"So let's start with a discriminative learning algorithm.","_id":"705e7fa54c7c4db1b36ea757bb723f98"},{"start":"2227.68","dur":"8.1","text":"Right. Um, here's example with two features X1 and X2 and positive and negative examples.","_id":"fd3d2e660fd347e8a1ca44b44f36e559"},{"start":"2224.14","dur":"3.54","text":"a generative learning algorithm will do on this dataset.","_id":"12a0084a84fa45c49114e0214703adac"},{"start":"2222.13","dur":"2.01","text":"a discriminative learning algorithm versus","_id":"3b33ca712a3548ac89bb8c7f14b01fec"},{"start":"2216.97","dur":"5.16","text":"All right. So let's look at the same dataset and compare and contrast what","_id":"f5c1831a198245dab7fc3375cc16f917"},{"start":"2209.41","dur":"7.56","text":"[NOISE].","_id":"051c88426a484c3fb5412c2d8c16ece5"},{"start":"2207.37","dur":"2.04","text":"what the algorithm is doing.","_id":"aac254bfe41c4b9ab4d11148b1158e33"},{"start":"2196.95","dur":"10.42","text":"Okay. So let's examine","_id":"a6d55f622210421b86c9c7c6a163db7f"},{"start":"2190.375","dur":"3.595","text":"then you'd have to normalize the probability, okay?","_id":"2a9039b427b149ffac301e3a5aaf8a16"},{"start":"2188.5","dur":"1.875","text":"but if you'd actually need a probability,","_id":"b3e362eb5fdb41f6989cdb6ba55062b2"},{"start":"2185.98","dur":"2.52","text":"if all you care about is to make a prediction,","_id":"9b6f25d4902d4c62a47f8f3091fe0a44"},{"start":"2183.775","dur":"2.205","text":"you don't bother to calculate the denominator,","_id":"5a0e661399f944e38ba6d9f23d26f71d"},{"start":"2181.75","dur":"2.025","text":"sometimes to save on computation,","_id":"88050d24047e49d2add1ae1828ae2a63"},{"start":"2179.05","dur":"2.7","text":"in a- with the generative learning algorithms,","_id":"55ec3c0a841740828dcd4a8af1bb46ba"},{"start":"2175.9","dur":"3.15","text":"when- when making predictions with Gaussian disc-","_id":"e8eab34bdff844149b9feca7b1e90c45"},{"start":"2171.46","dur":"4.44","text":"So when implementing, um, ah,","_id":"8262510d35cf4e0ebf71eae707fe9abd"},{"start":"2167.2","dur":"4.26","text":"p of x given y times p of y, okay?","_id":"cdcdad48a4444b4fbd1a5aa5dd523b94"},{"start":"2164.89","dur":"2.31","text":"just arg max over y,","_id":"384760d964c549a4805cfa1b318a3ee9"},{"start":"2162.1","dur":"2.79","text":"And so this is equal to,","_id":"c0eb00ddfbd1471985a311f5d34dcbcf"},{"start":"2160.48","dur":"1.62","text":"It's just some positive number.","_id":"c75f56faf0234af89a6533c8609f7db8"},{"start":"2158.44","dur":"2.04","text":"it's- y doesn't even appear in there?","_id":"c8cb6ae6870345eb834444b3ec050509"},{"start":"2156.58","dur":"1.86","text":"It doesn't- it doesn't- it's a p of x,","_id":"7fb53a70e26147a7b504294f24e10d71"},{"start":"2153.745","dur":"2.835","text":"ah, this denominator is just a constant, right?","_id":"ed9645ea0eb8498f843e3a711182a31d"},{"start":"2151.51","dur":"2.235","text":"and you notice that,","_id":"4e861d4f465641b4bbbca0e8518166cc"},{"start":"2149.17","dur":"2.34","text":"Um, so that's equal to arg max of that,","_id":"82aac7c53d4b4fd3a8aa20c30957d0c4"},{"start":"2145.24","dur":"3.93","text":"and- so- so there's the arg max of this and this would be either 0 or 1, right?","_id":"19f8c979cb1c4b2c911ee26aaffb206a"},{"start":"2142.405","dur":"2.835","text":"So you might choose a value of y that maximizes this,","_id":"d22563556b694cd2ad911a5e43dfc6cb"},{"start":"2140.62","dur":"1.785","text":"You know what I'm saying? Well, what do I think is the value of y?","_id":"8226d582f3964f46a91d4215fe36f2fc"},{"start":"2138.565","dur":"2.055","text":"you don't wanna output a probability, right?","_id":"ff7eada387c946b6b4110881cee4edca"},{"start":"2136.93","dur":"1.635","text":"if you want to output a value for y,","_id":"368a02c7a9804e40bbcd2aeaaec59217"},{"start":"2134.17","dur":"2.76","text":"So ah, the prediction you actually want to make,","_id":"85a27c6b96b1415db5e23b4167b2675d"},{"start":"2131.08","dur":"3.09","text":"plug in to achieve that smallest possible value, right?","_id":"6f64709e46704515a25b0fb5362d8be9"},{"start":"2128.44","dur":"2.64","text":"and the arg min is the value you need to","_id":"8469c54d35bf4f52a9af2cf3e1b53bc0"},{"start":"2123.1","dur":"5.34","text":"Okay? So the min is the smallest possible value attained by the thing inside","_id":"5b3d248d55c34e9486743eb988553e40"},{"start":"2115.225","dur":"7.875","text":"and the arg min over z of z minus 5 squared is equal to 5.","_id":"8c9f19824f6a47269abee130714a8e02"},{"start":"2111.19","dur":"4.035","text":"because the smallest possible value of z by a 5 squared is 0, right?","_id":"0fb207ba83b6437bbe3b7245f4c93b25"},{"start":"2107.53","dur":"3.66","text":"z minus 5 squared is equal to 0","_id":"d9f839bac1094646af19ac329bed457b"},{"start":"2103.075","dur":"4.455","text":"So, you know, the Min over z of, uh,","_id":"22e9ea3724cf4972b7f3f93c9d6809a6"},{"start":"2097.885","dur":"5.19","text":"Ah, boy. All right.","_id":"e40465e8a5574e4a9613785f4de72aeb"},{"start":"2092.95","dur":"4.935","text":"So the, um, let's see.","_id":"edca6a70006d4194aa2871ff67f171a2"},{"start":"2089.155","dur":"3.795","text":"So, um, this is just an example.","_id":"f7c1a223cbe64a4a8f54c81847dba921"},{"start":"2087.61","dur":"1.545","text":"I- I- I'll go over this quickly.","_id":"86845f2943104246a93da426ffabcb9a"},{"start":"2083.8","dur":"3.81","text":"Most of you? Like two- two-thirds? Okay, cool.","_id":"f9ee401c491b4a9da7b103fe21abdee7"},{"start":"2079.84","dur":"3.96","text":"actually, how- how many of you are familiar with the arg max notation?","_id":"fcbbe2652c96488998a2ef1331933252"},{"start":"2076.315","dur":"3.525","text":"ah, I wanna introduce,","_id":"989fec1fe4a3468794664e87b0b19553"},{"start":"2074.065","dur":"2.25","text":"one- one more piece of notation which is,","_id":"2ae5cd08eb0f4a93b34fad3545cbf59b"},{"start":"2069.205","dur":"4.86","text":"Okay? Now, um, I wanna introduce one esh- well,","_id":"cbc9a7e810914e4bb079b936cf77da8f"},{"start":"2063.73","dur":"5.475","text":"p of y divided by p of x.","_id":"ffddb62b542b4f719292a2eb2d2bb7ce"},{"start":"2060.28","dur":"3.45","text":"this is max over y of p of x given y,","_id":"3d35ce8faf2f4c64a4d89505a68da6ae"},{"start":"2057.22","dur":"3.06","text":"Um, and by Bayes' rule,","_id":"40087fb022c4413f9a33ff0fe6ff37b4"},{"start":"2052.615","dur":"4.605","text":"of p of y, given x, right?","_id":"5ea8732be6b54ea28997c0038c2c3488"},{"start":"2049.78","dur":"2.835","text":"ah, you choose max over y,","_id":"d18dd21ae191414eb53ef7c3ca92fd3e"},{"start":"2041.935","dur":"7.845","text":"Um, so if you want to predict the most likely class label,","_id":"a8223333d35648d6b65d81cd2b94754b"},{"start":"2036.94","dur":"4.995","text":"how do you make a prediction for whether their tumor is malignant or benign?","_id":"8339ba1374ee46d49e3edbd4c9d0425f"},{"start":"2034.81","dur":"2.13","text":"So given the new patient, ah,","_id":"3d5efd546c3f4507be2e5a7f9ae2ad83"},{"start":"2026.545","dur":"8.265","text":"um, if you want to make a prediction, right?","_id":"8eac32adfb044431a60b1e8925a1c453"},{"start":"2020.05","dur":"6.495","text":"Um, finally, having fit these parameters,","_id":"ac63f47123dc469c9d9a2541fe909af0"},{"start":"2010.945","dur":"9.105","text":"Okay? So- All right.","_id":"f26d54995f194984b5f5fd25cdca4c9c"},{"start":"2008.575","dur":"2.37","text":"um, in the problem sets.","_id":"6793fbbbddb344de9a4c427c258f3417"},{"start":"2006.655","dur":"1.92","text":"so you can see that for yourself,","_id":"f5915be6e93e41ffb6cc0051bd8d71d6"},{"start":"2004.84","dur":"1.815","text":"By- by the same theories as you solved,","_id":"6d0a61f78308427ab160cb1dd34eeb8f"},{"start":"2001.405","dur":"3.435","text":"are the actual values that maximize this thing, right?","_id":"232885a061614feb819c6988959a4608"},{"start":"1998.015","dur":"3.39","text":"solve for all these values and prove more formally that these","_id":"b6adf645bf8b4adb97d5b44f1e2a1870"},{"start":"1995.63","dur":"2.385","text":"take derivatives, set derivatives equal to 0,","_id":"9987326c549945e9b7c3ab5f7eaa6e94"},{"start":"1993.98","dur":"1.65","text":"take logs, get the log likelihood,","_id":"b03c1801019a409dbe2ed1119576df6d"},{"start":"1990.785","dur":"3.195","text":"it's instead to look at the likelihood, ah,","_id":"d259d16d34364b65a15f78622870841c"},{"start":"1986.78","dur":"4.005","text":"these formulas is not by this intuitive argument that I just gave,","_id":"17c7b9e7646b45c99faa8e023f387364"},{"start":"1984.32","dur":"2.46","text":"But the mathematically sound way to get","_id":"df5418071b044422943fe6689a512af7"},{"start":"1980.93","dur":"3.39","text":"So that- that was the intuitive explanation for how you get these formulas.","_id":"b8dc7bf781ca477f9fa05f727c8012e5"},{"start":"1978.47","dur":"2.46","text":"you just look at these examples and pick the mean, right?","_id":"526d262703174a3ca5ff668eb1f09430"},{"start":"1975.5","dur":"2.97","text":"ah, and then it seems that the mean for Mu_0 and Mu_1,","_id":"a6a98ea0628a4766b484e5a1c54e42b1"},{"start":"1973.1","dur":"2.4","text":"just count up the fraction of coin tosses, they came up heads,","_id":"871e02078dba4b07b9a846f7c4565991"},{"start":"1971.12","dur":"1.98","text":"if you want to estimate the mean of a coin toss,","_id":"a609c47c502d4dd79d5a406ceceda3db"},{"start":"1969.995","dur":"1.125","text":"you know, I said, well,","_id":"c9877113b33b410ca50b79d15e4503bd"},{"start":"1963.185","dur":"6.81","text":"Okay? Um, So these are the- so- so- so the way- so the way I motivated this was,","_id":"8d024ecacff94a7aa1aab4aa9a9cf875"},{"start":"1958.94","dur":"4.245","text":"these corresponding means but you want one covariance matrix to both of these.","_id":"681d4fbf99b548daa4777ae4eea29ad4"},{"start":"1955.58","dur":"3.36","text":"Like we saw, ah, so- so try to fill the Gaussian to both of these with","_id":"56d217dddcc04fc8837aabb9bc9b7f42"},{"start":"1951.845","dur":"3.735","text":"you know, fit contours to the ellipse, right?","_id":"11fdcc384e6644c99e3eced3c514f86b"},{"start":"1948.11","dur":"3.735","text":"But the covariance matrix, basically tries to,","_id":"27cf14f25dcf4cdda6bc5b85dc700fcd"},{"start":"1945.53","dur":"2.58","text":"So we'll know how it works, okay?","_id":"1ed10fbe1b4f4c4cab136f7b5c290929"},{"start":"1942.68","dur":"2.85","text":"Ah, you can unpack the details in the lecture notes.","_id":"10f3e6f96bf74359ae686e0155c41f0e"},{"start":"1939.88","dur":"2.8","text":"Okay. Don't worry too much about that.","_id":"1f5090d046914cb5bf1b2c5035250d48"},{"start":"1927.5","dur":"5.98","text":"then I guess you can see the details in the homework.","_id":"d855363a5bec4af28574097a087491d7"},{"start":"1924.71","dur":"2.79","text":"But if you're less familiar,","_id":"988bb26240db4ea1903c3b26528505d4"},{"start":"1921.74","dur":"2.97","text":"this formula may not surprise you.","_id":"e4986300e8e344c394326fd1f1e00799"},{"start":"1917.8","dur":"3.94","text":"If you are familiar with covariance matrices,","_id":"0e125bc3b3b343d49f010fd1df61ffae"},{"start":"1912.515","dur":"4.345","text":"um, and then I just write this out.","_id":"f33bda144b7a4c798a46a6233ab45112"},{"start":"1909.14","dur":"3.375","text":"So that's maximum likelihood for Mu_1,","_id":"b8684d858c6d47c4a1eda7d56dd4fc92"},{"start":"1906.38","dur":"2.76","text":"divide by the total number of positive examples and get the means.","_id":"e494647f120a4984803b3ed3ca44198b"},{"start":"1903.875","dur":"2.505","text":"sum up all of the positive examples and","_id":"89caf9f2820544e4882ef95c5a9f0af4"},{"start":"1901.22","dur":"2.655","text":"no surprises, is sort of kind of what you'd expect,","_id":"e551cc00456448bd8baf7f0f016fe329"},{"start":"1898.63","dur":"2.59","text":"right, maximum likelihood for Mu 1,","_id":"09cdbdc4f5f5491fac3ab7341aea59b7"},{"start":"1881.675","dur":"8.845","text":"Okay? Um, and then,","_id":"7f999734b4444dde873a27849eb00227"},{"start":"1875.66","dur":"6.015","text":"and so that's just the mean of the feature vectors of all of the benign examples.","_id":"9b7590b387f34d6ebda55e9bed6d4564"},{"start":"1871.4","dur":"4.26","text":"the benign tumors divide by the total number of benign tumors in the training set,","_id":"700481d3020b4e0abae05ae1dd652a28"},{"start":"1868.73","dur":"2.67","text":"then you're summing up all of the feature vectors for","_id":"a9361767955f4537b2f87dbe58330ffb"},{"start":"1867.665","dur":"1.065","text":"if you take this fraction,","_id":"7fccb87c73d440aeb9fc3f9273312275"},{"start":"1865.115","dur":"2.55","text":"And then if you take this ratio,","_id":"ea3ff30419994b2693e221b4969cfb01"},{"start":"1861.13","dur":"3.985","text":"where y equals 0, okay?","_id":"7cd883b2c68c408691cd7c9aff9a138b"},{"start":"1850.79","dur":"7.69","text":"with y equals 0 and the denominator is a number of the examples,","_id":"d75071985cbd4b1892cb2952dc3d68cd"},{"start":"1847.445","dur":"3.345","text":"um, for all the examples","_id":"eb23dc8771a9400192a926c96f41f36d"},{"start":"1839.495","dur":"7.95","text":"so this is the sum of feature vectors for,","_id":"aa8d7a7ba5a44e4cbbca2082c6d46b4a"},{"start":"1838.31","dur":"1.185","text":"I- I just write this out,","_id":"dc27cc1266d1424f9c48857e507d994f"},{"start":"1836.48","dur":"1.83","text":"Does that make sense?","_id":"e3d76072c9f04442ac3d8c3cad0f5153"},{"start":"1832.85","dur":"3.63","text":"all the feature vectors for all of the examples that are benign.","_id":"183a7c05289c4e2990f66119329c7649"},{"start":"1827.09","dur":"5.76","text":"0 times the features and so the numerator is summing up all the features,","_id":"d609c585846444adaaf222c1893e29cb"},{"start":"1823.654","dur":"3.436","text":"whenever an example is malignant is","_id":"71cab4a1b0414a8596f51f0803ae8461"},{"start":"1818.54","dur":"5.114","text":"whenever, a tumor is benign is 1 times the features,","_id":"116099e392be489dbcc570d57a2b24f3"},{"start":"1815.375","dur":"3.165","text":"So the effect of that is, um,","_id":"dc1e7c4291c1462185517afd6c54e5e2"},{"start":"1809.675","dur":"5.7","text":"sum for m equals 1 through m indicator is a benign tumor times x_i.","_id":"8f7ab1ae652444588047b9324767bd79"},{"start":"1806.345","dur":"3.33","text":"Okay? Um, and the numerator, ah,","_id":"7b30ca6f0df742229baa0a1f90c51091"},{"start":"1802.25","dur":"4.095","text":"the total number of benign tumors in your training set.","_id":"7fcaa254ec0a4d7caefee0bb9a4ae2a7"},{"start":"1799.025","dur":"3.225","text":"um, ah, and so the denominator ends up being","_id":"d2d9dfb84fe9461f88127b677d8bf572"},{"start":"1795.545","dur":"3.48","text":"you get an extra 1 in this sum,","_id":"e8afd80b18144fb2a8e105a6028d53a3"},{"start":"1793.16","dur":"2.385","text":"Because every time y_i equals 0,","_id":"5328a917e7484f34ab8d361f54889699"},{"start":"1789.77","dur":"3.39","text":"examples that have benign tumors, right?","_id":"fb805dcd2c1b4d4d992d1c759578d058"},{"start":"1786.56","dur":"3.21","text":"and so the denominator will count up the number of","_id":"998a9bbe05154cfc9b3bb9ef6ba0c0d0"},{"start":"1780.89","dur":"5.67","text":"Um, So the denominator is sum from i equals 1 through m indicates a y_i equals, 0,","_id":"114aafc3ab8a4fb18cfab67903caba8e"},{"start":"1777.89","dur":"3","text":"So this is a way of writing out that intuition.","_id":"47a763ccb68f4ad78df5633661c27bbc"},{"start":"1775.325","dur":"2.565","text":"Look all of your negative examples and average their features.","_id":"8dd12dc31e7d400a87f45ad8cb2c9817"},{"start":"1771.665","dur":"3.66","text":"you know, seems like a pretty reasonable way to estimate Mu 0, right?","_id":"552185d12e0f4c47b9db4e949e102058"},{"start":"1768.8","dur":"2.865","text":"I guess, and you just take the mean of these, and that,","_id":"7f21d27cdc6b4da3b34721c7dfd2f6f9"},{"start":"1763.7","dur":"5.1","text":"Look at all of the- look at all of the benign tumors, all the Os,","_id":"2afb5b47762f4e4a94099ba21422a389"},{"start":"1761.885","dur":"1.815","text":"Just look- look at your training set.","_id":"b1290b49320a4584986ac8f94be5b197"},{"start":"1760.385","dur":"1.5","text":"that seems like a very reasonable way.","_id":"c4a4df77a4834731b43146c967dbf9cd"},{"start":"1757.16","dur":"3.225","text":"all the benign tumors in your training set and just take the average,","_id":"55cd902c86df456bb9b0560f6c644478"},{"start":"1756.14","dur":"1.02","text":"Well, what you do is you take","_id":"be2fbdb9f151401c81dffe80f92ee32e"},{"start":"1753.56","dur":"2.58","text":"ah, features for the benign tumors, right?","_id":"bd684c42f2f24b5a98a7d2cb05855fb9"},{"start":"1750.32","dur":"3.24","text":"what do you think is the maximum likelihood estimate of the mean of all of the,","_id":"82eb3bd9c2c043c89118c28def5f86aa"},{"start":"1748.4","dur":"1.92","text":"ah, put aside the math for now,","_id":"ca244bf773e84ca2b0046eec183eaa96"},{"start":"1744.75","dur":"3.65","text":"Ah, so, well, it- it actually if you,","_id":"814625868131477fb2ab8411d5c9ffc4"},{"start":"1742.86","dur":"1.89","text":"Okay.","_id":"c47341d7ff1e4fe8b9a694b3608c5fc3"},{"start":"1726.01","dur":"1.75","text":"I'll just write out.","_id":"672c5e4a9d0942cba9ebdfb79e535b32"},{"start":"1719.725","dur":"6.285","text":"Um, and then the maximum likelihood estimate for mu0 is this, um,","_id":"5afa355633c9429885740faa630d3db6"},{"start":"1716.925","dur":"2.8","text":"writing this formula, right.","_id":"29391bc026484a819bf15fc332f0eebd"},{"start":"1715.13","dur":"1.795","text":"So that's another way of writing,","_id":"a5de3a022834402f90e2d42a6b9a084b"},{"start":"1709.91","dur":"5.22","text":"a true statement is equal to 1 and indicator of a false statement is equal to 0.","_id":"03bd474994d64b639fc3843e1fdcfa40"},{"start":"1705.92","dur":"3.99","text":"So there's an indicator notation in which an indicator of","_id":"38848fb241804d388ec77a27af7d19d3"},{"start":"1702.14","dur":"3.78","text":"uh, uh, return 0 or 1 depending on whether the thing inside is true, right?","_id":"61c42b5bf37640ae9b7bf52f11850409"},{"start":"1698.96","dur":"3.18","text":"um, indicator yi, equals 1 is,","_id":"1c659edf9e32459d8035a2f5b9f5661e"},{"start":"1694.535","dur":"4.425","text":"this notation is an indicator function, uh, where,","_id":"d884705e12324600b5dc20917f7ce447"},{"start":"1692.69","dur":"1.845","text":"Okay. Um, so, um, uh,","_id":"6455f2aae0f449fab0588bad108c7f9d"},{"start":"1692.48","dur":"0.21","text":"No.","_id":"89fdc048017f4684a8856ae0f02514b8"},{"start":"1689.28","dur":"3.2","text":"Uh, did you so- do, did we talk about the indicator notation on Wednesday?","_id":"8bb7b5116e1f4ceb8046598dc7923931"},{"start":"1688.615","dur":"0.665","text":"No.","_id":"883c8004b89e43c494f3665c37aaced1"},{"start":"1684.605","dur":"4.01","text":"So as you saw the indicator notation on Wednesday, did you?","_id":"41f1c99a5cbf4310bf9b4ce0a509cc3b"},{"start":"1674.11","dur":"10.495","text":"Okay. Right. Um, let's see.","_id":"2d27dd15706d4e1393ffc0e0ab9465d4"},{"start":"1665.105","dur":"6.235","text":"um, sum from i equals 1 through m indicator.","_id":"f3bd906dcf5a48e79f29ac94daa28f6a"},{"start":"1662.12","dur":"2.985","text":"um, and one other way to write this is,","_id":"e107ccb3936540aa852044fda2d90f6a"},{"start":"1660.575","dur":"1.545","text":"okay? So this, this is it.","_id":"54643e712d1c4e5490d959cdadae6b3f"},{"start":"1656.465","dur":"4.11","text":"bias of a coin toss is just, well, count up the fraction of heads you got,","_id":"f15ea989c81f45f9b514610267e05c3a"},{"start":"1654.215","dur":"2.25","text":"So it's the, the maximum likelihood of the, uh,","_id":"3d4ade69071a40c8845d93d2f0f7a1a6"},{"start":"1651.32","dur":"2.895","text":"what's the fraction with label y equals 1, right.","_id":"68f3ddf488b249b8a1663b1f03c30625"},{"start":"1649.085","dur":"2.235","text":"it's just of all of your training examples,","_id":"3f87f665efa547b292f38b1772101b2d"},{"start":"1645.755","dur":"3.33","text":"And so the maximum likelihood estimate for phi is, um,","_id":"bb1df9a756dc4a209c57b6598a9b12e9"},{"start":"1642.2","dur":"3.555","text":"doctor's office that they have a, a malignant tumor?","_id":"37ce6c636cfd4ab68ddc22cdd976511b"},{"start":"1638.99","dur":"3.21","text":"So what's the chance when the next patient walks into your, uh,","_id":"4def5af2b8c748c69f65791cddef160d"},{"start":"1633.515","dur":"5.475","text":"So, so phi is the estimate of probability of y being equal to 1, right?","_id":"7c37c9672d0a4b618e540ae8bcce6e52"},{"start":"1631.1","dur":"2.415","text":"you know, not that surprisingly.","_id":"6e2e09b2b7ff454a950a6c6784694eb9"},{"start":"1625.15","dur":"5.95","text":"Right. um, the value of phi that maximizes this is,","_id":"f75468fe4e69463ab5c2fac6b1b42494"},{"start":"1620","dur":"2.95","text":"But you still have to do the derivation.","_id":"f1cb251458db46ad8888b954e7752488"},{"start":"1619.1","dur":"0.9","text":"[LAUGHTER].","_id":"31d57bac8e074ebe8e8c78ac731c4480"},{"start":"1616.115","dur":"2.985","text":"And I'll, I'll, I'll just tell you the answers you are supposed to get.","_id":"2dab7f63d386419080928934289ce3f6"},{"start":"1613.55","dur":"2.565","text":"the values of the parameters that maximize this whole thing.","_id":"b7d192a361d54b109a9694d40b4e8152"},{"start":"1611.42","dur":"2.13","text":"set the derivative equal to 0 and then solve for","_id":"d7c02bf2db574ca4b7c340969d7dd229"},{"start":"1609.005","dur":"2.415","text":"take logs, take derivatives of this thing,","_id":"7122d2cc640542f7a73a1eb8fd79c3dd"},{"start":"1605.585","dur":"3.42","text":"um, look at that formula for the likelihood,","_id":"8a45d17147ae46cf9fd79238c629a70a"},{"start":"1603.41","dur":"2.175","text":"But so the way you maximize this is,","_id":"9b1ddcd6687649f68ab1dccdfdadba94"},{"start":"1599.975","dur":"3.435","text":"we actually ask you to do this as a problem set in the next homework.","_id":"fa78061ef688495096376ffa4ecfae77"},{"start":"1598.04","dur":"1.935","text":"Um, and so, uh, th- we,","_id":"1bf49a90d2724a77b86328c845a7bb0a"},{"start":"1593.945","dur":"4.095","text":"log of the likelihood that we defined up there.","_id":"9492479ac4004495abacdeacc0da061a"},{"start":"1591.65","dur":"2.295","text":"Where this you define as, you know,","_id":"c007808926934343861e44c38953aaf6"},{"start":"1585.29","dur":"6.36","text":"and sigma they maximize the log likelihood, right.","_id":"9f375d4db2e348d991cea2444b4ac268"},{"start":"1578.65","dur":"6.64","text":"Um, so you choose the parameters phi, mu0, mu1,","_id":"b34843b5ec0d4d56ab3d0bda5eef3536"},{"start":"1565.565","dur":"3.205","text":"um, maximum likelihood estimation.","_id":"a569748d97bd4a4e821dd03e98e836e1"},{"start":"1563.08","dur":"2.485","text":"So if you use,","_id":"00dec3f461ef46a1ac71bbe246f9b3b5"},{"start":"1536.68","dur":"8.48","text":"So all right.","_id":"d791d449d4d144eba4e574a60eee6c41"},{"start":"1532.445","dur":"1.975","text":"Okay?","_id":"bf31f1a4f20941da95d0364ae2ac6ec6"},{"start":"1526.43","dur":"6.015","text":"we're gonna try to choose parameters that maximize p of x and y or p of x, y, right.","_id":"6e9910740e5c49f1988e9ffff07d841d"},{"start":"1524.48","dur":"1.95","text":"But for generative learning algorithms,","_id":"25399e672bbf4441bd491d9c11b6a626"},{"start":"1521.06","dur":"3.42","text":"that maximize p of y given x.","_id":"6450dd4b901e4bdabf1fafe96ca50f61"},{"start":"1518.99","dur":"2.07","text":"you were trying to choose parameters theta,","_id":"a103a00aedd145dcaa53c1378a720337"},{"start":"1514.34","dur":"4.65","text":"is that for logistic regression or linear regression and generalized linear models,","_id":"fa37928f12484d758f6babade320ee96"},{"start":"1510.575","dur":"3.765","text":"So the big difference between the- these two cost functions,","_id":"60015f16e6454841afc5038995da26a7"},{"start":"1500.16","dur":"10.415","text":"Uh, which is sometimes also called the conditional likelihood, okay?","_id":"58c2aa225dca43b0894595a27d19e097"},{"start":"1489.605","dur":"4.186","text":"this other thing, right.","_id":"165c45bcd40a49b2b35710f96541f1bf"},{"start":"1486.79","dur":"2.815","text":"we were maximizing, um,","_id":"3113be09496d494ea372e67afffb9f16"},{"start":"1481.76","dur":"3.73","text":"Whereas for a discriminative learning algorithm,","_id":"05d1ed06d5ff40a8b8212b26e03882cc"},{"start":"1471.994","dur":"9.766","text":"is that the cost function you maximize is this joint likelihood which is p of x, y.","_id":"77008bc4891d4038b9e5161ad2dcc97a"},{"start":"1469.415","dur":"2.579","text":"compared to a discriminative learning algorithm,","_id":"621164ccda4a46158c1004468367655b"},{"start":"1467.135","dur":"2.28","text":"a generative learning algorithm like this,","_id":"4aa5152461364923b91fb8eb520fc4c6"},{"start":"1462.41","dur":"4.725","text":"And the big difference between, um,","_id":"9ff3882bae414ccd8429b4a586183f95"},{"start":"1459.23","dur":"3.18","text":"To simplify the notation a little bit, okay?","_id":"1cb3bf1d3c6348e396a483838ceea844"},{"start":"1453.79","dur":"5.44","text":"Um, and I'm, I'm just like dropped the parameters here, right?","_id":"1883667adb2f4daca53bc97f809b0e7f"},{"start":"1435.245","dur":"10.465","text":"parameterized by the, um, the parameters, okay?","_id":"69845ca4fd0e49ccb29495494060a2b4"},{"start":"1430.49","dur":"4.755","text":"up here, xi, yi, you know,","_id":"a9a1bd8ea68a40db961bc994a1f2319a"},{"start":"1426.86","dur":"3.63","text":"equal to the product from i equals 1 through m,","_id":"a2fc1d909bb24bbc8cc1632a3cef2e60"},{"start":"1418.445","dur":"8.415","text":"the parameters to be","_id":"94b03fbbe5414767be5cd4d0c4ca646b"},{"start":"1415.835","dur":"2.61","text":"let me define the likelihood of","_id":"67bbdd5506c84598a722a2516e33e532"},{"start":"1412.6","dur":"3.235","text":"And in particular, um,","_id":"05c8c55983db46dda6a2f29b29c87243"},{"start":"1404.075","dur":"5.935","text":"in order to fit these parameters is maximize the joint likelihood.","_id":"a2ce3043a22945f1b0be65a8a664e54b"},{"start":"1399.23","dur":"4.845","text":"Um, and what we're going to do,","_id":"da5e93c84ce64f47973ea760d8881ec8"},{"start":"1397.43","dur":"1.8","text":"This is a usual training set.","_id":"0aabe8064382455a9c0b1d9deba4ecc1"},{"start":"1394.805","dur":"2.625","text":"yi, for i equals 1 through m, right?","_id":"fb27cbab55d043f7b5d6ca248c18cf71"},{"start":"1391.85","dur":"2.955","text":"I'm go- let me write the training set like this xi,","_id":"9bc05f95e79c4b2fb77fcb83db79091d"},{"start":"1390.5","dur":"1.35","text":"I'm gonna write the tre- well,","_id":"8adbea7bfda3464492e7513d739b760a"},{"start":"1385.46","dur":"5.04","text":"So you have a training set, um, as usual,","_id":"0df1e896b4de4d2ca8e973d6938e8d1b"},{"start":"1381.32","dur":"4.14","text":"Right. So let's talk about how to fit the parameters.","_id":"6917f38876d349d7aae04a4a58776a9f"},{"start":"1378.575","dur":"2.745","text":"you know, malignant or benign tumor.","_id":"d02e8c540f9e49d09c55d8a6cf2cc5f5"},{"start":"1374.81","dur":"3.765","text":"you get a number alpha P of y equals 1 given x and you can then predict,","_id":"8ce739f827004fe595da7fb65077b353"},{"start":"1372.95","dur":"1.86","text":"and by plugging all these numbers in the formula,","_id":"2b7052fd10f84229876ac94c035f3b6f"},{"start":"1371.78","dur":"1.17","text":"Each of these is a number,","_id":"b22cb3d654b84af5a6b194acd540ca3e"},{"start":"1368.735","dur":"3.045","text":"right, these things in the red and the orange boxes.","_id":"0e50aca99ee74a35bc59b5d0f05e1cb4"},{"start":"1367.43","dur":"1.305","text":"then you can compute,","_id":"59f6d3ca3ac6490399d497f78b333d4e"},{"start":"1365.825","dur":"1.605","text":"and you need to compute this,","_id":"35620261ce504ae89d4efd6a1f5a2cf8"},{"start":"1360.965","dur":"4.86","text":"And so, if at test time you have a new patient walk into your office,","_id":"dd12cfdfb93c412d802c8d6605246c35"},{"start":"1354.53","dur":"6.435","text":"then these parameters will define P of x given y and P of y.","_id":"41b239189a66409d99397600f2a9668a"},{"start":"1351.125","dur":"3.405","text":"mu1, sigma, and phi to your data,","_id":"600167a52b5841728c6dba2276b4fee4"},{"start":"1349.73","dur":"1.395","text":"So if you can fit mu0,","_id":"d2a463158e0b476d96a1e7f2ffacede8"},{"start":"1345.22","dur":"4.51","text":"So, um, for any- let's see.","_id":"629da24b1678401ab6832dae066c86d5"},{"start":"1326.14","dur":"8.71","text":"this is Rn by n and that's just a real number between 0 and 1, okay?","_id":"61bdaf96690b4d8791e6c801e5ebccb7"},{"start":"1324.4","dur":"1.74","text":"this is also Rn,","_id":"92005437c2a948ed896f01a9322b9248"},{"start":"1321.08","dur":"3.32","text":"So this is Rn,","_id":"202c2b0cd4414608893b4462a0135715"},{"start":"1317.465","dur":"3.615","text":"And so, the last parameter is phi.","_id":"270f13a2ad8a42369de65b78c5fc501f"},{"start":"1315.5","dur":"1.965","text":"one week ago, last Monday.","_id":"416d527b39634aea9a7eae70de85e39c"},{"start":"1313.28","dur":"2.22","text":"um, logistic regression, right,","_id":"66f215d6c8bb42a2bf9c188da6525b66"},{"start":"1310.94","dur":"2.34","text":"it's a notation when we're talking about,","_id":"20f00e5785ed437fb671c578dcaf2ff0"},{"start":"1309.005","dur":"1.935","text":"And, uh, you saw a similar explanation,","_id":"1f5920292cb24a10acfbbd53d3df5ebb"},{"start":"1305.15","dur":"3.855","text":"uh, uh, probability of y equals 1 is equal to phi, okay?","_id":"8f60ff5ccaaf4c8fb0e7e742a774cde2"},{"start":"1302.165","dur":"2.985","text":"And so, um, this is the way of writing,","_id":"5069165910ea47de93b8f44d89fd3515"},{"start":"1300.575","dur":"1.59","text":"Because y is either 0 or 1.","_id":"603e61d891c94f068a345fcef941179e"},{"start":"1295.355","dur":"5.22","text":"probability of y being equal to 1 is equal to phi, right.","_id":"ba938982f9774b61bb28529acdd80561"},{"start":"1292.655","dur":"2.7","text":"but all this means is that, um, you know,","_id":"d56dc5255b8345808d7f9bddc7d5f121"},{"start":"1286.94","dur":"5.715","text":"Um, and you saw this kind of notation when we talked about logistic regression,","_id":"520cafb014cf47cca23003f0c5214d43"},{"start":"1278.275","dur":"8.665","text":"phi to the y times 1 minus phi to the 1 minus y, okay?","_id":"1d78be7344824ac6a1a46adcfc8e1fa2"},{"start":"1275.335","dur":"2.94","text":"And so, I'm going to write it like this,","_id":"d137adf4b8d445cab38e9945ec7c455d"},{"start":"1272.74","dur":"2.595","text":"It takes on, you know, the values 0 or 1.","_id":"527fec567eff4a318e292b0fc0f4d975"},{"start":"1269.43","dur":"3.31","text":"Uh, so y is just a Bernoulli random variable, right.","_id":"1a6486f1311b4e03b12a1a95d360ae8c"},{"start":"1265.04","dur":"4.39","text":"The other thing we need to do is model P of y.","_id":"9619bab63eb94aaba23a9f18407ed591"},{"start":"1260.4","dur":"4.64","text":"Um, so this is a model for P of y given x.","_id":"341cb409b5344ec994567f36f24c9964"},{"start":"1256.49","dur":"3.91","text":"And then we can talk about the reason why we tend to do that in a second.","_id":"89d9890573a0413a956a4dc227a77d93"},{"start":"1254.48","dur":"2.01","text":"but this is the way it's most commonly done.","_id":"583ab16221b542c1bf889c01e65bee1f"},{"start":"1252.5","dur":"1.98","text":"Uh, you don't have to make this assumption,","_id":"7162adff835942c19292b02ac4f77841"},{"start":"1251.32","dur":"1.18","text":"they have different means.","_id":"5a10e2905342487aa4055f08b971ba91"},{"start":"1249.53","dur":"1.79","text":"have the same covariance matrix but they,","_id":"ed594c52384440e59ac961014e69abca"},{"start":"1248.18","dur":"1.35","text":"for the positive and negative classes,","_id":"fad1866e15504bc7a317e13e4954c293"},{"start":"1245.87","dur":"2.31","text":"So we're going to assume that the two Gaussians,","_id":"972151b71f6844238adab19eef33e41a"},{"start":"1244.28","dur":"1.59","text":"but that's not usually done.","_id":"b1d7bd2bc12746a880b289e0cb189e2a"},{"start":"1242.69","dur":"1.59","text":"sigma 0 and sigma 1,","_id":"9fe575b285304372b963d23d11493070"},{"start":"1239.795","dur":"2.895","text":"If you want, you could use separate parameters, you know,","_id":"04905b97240e4f8293b23a7d75d7c564"},{"start":"1237.32","dur":"2.475","text":"Uh, and we can come back to this later.","_id":"27b9039752684a0fbad3832a37a9b878"},{"start":"1231.94","dur":"5.38","text":"Um, but we use different means, 0 and 1, okay?","_id":"121ea241c3e0446fa6c9c97a1c79aeec"},{"start":"1227.165","dur":"4.675","text":"we'll use the same sigma for both class.","_id":"92e6b104d18c49f499023658ecf168e3"},{"start":"1226.04","dur":"1.125","text":"we'll go into a little bit,","_id":"7393f87a289b4820b65fe148236ca7f1"},{"start":"1224.63","dur":"1.41","text":"Um, and for reasons,","_id":"f2009dbe2af542ada2cf5d2e0960413f"},{"start":"1218.6","dur":"6.03","text":"mu0, mu1, and sigma.","_id":"210258e4fabf45448b686a92e89188ec"},{"start":"1213.26","dur":"5.34","text":"so the parameters of the GDA model are","_id":"c96a22667b964a1da466b622e9c7ba80"},{"start":"1211.19","dur":"2.07","text":"And, um, I wanna point out a couple of things,","_id":"80bfca8aa09245a78773157e6f5bc8c6"},{"start":"1201.995","dur":"9.195","text":"that the density of the features is also Gaussian, okay?","_id":"07b1c8feab56458c8cf40b005ab483c4"},{"start":"1198.065","dur":"3.93","text":"that if is a malignant tumor as if y is equal to 1,","_id":"988fa7f0d370402fb1b57fa327515782"},{"start":"1195.215","dur":"2.85","text":"And then similarly, I'm going to assume","_id":"e500d1faf0dc4d389b6d3021877f0aa4"},{"start":"1170.33","dur":"24.885","text":"[NOISE]","_id":"96acbc4103814db1939e99edddea26bf"},{"start":"1167.51","dur":"2.82","text":"So I'm just going to write down the formula for Gaussian.","_id":"7cdb42874b1141299ae52fc70899d2a7"},{"start":"1164.225","dur":"3.285","text":"Um, I'm going to assume it's Gaussian.","_id":"07969e92d2ce494d9b8aa6c3876a9359"},{"start":"1159.83","dur":"4.395","text":"probability density of the features if it is a benign tumor?","_id":"3a57ac99166d4497b5c67116683dfe0f"},{"start":"1158.075","dur":"1.755","text":"So what's the chance- what's the, uh,","_id":"ef239bbe26a74d5e80930dc2799187e8"},{"start":"1153.02","dur":"5.055","text":"So I'm gonna write this separately in two separate equations P of x given y equals 0.","_id":"ef0deb4013574492800e324bca464fbc"},{"start":"1151.76","dur":"1.26","text":"right? It's up here, y given x.","_id":"c453966b7fbb449aaacd12d50ed1ae2d"},{"start":"1149.09","dur":"2.67","text":"we need to model P of x given y,","_id":"702151ff3f684415911030eb8395fa84"},{"start":"1142.85","dur":"6.24","text":"So, um, remember for GDA,","_id":"749ce1bd65e14e5c9fc8f7aa1e1300d6"},{"start":"1136.805","dur":"6.045","text":"Um, and- and, uh, let's see.","_id":"4e70864a87894ebd92aa5691ee6709e8"},{"start":"1129.07","dur":"7.735","text":"Here is a GDA, right, model.","_id":"6310d6ac52484462ba70143ed0cd4cb4"},{"start":"1092.65","dur":"5","text":"Raise the screen. [NOISE] All right, cool.","_id":"fdd6d5c2a759492bb7a79bde8e7eab85"},{"start":"1086.21","dur":"4.21","text":"Okay? Um, any other questions about this?","_id":"75aaa5841a6a42e1a8241770feae3288"},{"start":"1083.15","dur":"3.06","text":"a result of changing Mu and Sigma.","_id":"98a017b67ec64892811d59e69b935f52"},{"start":"1080.69","dur":"2.46","text":"those are probably- probably density functions you can get as","_id":"df94ba94b2734cda9e829415146180fa"},{"start":"1075.619","dur":"5.071","text":"the mean and the covariance matrix of the 2D Gaussian density, um,","_id":"7a22d550d34046178d3f5bebd86f01b3"},{"start":"1073.73","dur":"1.889","text":"um, as you vary the parameters,","_id":"c56c56229a844b33843ae71a7466aad9"},{"start":"1070.28","dur":"3.45","text":"Okay? So I hope this gives you a sense of,","_id":"c841ed809d7b45bfa6498232b1fead1f"},{"start":"1066.725","dur":"3.555","text":"you could also shift the center of the Gaussian density around.","_id":"dbfe41a64fd042dd927ef4ff705d60e2"},{"start":"1064.535","dur":"2.19","text":"And so by varying the value of Mu,","_id":"abcf8b71ceb849b686cbbe432d968a44"},{"start":"1061.67","dur":"2.865","text":"Move it to minus 1.5, minus 1.","_id":"628c7eb51f124575ba97dd5accd20ac9"},{"start":"1059.345","dur":"2.325","text":"Now let's move it to a different location.","_id":"9d0b6e2f2c5c41939ad3a10a26385ed2"},{"start":"1057.095","dur":"2.25","text":"the position of the Gaussian density right.","_id":"8f373ded17024ed485ddbcf733db7693"},{"start":"1055.175","dur":"1.92","text":"So that moves the Gaussian, uh,","_id":"64440f3e51c74430831dd439d3f1009a"},{"start":"1053.27","dur":"1.905","text":"1.5.","_id":"d68dbeb72fd947bb92f47f46068cc9c8"},{"start":"1051.11","dur":"2.16","text":"So I'm going to move, you know, Mu to 0,","_id":"5494166689ec4f4d8da90b1cc0f32b69"},{"start":"1049.655","dur":"1.455","text":"Uh, let's move Mu around.","_id":"f063c2955e4a4e1a987f79579aca9577"},{"start":"1047.36","dur":"2.295","text":"0 because mu is 0, 0.","_id":"d221e466fee347deaefb6e35bdcf773f"},{"start":"1045.17","dur":"2.19","text":"So the Gaussian bump is centered at 0,","_id":"3fc03e9e1af944a685870bfaab239b2d"},{"start":"1038.09","dur":"7.08","text":"Yeah. Cool. Okay. Um, so this standard Gaussian would mean 0.","_id":"3758cf9e7a434bc18b2b84bcd3ce3686"},{"start":"1036.65","dur":"1.44","text":"That's defined by the contents.","_id":"93f23d5ecdea425ca1f78b892b8767fb"},{"start":"1034.64","dur":"2.01","text":"points in the principal axes of the ellipse.","_id":"f16046fb61a24d6e95383ad9c6f6b6c6"},{"start":"1033.34","dur":"1.3","text":"So the Eigenvectors are a covariance matrix,","_id":"757aca0f291444788a0c3e7b4e4995d8"},{"start":"1032.92","dur":"0.42","text":"Uh, yeah.","_id":"d100a01b6e594ff1937714fabe4ce422"},{"start":"1028.56","dur":"4.36","text":"[inaudible]","_id":"f9c99cba70cb42efa83bd57ec0aaa9a2"},{"start":"1026.27","dur":"2.29","text":"uh, yeah we- we- we- we'll get to that later.","_id":"39d00439bba94741bdb953eeb4b9cb35"},{"start":"1023.9","dur":"2.37","text":"which are the principle directions in which it points but,","_id":"3e921ce661d04810a881f2a6b9907499"},{"start":"1021.35","dur":"2.55","text":"we talk about the Eigenvectors of the covariance matrix,","_id":"d16ba96d553c4c799750e3feaeea2f4c"},{"start":"1019.07","dur":"2.28","text":"Uh, when we talk about Principal components analysis,","_id":"a52724c5923442158981d6b3e12f52e3"},{"start":"1012.68","dur":"6.39","text":"And so I would usually not look at single columns of the covariance matrix in isolation.","_id":"bf87a316992b42aaa682eeb6ee0acd31"},{"start":"1008.93","dur":"3.75","text":"no I- I- I think the covariance matrix is always symmetric.","_id":"9aa6cb5950ac43b6a5afecc31d9cf63c"},{"start":"1005.345","dur":"3.585","text":"Maybe you should- yeah- yeah- uh,","_id":"8be129021e7c4469b08a1bdaedf7ad4e"},{"start":"1002.585","dur":"2.76","text":"Um, let me think.","_id":"bb10dd8ab56848f9bff457476512a8e6"},{"start":"1000.95","dur":"1.635","text":"Not really.","_id":"cd8f55c460714ffd956adbac58031ab6"},{"start":"999.66","dur":"1.29","text":"that point in interesting directions.","_id":"bcb002ce59624cd58f4af07015759777"},{"start":"997.23","dur":"2.43","text":"the covariance matrix has interesting column vectors,","_id":"b925efff2ff244b295307b6d646819cd"},{"start":"987.57","dur":"9.66","text":"Uh, the true thing about","_id":"309bb9434cbf468aa8b18e5374cd6ed0"},{"start":"986.64","dur":"0.93","text":"[inaudible]","_id":"dcb4349616d74bad942e2b364cdc9d5e"},{"start":"984.36","dur":"2.28","text":"Uh, yes. Every covariance matrix is symmetric. Yeah.","_id":"40ff51056dcb4dd88e8fc16330988093"},{"start":"980.91","dur":"3.45","text":"[inaudible].","_id":"b0cfbd703c4941f895818d758b2598a1"},{"start":"979.665","dur":"1.245","text":"Um, oh good. Yeah?","_id":"941a829fdf8d48f79c941f4511513c50"},{"start":"976.47","dur":"3.195","text":"0 and just varying the covariance matrix.","_id":"7601ea667e614168b480f5abfc320d2a"},{"start":"973.53","dur":"2.94","text":"So- so far we've been keeping the mean vector as","_id":"c43963e6931c44f989c4ca6eeae38740"},{"start":"972.08","dur":"1.45","text":"Okay? All right.","_id":"5bbfe007a682492797a9340bcd37e5dc"},{"start":"970.49","dur":"1.59","text":"And that's 0.8, 0.8.","_id":"896fe5cbbf494054bed28e794078abe0"},{"start":"967.79","dur":"2.7","text":"So now z1 and z2 have a negative correlation.","_id":"ba2c2c4cba28496e8bbaf80000df3bdf"},{"start":"965.03","dur":"2.76","text":"Okay? Whe- whereas now slanted the other way.","_id":"fdc1745a142642f8a755768086edd39d"},{"start":"962.15","dur":"2.88","text":"Uh, and the contours, it looks like this.","_id":"bb2b27087ddc460ab0cb68056afea22f"},{"start":"958.545","dur":"3.605","text":"this type of probability density function, right?","_id":"13a2cc7e44374735bedf732084c0d214"},{"start":"956.729","dur":"1.816","text":"so you end up with, um,","_id":"27b258db502a4c8ca686c65821a3f4a1"},{"start":"952.89","dur":"3.839","text":"So- so- so as you- you endow the two random variables with negative correlation,","_id":"cd0a9fbaac2c4aadba890b9d417bcd54"},{"start":"951.435","dur":"1.455","text":"Right. [LAUGHTER] Right.","_id":"7ad5da0459a643a0b7700d9aa108a970"},{"start":"949.545","dur":"1.89","text":"fewer making that hand gesture. Okay, cool.","_id":"6d050d6cfe02423eb4153db709e3a880"},{"start":"947.24","dur":"2.305","text":"Right. Oh well. People are seeing,","_id":"ecea8b0442ef4c48a7b7d59901ddcebc"},{"start":"942.33","dur":"3.97","text":"Let's set the off-diagonals to negative 0.5, 0.5.","_id":"0bb6584a42a14ae4ac2c036bb182d079"},{"start":"939.69","dur":"2.64","text":"So, um, actually what do you think will happen?","_id":"96ef17b9cff842b587ba3fe050dac870"},{"start":"934.035","dur":"5.655","text":"what happens if we set the off-diagonal elements to negative values, right?","_id":"b06a63af2f424ae480619cab3ccde532"},{"start":"931.53","dur":"2.505","text":"Um, next, let's look at, uh,","_id":"3c27783e00e946fca9b53947802018ce"},{"start":"927","dur":"4.53","text":"um, z1 and z2 being positively correlated.","_id":"a985411856b24ad4b71c5e0b1b88e730"},{"start":"922.95","dur":"4.05","text":"the probability mass- probability ma- most probably density function places value on,","_id":"07862bf1c4b04639838e75aab8d73c71"},{"start":"921.27","dur":"1.68","text":"Uh, where now, most of","_id":"fa83c5385cf543d3bd49e42c71f2a609"},{"start":"918.69","dur":"2.58","text":"0.8, it looks like that, okay?","_id":"47c2579582344512a085d393165ee84d"},{"start":"917.205","dur":"1.485","text":"If you increase it further to 0.8,","_id":"ace616ecde7d46fb848b3e6fb5b7d9a7"},{"start":"914.655","dur":"2.55","text":"excuse me, then it looks like that.","_id":"50ebb24e01f244d4b4f6e164cd91ca4a"},{"start":"912.825","dur":"1.83","text":"And if you increase the off-diagonal,","_id":"c27f3f3d1ae3470289bd90766454a502"},{"start":"910.335","dur":"2.49","text":"of the Gaussian density look like brown circles.","_id":"17fde2dcceb84f3a8825ef192c6ea8cd"},{"start":"907.335","dur":"3","text":"Um, uh, and the contours of the Gaussian bump,","_id":"7970fc1abfdc440ab0d27ec6f3804190"},{"start":"904.425","dur":"2.91","text":"you know, z1 and z2 are uncorrelated.","_id":"80006b13901449f3bf2db594964b5ca2"},{"start":"902.16","dur":"2.265","text":"the covariance matrix is the identity matrix,","_id":"d182a1c468be4d10964f38fbebae1452"},{"start":"899.115","dur":"3.045","text":"Um, and so, uh, when, uh,","_id":"d81a09bc90c14f2eada166fa477d51e2"},{"start":"896.43","dur":"2.685","text":"but this is supposed to be perfectly round circles.","_id":"1166ff633647499ca60e27c2fa086f0c"},{"start":"893.85","dur":"2.58","text":"but the aspect ratio makes this look a little bit fatter,","_id":"59965c36f2084424a100627e00b2cb50"},{"start":"891.765","dur":"2.085","text":"These are supposed to be perfectly round circles","_id":"e98e4d92f7674545b76e1b08463dda13"},{"start":"888.675","dur":"3.09","text":"is the identity matrix and I apologize the aspect ratio.","_id":"6ef0332b78ca4501b3f58707c5a2cef9"},{"start":"885.96","dur":"2.715","text":"the Gaussian density when the covariance matrix","_id":"f93a2b4aea134ccf8a782509cf322abc"},{"start":"882.12","dur":"3.84","text":"So uh, this is the contours of","_id":"08d36e27e9084c7db022553160eae258"},{"start":"877.95","dur":"4.17","text":"But now looking at contours of these Gaussian densities instead of these 3-D bumps.","_id":"39bf16b5fd6c43b58fe52c8ae63b2ee1"},{"start":"875.1","dur":"2.85","text":"Okay? So let's go through all of these plots.","_id":"23f49a05cbfc4ea291a8bddbf1c23aa8"},{"start":"868.98","dur":"6.12","text":"where now, it's more likely that z1 now- z1 and z2 are positively correlated.","_id":"74cbb5646a4442ac9f12931b0bc4ba10"},{"start":"865.32","dur":"3.66","text":"Then the density ends up looking like that, um,","_id":"d83b56d454be4c5b87d810e86b5cc32c"},{"start":"863.055","dur":"2.265","text":"Let's increase that further to 0.8, 0.8.","_id":"d3be00b9423841fe99e86a46ba796721"},{"start":"859.76","dur":"3.295","text":"It goes from this round shape to this slightly narrower thing.","_id":"0ff0c08ef5984b27b1af7800a901dbee"},{"start":"858.11","dur":"1.65","text":"uh, hope you can see see the change, right?","_id":"b4a44b5eda9040ad8face5383088fd6a"},{"start":"856.025","dur":"2.085","text":"then the Gaussian density,","_id":"66858e4ca00743d48bc2e38a87b74d8e"},{"start":"854.78","dur":"1.245","text":"So if you do that,","_id":"4ec7aa6623e64924b1040368bbcdba01"},{"start":"851.78","dur":"3","text":"Let's increase that to 0.5 and see what happens.","_id":"efdb1b9a05704099bd205e2a9dfd51c2"},{"start":"849.315","dur":"2.465","text":"the off-diagonal elements are 0, 0.","_id":"2aed78e0f86b4a47953e0b728b192fe1"},{"start":"847.77","dur":"1.545","text":"So in this Gaussian density,","_id":"0dd2acedea1740fc86a720a9fdf6526d"},{"start":"845.04","dur":"2.73","text":"the off diagonal entries are 0, right?","_id":"3dee683d71194c18bfed02a5e09474fa"},{"start":"843.39","dur":"1.65","text":"Um, I'm gonna- So right now,","_id":"54bb055b68d54743b87a91b373713a7d"},{"start":"839.565","dur":"3.825","text":"Now, let's try fooling around with the off-diagonal entries.","_id":"6cc6f7733c6b40ffa995d6f836ad9037"},{"start":"837.765","dur":"1.8","text":"uh, covariance equal 1, 1.","_id":"6e3aacf087cc49e4ac0abfe8e9890107"},{"start":"835.395","dur":"2.37","text":"So let's go back to a standard Gaussian,","_id":"6bcc1bd82b0a41cc8966430ee1c87195"},{"start":"833.07","dur":"2.325","text":"Increases the variance of the density.","_id":"ccba06a4756d4e29b981c336f762e1a2"},{"start":"831.06","dur":"2.01","text":"the two-dimensional Gaussian density, right?","_id":"cff08ff3357f4405b028475fd3512801"},{"start":"826.395","dur":"4.665","text":"um- I guess the axes here, this would be the z1 and the z2 axis;","_id":"b3376ea14f984120be5ce30d5267767d"},{"start":"822.135","dur":"4.26","text":"Then you end up with a wider distribution where the values of","_id":"386b1272a466464eb91e0de611e51229"},{"start":"819.135","dur":"3","text":"Let's make the covariance two times the identity.","_id":"2623b5cdbfbf4985bfb23f6a495d70dc"},{"start":"816.69","dur":"2.445","text":"Now let's make it fatter.","_id":"480f3e22cfed4f7bb6767d150bdd0162"},{"start":"813.975","dur":"2.715","text":"you know, the area under the curve must integrate to 1.","_id":"54577af795504781a1f809da2f84638f"},{"start":"811.83","dur":"2.145","text":"but it also makes it tall as a result, because,","_id":"cd28519589f84399a15ee915ed2f72c8"},{"start":"808.275","dur":"3.555","text":"it reduces the spread of the Gaussian density, um,","_id":"cd8639348551450ba7e3fd949310b62e"},{"start":"802.995","dur":"5.28","text":"And so by reducing the covariance from the identity to 0.6 times the identity,","_id":"694a82cd82684f1db9e0ac22f7d333c6"},{"start":"801.51","dur":"1.485","text":"you know, is- is 1.","_id":"f421dbf3e6f242d7a4ed7aea2cf36f40"},{"start":"800.37","dur":"1.14","text":"The area under the curve,","_id":"73ead06521ad4d4d845a48a1bf056748"},{"start":"798.09","dur":"2.28","text":"So it always integrates to 1, right?","_id":"747b716059f5473a945405dc5ed686ef"},{"start":"795.96","dur":"2.13","text":"Uh, this- this is a probability density function.","_id":"5a08f75db2814da2aafee62db0b1fab7"},{"start":"792.39","dur":"3.57","text":"the p- probability density function becomes taller.","_id":"e0fffd839b86464c8fb8f9b72c38cb61"},{"start":"789.03","dur":"3.36","text":"If I do that, the density um,","_id":"74da491059274887958dbf391a3be5df"},{"start":"784.905","dur":"4.125","text":"That should shrink the variance- reduce the variability of distributions.","_id":"5a69471443204306bfe60ecfc2bbec77"},{"start":"781.86","dur":"3.045","text":"So take a covariance matrix and multiply it by a number less than 1.","_id":"c247b65951db48ed834638119a2b03a1"},{"start":"778.29","dur":"3.57","text":"Now, I'm gonna take the covariance matrix and shrink it, right?","_id":"c9c3fb7bf53346f1afa24ac824d1f940"},{"start":"774.33","dur":"3.96","text":"distribution which means 0 and covariance equals to the identity.","_id":"1ca634a93b554b2c97fbe33d45e4f632"},{"start":"770.82","dur":"3.51","text":"so- so you've have this standard- this is also called the standard Gaussian","_id":"1894ad9537e743f3b20b078962228bf7"},{"start":"768.525","dur":"2.295","text":"So uh, so you know, well,","_id":"4e8d9037e71348cda7c3d57e6490b057"},{"start":"765.325","dur":"3.2","text":"um, i- i- i - is the identity matrix.","_id":"467a2cb2638f4d3186283349976199af"},{"start":"758.58","dur":"6.745","text":"Um, and the Co-variance matrix Sigma is the identity,","_id":"9d994b3b46394a529bdff9203db693dd"},{"start":"753.24","dur":"5.34","text":"which is why this Gaussian bump is centered at 0.","_id":"517a932b1528467894a08904dc480942"},{"start":"752.19","dur":"1.05","text":"it's uh, it's 0, 0,","_id":"906eb635598b40b5b87a736159b06ee9"},{"start":"750.21","dur":"1.98","text":"So Mu is a two dimensional parameter,","_id":"bfaa6b3bfdb24fc186f09f6e9b8ec800"},{"start":"746.115","dur":"4.095","text":"And for now, I've set the mean parameter to 0.","_id":"9ebf8deb2e9c4e858d0d47bda16b55dd"},{"start":"743.28","dur":"2.835","text":"Um, this is a two-dimensional Gaussian bump.","_id":"9489dbc302ea405fb6dfa7d3bf25089d"},{"start":"738.78","dur":"4.5","text":"Okay? So this is a picture of the Gaussian density.","_id":"7b4d6b2b46d4460f833e7980e8956483"},{"start":"733.395","dur":"5.385","text":"They control the mean and the variance of this density.","_id":"dd1b1109c30949a29070929d2a868d67"},{"start":"728.565","dur":"4.83","text":"So the Multivariate Gaussian density has two parameters; Mu and Sigma.","_id":"545aa9c6f7084376ae44100ef42eb577"},{"start":"724.77","dur":"3.795","text":"um, that might be more useful.","_id":"9825b609cede4cae9310516498b23204"},{"start":"720.705","dur":"4.065","text":"But let me show you some pictures of what this looks like since I think that would,","_id":"59a2e55aa35d41a38fb7554f2fe213be"},{"start":"718.62","dur":"2.085","text":"you- you- you end up memorizing it.","_id":"f79fc010e70347f8b5338f21e4e4d2d5"},{"start":"715.8","dur":"2.82","text":"but most people don't even- when you've used it enough,","_id":"ecacc34520e34fd8a2c766a64e4b966e"},{"start":"712.5","dur":"3.3","text":"I've used it so many times I seem to have it seared in my brain by now,","_id":"3a0d4a77e2884fe6ae5dbb1405c9d60c"},{"start":"710.91","dur":"1.59","text":"Just look at it every time you need it.","_id":"dcd352f1471b42aaa3912b6071cf64c2"},{"start":"708.27","dur":"2.64","text":"very few people start their machine learning and memorize this formula.","_id":"476fa9555b3f49ce9d75b54c0fddf7fe"},{"start":"704.145","dur":"4.125","text":"But what I've seen for a lot of people is al- almost no one- well,","_id":"c3588a860b8c49c28e14245eae32a1e8"},{"start":"701.055","dur":"3.09","text":"When you're implementing these algorithms you use it over and over.","_id":"3d02426574cc43afb6d795ae05f19bd9"},{"start":"696.45","dur":"4.605","text":"And this is one of those formulas that, I don't know.","_id":"9bfeb36d09704ee1a0f9515d179977ad"},{"start":"690.76","dur":"5.69","text":"[NOISE]","_id":"418a0363e3d64f3d8f7e1838955f4efc"},{"start":"685.51","dur":"5.25","text":"[NOISE] the probability density function for a Gaussian looks like this.","_id":"9d4178529b034b498c61fea92287bf7e"},{"start":"682.645","dur":"2.865","text":"Um, and so, well,","_id":"247e54af9957425c9df91734ef9ab3e6"},{"start":"677.51","dur":"5.135","text":"Okay? And the derivation from this step to this step is given in the lecture notes.","_id":"7a08fb7239d440459ac24b78869bd411"},{"start":"673.84","dur":"3.67","text":"And omit- omit the square brackets to simplify the notation a bit.","_id":"96424406f838460b88bcd04c9100135d"},{"start":"670.255","dur":"3.585","text":"meaning the mean of z, sometimes I just write to this e, z right?","_id":"61eeb382dbe0425d8cf030dfcecb4fb3"},{"start":"667.9","dur":"2.355","text":"So instead of writing the expected value of z,","_id":"fc27af4907884cff95e269aad616da60"},{"start":"665.835","dur":"2.065","text":"I'm sometimes gonna omit the square brackets.","_id":"f73d0bf0e990497a8a3b064776cff401"},{"start":"662.805","dur":"3.03","text":"following sometimes semi-standard convention,","_id":"a845f15e8e4e43679812415cbb62fda8"},{"start":"657","dur":"5.805","text":"[NOISE] So you- and uh,","_id":"7e81c4fe6e674c2dbff5f3757e092dbe"},{"start":"655.83","dur":"1.17","text":"You can get this in the lecture notes.","_id":"58ee535fcd7f49b395ebcc2619d03d23"},{"start":"654.735","dur":"1.095","text":"we show in the lecture notes.","_id":"aa7b2416c5ab4afb904e98527b8472d0"},{"start":"651.65","dur":"3.085","text":"Right. Um, and this simplifies,","_id":"6ce5076b5f03440d9651206791984712"},{"start":"648.41","dur":"3.24","text":"uh, this is the formula.","_id":"4786e385de834cc389d1ebf9ffaaf6cb"},{"start":"644.44","dur":"3.97","text":"[NOISE] if you're familiar with multivariate co-variances,","_id":"5467b45591cf489198061aad0397a631"},{"start":"639.52","dur":"4.92","text":"And the um, covariance of z,","_id":"346e49effff54642ac1df4595071507a"},{"start":"633.365","dur":"6.155","text":"And the expected value of z is equal to um, the mean.","_id":"4af6d1793bc345ec885457402bdd55b8"},{"start":"630.84","dur":"2.525","text":"mu is two-dimensional and sigma is two-dimensional.","_id":"807abbdcf82e4d2f930d51b8ea57f275"},{"start":"628.5","dur":"2.34","text":"will be n by n. So z is two-dimensional,","_id":"8c4634528d384f90b4edc3da74c86e13"},{"start":"626.815","dur":"1.685","text":"And sigma, the covariance matrix,","_id":"efbce0765a5b436d8588fd8e1360d39e"},{"start":"618.615","dur":"8.2","text":"so if z is in Rn then mu would be Rn as well.","_id":"c476b7ec0d164184a80293b136256a7e"},{"start":"612.62","dur":"5.995","text":"with some mean vector mu and some covariance matrix sigma um,","_id":"62c0730f57e249c69cf2c90452d6a6e3"},{"start":"609.98","dur":"2.64","text":"[NOISE] this is due to Gaussian,","_id":"a39c956c359f4adab3d4d7aec122b986"},{"start":"607.655","dur":"2.325","text":"So um, if z,","_id":"bdd517bd8a934a7390c204be4c46ae37"},{"start":"603.83","dur":"3.825","text":"vector value random variables rather than a uni-variate random variable.","_id":"5438953c0fa446c6bc252539ca178987"},{"start":"600.41","dur":"3.42","text":"variable to multiple random variables at the same time to- to- to","_id":"291eccef83f14ef79e363ce1be7ee8aa"},{"start":"596.99","dur":"3.42","text":"this familiar bell-shaped curve over a 1-dimensional random","_id":"2db552af9c07453790d6caade3e5a889"},{"start":"594.17","dur":"2.82","text":"A multivariate Gaussian is the generalization of","_id":"fbc83b7ca512423598e3a5f160ddaf1e"},{"start":"591.32","dur":"2.85","text":"So the Gaussian is this familiar bell-shaped curve.","_id":"19be8302f3674d90b4ea1864a87c5076"},{"start":"586.595","dur":"4.725","text":"Cool. So let me- let me just go through what is a multivariate Gaussian distribution.","_id":"03e7126ae7494322ae377d2221298a6a"},{"start":"584.48","dur":"2.115","text":"Okay. Cool. Almost everyone. All right.","_id":"5876e4f8c8fc4231866c662846c488b6"},{"start":"582.635","dur":"1.845","text":"like a single dimensional Gaussian?","_id":"83ce775257f74e0a99fac93c6061831f"},{"start":"581.15","dur":"1.485","text":"of you are familiar about a uni-variate,","_id":"1108834483ad4634bd4abba1499fbc74"},{"start":"577.835","dur":"3.315","text":"One-third? No. Two-fifths? Okay. Cool. Alright. How many","_id":"19cb9dc0267846d0b6792eb8794f0bf6"},{"start":"575.405","dur":"2.43","text":"Raise your hand if you are. Like half of you?","_id":"3bfdbdba3b914dbd8d898c91da4d6d0d"},{"start":"571.13","dur":"4.275","text":"So um, actually, how many of you are familiar with the multivariate Gaussian?","_id":"550370a364ad43d8ae0b9b1f8cc2743e"},{"start":"568.31","dur":"2.82","text":"the distribution is also Gaussian.","_id":"c5ec2eba23344a0fb0f8338b77c3b315"},{"start":"566.75","dur":"1.56","text":"and condition on it being benign,","_id":"9d734d96b154485f99ba1ccf64558914"},{"start":"562.47","dur":"4.28","text":"the- the cell adhesion or whatever features you use to measure a tumor um,","_id":"8ca4a6f3d50e4dc98f29ad0b9d33020b"},{"start":"560.67","dur":"1.8","text":"size of the- size of the tumor,","_id":"7455c24a3aa4499f86659a9063cc3106"},{"start":"559.385","dur":"1.285","text":"The other features like uh,","_id":"eb7cdaa8b54446dfb541da28a3f259c5"},{"start":"556.854","dur":"2.531","text":"the distribution of the features is Gaussian.","_id":"fd5caccc6dec42b184e45494b1c6ab71"},{"start":"553.59","dur":"3.264","text":"In other words conditioned on the tumors being malignant,","_id":"5947a28fe2d344eab6a5657d5a152a34"},{"start":"547.97","dur":"5.62","text":"[NOISE] is distributed Gaussian, right?","_id":"10587968a4104899896b9c47128a69b7"},{"start":"543.11","dur":"4.86","text":"we're going to assume that P of x given y","_id":"1f5b93597cc84920b16b6de1bc074aa3"},{"start":"538.985","dur":"4.125","text":"And the key assumption in Gaussian discriminant analysis is,","_id":"6749655ea21b4754870603b28efc025c"},{"start":"535.19","dur":"3.795","text":"So x is now Rn rather than Rn plus 1.","_id":"b284bd20b13248a7959939bb4feecc40"},{"start":"530.67","dur":"4.52","text":"So I'm not gonna- we're not gonna need that extra x equals 1.","_id":"5d46ff95441d453785d516355c9298bc"},{"start":"525.45","dur":"5.22","text":"So you know, I'm gonna drop the x 0 equals 1, convention.","_id":"04da635ddd684e5cb4450175597608e4"},{"start":"521.715","dur":"3.735","text":"generative learning algorithms, I'm gonna use x and Rn.","_id":"08504dced9fc4883ac0bb559732b97f8"},{"start":"519.21","dur":"2.505","text":"And when we develop um,","_id":"11914d2ef71d47cd877f5121c183d5eb"},{"start":"516.435","dur":"2.775","text":"assuming that the features x are continuous values.","_id":"0a144354e9f6421e846f4b7cc335a22b"},{"start":"514.19","dur":"2.245","text":"let's develop this model,","_id":"47946f6cb89542d19643efe14664211f"},{"start":"504.21","dur":"5.98","text":"Um, so uh,","_id":"cdc1237584b84becaab75bc12cad5b00"},{"start":"497.46","dur":"6.75","text":"[NOISE] GDA.","_id":"52a32be998a5471190e78abc68660839"},{"start":"490.105","dur":"7.355","text":"So um, let's talk about Gaussian discriminant analysis.","_id":"38758bc31f294bf0bdfe07164956d97a"},{"start":"486.62","dur":"3.485","text":"right? Well we'll have a natural language processing example later.","_id":"3a77bf3d2eb44ef9b4f78e5bdad57b49"},{"start":"482.085","dur":"4.535","text":"Twitter things and see how positive or negative a sentiment on Twitter is or something.","_id":"9304711b299640539453a811815d1fc7"},{"start":"479.55","dur":"2.535","text":"Or - or I don't know. Or If you want to download","_id":"a68d069b182a492eb64114f496615970"},{"start":"477.78","dur":"1.77","text":"like in email spam, for example, right?","_id":"feccb7912a16431bb2b172d2f7e26696"},{"start":"476.025","dur":"1.755","text":"which uh, you can use for building,","_id":"32e9c84575154f0b89dfa5f1f2e35bc0"},{"start":"471.035","dur":"4.99","text":"which is used for things like the tumor classification and one for discrete features,","_id":"c0c347e7aaed486dbcfda071ebaa23be"},{"start":"468.86","dur":"2.175","text":"One for continuous value features,","_id":"31038ef028504e9a958875249fc4cf78"},{"start":"464.99","dur":"3.87","text":"And in fact, today you see two examples of generative learning algorithms.","_id":"22a68f43c703495988338d4a9b4d645e"},{"start":"461.54","dur":"3.45","text":"that's the framework we'll use to build generative learning algorithms.","_id":"37537c20b79a4234836e123ef359e985"},{"start":"458.18","dur":"3.36","text":"Okay? So um, [NOISE]","_id":"5a104fb1c1ac4e7d8a143775e621f127"},{"start":"453.005","dur":"5.175","text":"these - these two quantities in the red and in the orange circles.","_id":"451d9d39d2bf459e93bc8887861bbf6b"},{"start":"451.115","dur":"1.89","text":"If you've estimated you know","_id":"10a980f361e345cc8c3ecbfe00e780cc"},{"start":"447.3","dur":"3.815","text":"you could use this formula to calculate what's the chance that a tumor is malignant.","_id":"4ff3ab78f50142f0813b5005437c63b4"},{"start":"445.35","dur":"1.95","text":"So given the new patient with features x,","_id":"bbedc586b1ec4c2fb0878136dd415938"},{"start":"440.855","dur":"4.495","text":"Bayes' rule to calculate P of y equals 1, given x.","_id":"821ad133952e48a285ff84d1f87fba9c"},{"start":"437.845","dur":"3.01","text":"you could plug it into all of those terms and therefore use","_id":"305e18cdce2948ad9dd7b6d32ae93536"},{"start":"435.45","dur":"2.395","text":"in the red square and in the orange square,","_id":"030942029fe14624b930c93e86eb9c4e"},{"start":"433.215","dur":"2.235","text":"So if you've learned both- both of those terms","_id":"8ada1f0fd2224474b1259abcac31af8c"},{"start":"430.86","dur":"2.355","text":"goes into denominator, okay?","_id":"85d5dd0974a54c69b91271904780a4eb"},{"start":"423.795","dur":"7.065","text":"Right. Um, and so P of x in the denominators,","_id":"e5e4d82acbc9454a9e94b372cd8851fe"},{"start":"421.485","dur":"2.31","text":"you can plug that in here.","_id":"a7bca57a4cf94bd1b90f0dc043842bf7"},{"start":"418.05","dur":"3.435","text":"And if you've also learned this term P of y,","_id":"012dcc185897463e811b22cf8f127cf2"},{"start":"410.82","dur":"7.23","text":"then you can plug that in here, right?","_id":"b1333f8428ca4263bdb43a9d3f1a1e4d"},{"start":"408","dur":"2.82","text":"if you learn this term, P of x given y,","_id":"60852d5294d24c8bbfca0b1c08b58a23"},{"start":"398.51","dur":"9.49","text":"[NOISE] Um, and so","_id":"838c9a304220470394895ef48c24142d"},{"start":"390.95","dur":"7.56","text":"by the - [NOISE] okay?","_id":"fa2ca81b948c4a0ea117d34717b010cd"},{"start":"385.97","dur":"3.28","text":"Where P of x","_id":"39595966633545b78cd6bb7ba30067ee"},{"start":"376.19","dur":"9.78","text":"[NOISE] right?","_id":"9cdc92d2d6214793b91be1dece939cb4"},{"start":"374.12","dur":"2.07","text":"1 as this,","_id":"f593f901ffed4615949541b5fceb3b0a"},{"start":"371.66","dur":"2.46","text":"you can then calculate the chance of y being equal to","_id":"1830c883eb0b482cbab64de3f891f05f"},{"start":"367.1","dur":"4.56","text":"when you have a new test example [NOISE] with features x,","_id":"2c6a69d53ffe4c838ab72963d2370a7f"},{"start":"363.37","dur":"3.73","text":"if you can calculate numbers for both of these quantities then using Bayes' rule,","_id":"cd16ebf32b6145d0b05f5d7615863084"},{"start":"361.68","dur":"1.69","text":"um, if- you know,","_id":"4b9af24910d74a2b8e2b40a7b765280a"},{"start":"354","dur":"7.68","text":"[NOISE] if you can build a model for P of x given y and for P of y,","_id":"5f76e5a9a3be4d94af83447961ede008"},{"start":"350.255","dur":"3.745","text":"And so using Bayes' Rule,","_id":"699e6e19d07447dd8b245176ef786be0"},{"start":"347.3","dur":"2.955","text":"Before you see any features, okay?","_id":"f80be6dfe105429cb9cf9275664d0a91"},{"start":"344.09","dur":"3.21","text":"what are the odds that their tumor is malignant versus benign, right?","_id":"1aade824d6e0471e8b78f1d20bb3342b"},{"start":"342.8","dur":"1.29","text":"before you've even seen them,","_id":"0f18212cf20c403fb5c9a662291e7c6b"},{"start":"341.38","dur":"1.42","text":"before you've even examined them,","_id":"680d6c6439794a7db033dec99cdbc693"},{"start":"338.925","dur":"2.455","text":"It's just- when the patient walks into your office,","_id":"f28c7a14d51440d395c2c65a93c93291"},{"start":"337.485","dur":"1.44","text":"It's called a class prior.","_id":"2e14e031ab6c47d0b0ce13a204c6b9a0"},{"start":"331.305","dur":"6.18","text":"So this is a- this is also called the class prior to be this probability, I guess.","_id":"231c3f5ad0754660960faa5f12c50245"},{"start":"329.775","dur":"1.53","text":"will also learn P of y.","_id":"d1de65375ee74e8488ba021a6f62794e"},{"start":"324.745","dur":"5.03","text":"Okay? And then as- and then they'll also- generative learning algorithm,","_id":"2a594e25024e489ca83fb54c99857034"},{"start":"322.28","dur":"2.465","text":"what are the features x gonna be like?","_id":"3c1a18f264914eb2af4298a891cd2599"},{"start":"320.705","dur":"1.575","text":"Or given the tumor's benign,","_id":"8228391bd4df43758de5f996bddfc744"},{"start":"318.88","dur":"1.825","text":"what are the features likely gonna be like?","_id":"bc4f48703d1a4c76979fd1473b811bad"},{"start":"316.52","dur":"2.36","text":"So in other words, given that a tumor is malignant,","_id":"b0e9e57ec9814344a72b5f854c34d344"},{"start":"313.715","dur":"2.805","text":"we're gonna learn p of x given y.","_id":"323bed2a83b645bc8dd010e7632abdd8"},{"start":"311.015","dur":"2.7","text":"So um, instead P of y given x,","_id":"1f3ab47ab96e4d2783a76d4d6b281d37"},{"start":"304.79","dur":"6.225","text":"[NOISE] given the class, right?","_id":"2d2a50f687dc46ec86dedc05a801acfe"},{"start":"301.05","dur":"3.74","text":"So this says, what are the features like,","_id":"3787888edc5f495398adc17091778e1d"},{"start":"294.68","dur":"6.37","text":"of um, x given y.","_id":"08bdc97c231642999f1f015604fe437f"},{"start":"285.32","dur":"9.36","text":"[NOISE] it learns P","_id":"41e98921d2c5443882ffe2699962e5cd"},{"start":"281.14","dur":"4.18","text":"[NOISE] In contrast, a generative learning algorithm,","_id":"d80f2d87c89c4dbfbf8ad0937742ac91"},{"start":"278.59","dur":"2.55","text":"We're trying to discriminate between positive and negative classes.","_id":"29ea2b3b1b0d457295ede42c19853d4b"},{"start":"276.7","dur":"1.89","text":"So that's a discriminative learning algorithm.","_id":"5dea24f59af644948d6f598ec9d485b6"},{"start":"273.35","dur":"3.35","text":"But learns a function mapping from x to the labels directly.","_id":"2347df8dcf1f4fafa3169d77b8b6feda"},{"start":"271.21","dur":"2.14","text":"it's helpful to support vector machines later.","_id":"074302b3d82b489ba000e116a33926b5"},{"start":"268.785","dur":"2.425","text":"I think Annan briefly talked about the Perceptron algorithm,","_id":"031ba2d2ec3e4275ad77c7fe28d5bb5a"},{"start":"267.09","dur":"1.695","text":"You know, as I learn- Or it can learn,","_id":"de36b26a3e054f5e92748abe0348a8c7"},{"start":"262.88","dur":"4.21","text":"Some mapping [NOISE] from x to y directly.","_id":"4584f485a7684b7495b482507fa16aab"},{"start":"256.67","dur":"6.21","text":"[NOISE] right?","_id":"10886e43a53f47f0b2f19dabc08ee87d"},{"start":"250.385","dur":"6.285","text":"Um, or uh, what it learns um,","_id":"d4bb2d516be844f0a6e6098e1d21c908"},{"start":"241.76","dur":"8.625","text":"learns P of y given x, right?","_id":"14856f4a8556468d907965289c4cff1b"},{"start":"236.45","dur":"5.31","text":"Um, a discriminative learning [NOISE] algorithm","_id":"e189e428922a4d499f6b90674da5f440"},{"start":"234.455","dur":"1.995","text":"So let's formalize this.","_id":"06c07d9f112848cfa2684e8902dbbe2d"},{"start":"230.69","dur":"3.765","text":"the two models it matches more closely against.","_id":"d08e36c54ccd4240acc1a41708472512"},{"start":"227.625","dur":"3.065","text":"evaluates against the malignant model and tries to see which of","_id":"99a7973b374a4c57802c76a0b38d0ba8"},{"start":"224.15","dur":"3.475","text":"it evaluates a new example against the benign model,","_id":"281f31fb31354d41b13c1c860f7e4d7e"},{"start":"222.365","dur":"1.785","text":"And then at test time uh,","_id":"1a071b6277d3472ba72e5c4cfe3f90a7"},{"start":"220.16","dur":"2.205","text":"with some details we'll learn about later.","_id":"f4ae6fe31d7940e7a882b89df2139fc3"},{"start":"218.36","dur":"1.8","text":"kind of almost in isolation,","_id":"f53cf7feba844ba4b8cb29ec48b31330"},{"start":"214.51","dur":"3.85","text":"instead builds a model of what each of the classes looks like,","_id":"dc34ebb4789441b4bda104b157560a29"},{"start":"211.91","dur":"2.6","text":"a generative learning algorithm, uh,","_id":"2d1257d2937d4f3daf56dc8c2315d82d"},{"start":"206.61","dur":"5.3","text":"than looking at both classes simultaneously and searching for a way to separate them,","_id":"39a3db2ceb2c444a856cea6081d0781f"},{"start":"202.4","dur":"4.21","text":"Okay? So um, rather","_id":"8b659114cc29421e9757fbc7e9dc310e"},{"start":"200.15","dur":"2.25","text":"so we're gonna classify that as a benign tumor.","_id":"522f889f712b4ba3a7de67262d860301"},{"start":"196.82","dur":"3.33","text":"Looks a lot more like the benign tumors I had previously seen,","_id":"729c502bf6a64efe8ffbb9ed4f3952f6"},{"start":"195.32","dur":"1.5","text":"in this case, ah, it looks like this one.","_id":"afaaf8c2c0a84f0a9e9e305c9ce99ead"},{"start":"189.815","dur":"5.505","text":"to the malignant tumor model compared to the benign tumor model and then say,","_id":"1a66e92f23a34132adc0a2cc4246f7f5"},{"start":"186.17","dur":"3.645","text":"it would then look at this new patient and compare it","_id":"3d17917f769e4ce8ba2f60eaa3f13df1"},{"start":"180.17","dur":"6","text":"if there's a new patient in your office with those features, uh,","_id":"95a65799408e4bc7b992e93e4e2a954f"},{"start":"178.115","dur":"2.055","text":"And then at classification time,","_id":"122ad19dcbe54cc7bc5a7603c8627e0e"},{"start":"174.08","dur":"4.035","text":"ah, it looks like all the benign tumors roughly live in that ellipse.","_id":"0cc859509202455aa395680d08e9d0c0"},{"start":"169.925","dur":"4.155","text":"And then you look at all the benign tumors in isolation and say,","_id":"2abf278b5c2e4cadbdc0330a21afe8da"},{"start":"162.86","dur":"7.065","text":"roughly [NOISE] all the malignant tumors roughly live in that ellipse.","_id":"4541e49fcbc24700b423517f2c044475"},{"start":"159.24","dur":"3.62","text":"So you might say, ah, it looks like all the malignant tumors um,","_id":"b674e4064a8442cfa5a3adb05940a153"},{"start":"154.215","dur":"5.025","text":"In the cancer example and try to build a model for what malignant tumors look like.","_id":"a19530abfb1c42c2acc86c5f641d16b7"},{"start":"150.74","dur":"3.475","text":"First, we'll look at all of the malignant tumors, right?","_id":"d3b440c3926a49498de100dd405b27eb"},{"start":"147.78","dur":"2.96","text":"Instead, the algorithm is going to look at the classes one at a time.","_id":"c4065d7df3b34da99d06e8ed0caed74e"},{"start":"143.97","dur":"3.81","text":"which is rather than looking at two classes and trying to find the separation.","_id":"7605edce68a14593b9d7cbd4e05a52c0"},{"start":"140.5","dur":"3.47","text":"here's an alternative, just call it generative learning algorithm;","_id":"ce292b085e1a4c0dbbd4f4d8ca7fcaf3"},{"start":"137.59","dur":"2.91","text":"the way you saw last week, which is um,","_id":"bed1ea2502dd46a3b0cc0b6f4b0d2ce4"},{"start":"134.84","dur":"2.75","text":"which isn't trying to maximize the likelihood that you -","_id":"94186561d6734504834fc5eef6e8c491"},{"start":"129.965","dur":"4.875","text":"Now, there's a different class of algorithm which isn't searching for this separation,","_id":"2c66b9677b074c029674a70c2fd07ab3"},{"start":"125.81","dur":"4.155","text":"right, that's - that's what logistic regression would do.","_id":"b454a14f0dcd4cebbc0bcdc63b94eb32"},{"start":"120.635","dur":"5.175","text":"malignant tumors [NOISE] and the benign tumors example,","_id":"9fa943885177408ba655f4eec8fe4c1d"},{"start":"118.16","dur":"2.475","text":"Um, and so if this was the uh,","_id":"9648dcadfcc74ef294ec12480895ddf4"},{"start":"113.97","dur":"4.19","text":"searching for a decision boundary that separates the positive and negative examples.","_id":"51dd5c1492344207a853a13c56f65ee0"},{"start":"109.7","dur":"4.27","text":"And um, logistic regression is really searching for a line,","_id":"810358a54aab43b480af26ddc7fd8a66"},{"start":"107.675","dur":"2.025","text":"that separates the positive and negative examples.","_id":"a2c09f669db24604b31eb7a6157351de"},{"start":"103.88","dur":"3.795","text":"the line migrates or evolves until you get maybe a line like that,","_id":"6a7cd8936f32498089df09f680ea58ea"},{"start":"101.18","dur":"2.7","text":"like that and over the course of gradient descent, you know,","_id":"a2045afe2cfd46ddada553c265d8f3f7"},{"start":"98.15","dur":"3.03","text":"maybe starts with some digital boundary","_id":"2c9331e4cd7b4d7190f19ba079e5de73"},{"start":"94.67","dur":"3.48","text":"So if you randomish - randomly initialize parameters,","_id":"8c8552b290e6423c83218ed8c4d1fa5c"},{"start":"91.76","dur":"2.91","text":"a line that separates the positive-negative examples, right?","_id":"a7d3f2c81a5b48f594579ba1265f46d1"},{"start":"89.24","dur":"2.52","text":"is use gradient descent to search for","_id":"339ed71b7a0744309c6d0dcc9a9f469b"},{"start":"87.29","dur":"1.95","text":"like logistic regression would do,","_id":"0f4bec2c19054c95ae7f315b6afebc3b"},{"start":"84.33","dur":"2.96","text":"then what a discriminative learning algorithm,","_id":"7a6c1e9ed9594341a242b38cfe624441"},{"start":"77.345","dur":"6.985","text":"And um, if you have a data set that looks like this with two classes,","_id":"149e742207924b038615f5e4a475bd84"},{"start":"69.23","dur":"8.115","text":"Okay? So um, we'll use binary classification as the motivating example for today.","_id":"00c5d3276dbd44a9a4a75e36dc01778b"},{"start":"66.61","dur":"2.62","text":"build a spam filter, for example.","_id":"ee5061e6bc124d0e9b82ddf2ce787c40"},{"start":"62.495","dur":"4.115","text":"And then we'll talk about naive Bayes and how you can use that to uh,","_id":"6d7e0871702241f88896bd1f8d09803f"},{"start":"60.26","dur":"2.235","text":"versus discriminative learn- learning algorithms.","_id":"d484187eb6f142a0a25b6a973b249002"},{"start":"58.25","dur":"2.01","text":"which is a new class of algorithms you hear about today,","_id":"8fd74b41a14848ed8bd7c08bfadc03a7"},{"start":"54.44","dur":"3.81","text":"Um, and there was a helpful comparison between generative learning algorithms,","_id":"0942baa595824f1fa5824b2e324c18b0"},{"start":"51.65","dur":"2.79","text":"very small data sets sometimes with some caveats.","_id":"cb8d0337a8f34c3ca948aca2a951b51b"},{"start":"48.02","dur":"3.63","text":"So um, and it sometimes works better if you have uh,","_id":"43c66aa096274a6da5e95a7784cb28c8"},{"start":"45.65","dur":"2.37","text":"efficient algorithm to implement ah, in some cases.","_id":"239ec83405104a00a3c3f9995879e3fd"},{"start":"43.35","dur":"2.3","text":"simpler and maybe more computationally","_id":"2ce0930a49114bab9eca3fbe4d1e58c7"},{"start":"40.74","dur":"2.61","text":"GDA is actually a um,","_id":"5c65aab1479340eb91905d743a68136c"},{"start":"37.29","dur":"3.45","text":"compared to say logistic regression for classification,","_id":"6861f652c38245159da7cd08c5ca477a"},{"start":"35.91","dur":"1.38","text":"And it turns out that uh,","_id":"04474d9e64e7484badfa6e2150d965fb"},{"start":"34.62","dur":"1.29","text":"you will know how to implement this.","_id":"7455c518ef6c4f1e817866a725a39986"},{"start":"32.19","dur":"2.43","text":"Gaussian discriminant analysis so by the end of the day,","_id":"ebff0df8b90a42ddb1b4f6dfb17b0055"},{"start":"30.6","dur":"1.59","text":"Um, and in particular you learned about","_id":"09f11dee394d454baa961af2f840f286"},{"start":"27.6","dur":"3","text":"you how generative learning algorithms work.","_id":"49bf9c7037b54470858204c8645bd38b"},{"start":"24.72","dur":"2.88","text":"And today um, what I'd like to do is share with","_id":"91c76129d6b944da8156f09e40c22263"},{"start":"22.77","dur":"1.95","text":"which is one big bucket of learning algorithms.","_id":"01ff5162762e4563bf58715fb7ca44b1"},{"start":"19.65","dur":"3.12","text":"so far are called discriminative learning algorithms,","_id":"2ba158f9b04647958d7f326e289f8ef6"},{"start":"16.47","dur":"3.18","text":"And it turns out all of the learning algorithms we've been learning about","_id":"b142151259c44ed9a07085d01c7dd3ce"},{"start":"13.945","dur":"2.525","text":"uh, generalized linear models.","_id":"f0793fa6e6c843db9fa6c0837fd4fdc5"},{"start":"11.36","dur":"2.585","text":"logistic regression and um,","_id":"ee0ed826c9fd4b849c721e9fbe317223"},{"start":"6.88","dur":"4.48","text":"Um, so last week you heard about uh,","_id":"ad56226ef4a741a281351eaaf07a94f1"},{"start":"3.47","dur":"3.41","text":"Hey, morning everyone. Welcome back.","_id":"c2779fbb482b4d2782edf94b578e1e26"}]